{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Keys in the Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'nevent',\n",
       " b'nlumi',\n",
       " b'nrun',\n",
       " b'trackEta',\n",
       " b'trackPhi',\n",
       " b'qoverp',\n",
       " b'dxy',\n",
       " b'dsz',\n",
       " b'trackPt',\n",
       " b'trackTPIndex',\n",
       " b'trackEtaError',\n",
       " b'trackPhiError',\n",
       " b'qoverpError',\n",
       " b'dxyError',\n",
       " b'dszError',\n",
       " b'trackPtError',\n",
       " b'trackParameters',\n",
       " b'covarianceArray',\n",
       " b'stereoHitX',\n",
       " b'stereoHitY',\n",
       " b'stereoHitZ',\n",
       " b'stereoHitR',\n",
       " b'stereoHitPhi',\n",
       " b'stereoHitEta',\n",
       " b'stereoHitLayer',\n",
       " b'stereoTPIndex',\n",
       " b'stereoHitMatch',\n",
       " b'monoHitX',\n",
       " b'monoHitY',\n",
       " b'monoHitZ',\n",
       " b'monoHitR',\n",
       " b'monoHitPhi',\n",
       " b'monoHitEta',\n",
       " b'monoHitLayer',\n",
       " b'monoTPIndex',\n",
       " b'monoHitMatch',\n",
       " b'simHitX',\n",
       " b'simHitY',\n",
       " b'simHitZ',\n",
       " b'simhitTPIndex',\n",
       " b'simHitMatch']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_events_ = 3000\n",
    "outfile_ = \"1.root\"\n",
    "data_ = uproot.open(outfile_)[\"ntuples\"][\"tree\"]\n",
    "data_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Integrity of the Imported Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 3000 events\n"
     ]
    }
   ],
   "source": [
    "stereo_tp_idx_ = data_.array('stereoTPIndex')\n",
    "mono_tp_idx_ = data_.array('monoTPIndex')\n",
    "track_tp_idx_ = data_.array('trackTPIndex')\n",
    "\n",
    "# Check that both have been generated for the same number of events\n",
    "# Just for clarity\n",
    "assert len(track_tp_idx_) == len(stereo_tp_idx_), \"Track and Stereo Number of Events do not match\"\n",
    "assert len(track_tp_idx_) == len(mono_tp_idx_), \"Track and Mono Number of Events do not match\"\n",
    "print (\"\\nTotal\", len(track_tp_idx_), \"events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_set(input_array_):\n",
    "    '''\n",
    "    Format: 3-level nested lists - [[[...] ...] ...]\n",
    "    '''\n",
    "    output_array_ = []\n",
    "    for index_ in range(len(input_array_)):\n",
    "        output_array_.append([])\n",
    "        for second_list_ in input_array_[index_]:\n",
    "            output_array_[index_].append(set(second_list_))\n",
    "    return output_array_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load the track parameters into the respective arrays to be added into the rechit_param_global dataframe\n",
    "'''\n",
    "\n",
    "rechit_cartesian_ = OrderedDict({})\n",
    "for key in ['stereoHitX', 'stereoHitY', 'stereoHitZ', 'monoHitX', 'monoHitY', 'monoHitZ']:\n",
    "    rechit_cartesian_[key] = data_.array(key)\n",
    "\n",
    "rechit_polar_ = OrderedDict({})\n",
    "for key in ['stereoHitR', 'stereoHitEta', 'stereoHitPhi', 'monoHitR', 'monoHitEta', 'monoHitPhi']:\n",
    "    rechit_polar_[key] = data_.array(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 1: Reformat List of Indices to Sets of Indices for each Rechit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all tracking particle index lists to sets for faster search\n",
    "\n",
    "mono_tp_idx_set_ = list_to_set(mono_tp_idx_)\n",
    "stereo_tp_idx_set_ = list_to_set(stereo_tp_idx_)\n",
    "track_tp_idx_set_ = list_to_set(track_tp_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Add all data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Global Dataframe of Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Adding stereo and mono rechit data into a global dataframe\n",
    "\n",
    ":event_id: int\n",
    ":rechit_id: int\n",
    ":track_id: int\n",
    ":rechit_ids: list(int)\n",
    ":track_ids: list(int)\n",
    ":track_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":rechit_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":match_count: int  # count the number of rechits/tracks matched to the given track/rechit\n",
    ":rechit_tp_index_: event-based list of rechit-based list of sets of int (tp_index)\n",
    "\n",
    "'''\n",
    "def create_global_rechit_df(stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_dict_, rechit_polar_dict_):\n",
    "    rechit_global_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                      'track_ids': [], 'match_count': [], 'rechit_local_id': []})\n",
    "    rechit_param_global_map_ = OrderedDict({'event_id': [], 'rechit_id':[], 'rechit_x': [], 'rechit_y': [], 'rechit_z': [], \n",
    "                                            'rechit_r': [], 'rechit_phi': [], 'rechit_eta': [], 'rechit_local_id': []})\n",
    "    global_counter_ = 0\n",
    "    \n",
    "    if len(stereo_tp_idx_) != len(stereo_tp_idx_):\n",
    "        raise ValueError('Rechit arrays represent differing event lengths [stereo, mono]:', len(stereo_tp_idx_), len(mono_tp_idx_))\n",
    "    \n",
    "    for event_id_ in range(len(stereo_tp_idx_)):\n",
    "        # Count the number of rechits in that event\n",
    "        event_rechit_count_ = len(stereo_tp_idx_[event_id_]) + len(mono_tp_idx_[event_id_])\n",
    "\n",
    "        rechit_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        # appends SAME instance of [event_id] event_rechit_count_ times\n",
    "        \n",
    "        rechit_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))     \n",
    "        rechit_global_map_['rechit_tp_index'].extend(stereo_tp_idx_[event_id_])\n",
    "        rechit_global_map_['rechit_tp_index'].extend(mono_tp_idx_[event_id_])\n",
    "        rechit_global_map_['track_ids'].extend([[] for _ in range(event_rechit_count_)])\n",
    "        rechit_global_map_['match_count'].extend([0 for _ in range(event_rechit_count_)])\n",
    "        rechit_global_map_['rechit_local_id'].extend(range(event_rechit_count_))\n",
    "        \n",
    "        # Extend the hit_param_global_map_ with rechit parameters\n",
    "        rechit_param_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))\n",
    "        rechit_param_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['stereoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['monoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['stereoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['monoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['stereoHitZ'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['monoHitZ'][event_id_])\n",
    "        \n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['stereoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['monoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['stereoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['monoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['stereoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['monoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_local_id'].extend(range(event_rechit_count_))\n",
    "        global_counter_ += event_rechit_count_\n",
    "    # Convert dict to dataframe\n",
    "    rechit_global_df_ = df.from_dict(rechit_global_map_)\n",
    "    rechit_param_global_df_ = df.from_dict(rechit_param_global_map_)\n",
    "    return rechit_global_df_, rechit_param_global_df_\n",
    "    \n",
    "# Check Memory Usage of DataFrame\n",
    "# print rechit_global_df_.memory_usage(deep=True)\n",
    "# print rechit_param_global_df_.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Global Rechit Array and Global Rechit Parameters Array'''\n",
    "rechit_global_df_uncut_, rechit_param_global_df_uncut_ = create_global_rechit_df(\n",
    "    stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_, rechit_polar_)\n",
    "#print rechit_global_df_.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the Cuts (create DF for Graph Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum hits in an event are:  14750\n",
      "Average hits in an event are 1760.6123333333333\n"
     ]
    }
   ],
   "source": [
    "'''Check the maximum number of hits in an event'''\n",
    "max_len_ = 0 \n",
    "for i in range(number_of_events_):\n",
    "    len_idx_ = len(stereo_tp_idx_[i]) + len(mono_tp_idx_[i])\n",
    "    if len_idx_ > max_len_:\n",
    "        max_len_ = len_idx_\n",
    "        #print max_len_\n",
    "        \n",
    "print (\"Maximum hits in an event are: \", max_len_)\n",
    "print(\"Average hits in an event are\", float(sum([len(x) for x in stereo_tp_idx_]))/float(number_of_events_))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and Cut the Rechit DataFrames - also reorder Rechit Global and Local IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rechits left after 1st cut:  12587193\n",
      "No. of rechits left after second cut: 3436014\n",
      "3436014 of 17505357 0.19628357193743606 hits remain\n"
     ]
    }
   ],
   "source": [
    "'''This is done here to generate a reduced number of local indices for the tracks to match to rechits.\n",
    "We will replace the rechit_global_df_ generated above and used below for matches with this new dataframe.'''\n",
    "\n",
    "total_number_of_rechits_ = len(rechit_global_df_uncut_)\n",
    "# Place the cuts on rechits not matched to any tracking particles (reduces rechits by about 75% for 10 events)\n",
    "tp_unmatched_indices_ = rechit_global_df_uncut_[rechit_global_df_uncut_.rechit_tp_index.map(len) == 0].index\n",
    "rechit_global_df_uncut_.drop(tp_unmatched_indices_, inplace=True)\n",
    "rechit_param_global_df_uncut_.drop(tp_unmatched_indices_, inplace = True)\n",
    "print(\"No of rechits left after 1st cut: \", len(rechit_global_df_uncut_))\n",
    "\n",
    "\n",
    "# After each cut, we reset the global rechit ids so that 'iloc' and 'loc' do not throw errors\n",
    "# Reset the Index of the Cut Dataframe that will become the new Global DataFrame\n",
    "rechit_global_df_uncut_.index = pd.RangeIndex(len(rechit_global_df_uncut_.index))  \n",
    "rechit_param_global_df_uncut_.index = pd.RangeIndex(len(rechit_global_df_uncut_.index))\n",
    "\n",
    "# Update the Global Rechit IDs\n",
    "rechit_global_id_dict_ = {}\n",
    "rechit_global_id_dict_['rechit_id'] = range(len(rechit_global_df_uncut_))\n",
    "rechit_global_df_uncut_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))    \n",
    "rechit_param_global_df_uncut_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))    \n",
    "\n",
    "# Place the cut on rechit eta to ensure you consider only the hits in the tracker\n",
    "rechit_param_global_df_ = rechit_param_global_df_uncut_[np.abs(rechit_param_global_df_uncut_['rechit_eta']) <= 0.9].copy()\n",
    "rechit_global_df_ = rechit_global_df_uncut_.iloc[rechit_param_global_df_['rechit_id'].index].copy()\n",
    "print(\"No. of rechits left after second cut:\", len(rechit_param_global_df_))\n",
    "\n",
    "# After each cut, we reset the global rechit ids so that 'iloc' and 'loc' do not throw errors\n",
    "\n",
    "# Reset the Index of the Cut Dataframe that will become the new Global DataFrame\n",
    "# This will lose the former global rechit index - can this affect the analysis in the future?\n",
    "rechit_global_df_.index = pd.RangeIndex(len(rechit_global_df_.index))  \n",
    "rechit_param_global_df_.index = pd.RangeIndex(len(rechit_global_df_.index))\n",
    "\n",
    "# Reset the local_rechit_ids for graph networks to have sequential nodes\n",
    "# And so that the node feature vector can be simpler to create sequentially\n",
    "rechit_local_id_dict_ = {'rechit_local_id' : []}\n",
    "# Find the minimum number of rechits in the final list of events\n",
    "min_num_of_rechits_ = 9999\n",
    "for event_id_ in range(number_of_events_):\n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    rechit_local_range_ = range(len(rechit_global_df_[rechit_global_df_['event_id']==event_id_]))\n",
    "    rechit_local_id_dict_['rechit_local_id'].extend(rechit_local_range_)\n",
    "    if rechit_local_range_[-1] < min_num_of_rechits_:\n",
    "        min_num_of_rechits_ = rechit_local_range_[-1]        \n",
    "        \n",
    "# Update the Global Rechit IDs\n",
    "rechit_global_id_dict_ = {}\n",
    "rechit_global_id_dict_['rechit_id'] = range(len(rechit_global_df_))\n",
    "rechit_global_df_.update(pd.DataFrame.from_dict(rechit_local_id_dict_))    \n",
    "rechit_param_global_df_.update(pd.DataFrame.from_dict(rechit_local_id_dict_))    \n",
    "\n",
    "# Update the Local Rechit IDs\n",
    "rechit_global_df_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))    \n",
    "rechit_param_global_df_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))\n",
    "\n",
    "print (len(rechit_param_global_df_), \"of\", total_number_of_rechits_, \\\n",
    "float(len(rechit_param_global_df_))/float(total_number_of_rechits_), \"hits remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the Rechits to Tracks and Create a Global Array of Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching tracks in event:  1\n",
      "Matching tracks in event:  11\n",
      "Matching tracks in event:  21\n",
      "Matching tracks in event:  31\n",
      "Matching tracks in event:  41\n",
      "Matching tracks in event:  51\n",
      "Matching tracks in event:  61\n",
      "Matching tracks in event:  71\n",
      "Matching tracks in event:  81\n",
      "Matching tracks in event:  91\n",
      "Matching tracks in event:  101\n",
      "Matching tracks in event:  111\n",
      "Matching tracks in event:  121\n",
      "Matching tracks in event:  131\n",
      "Matching tracks in event:  141\n",
      "Matching tracks in event:  151\n",
      "Matching tracks in event:  161\n",
      "Matching tracks in event:  171\n",
      "Matching tracks in event:  181\n",
      "Matching tracks in event:  191\n",
      "Matching tracks in event:  201\n",
      "Matching tracks in event:  211\n",
      "Matching tracks in event:  221\n",
      "Matching tracks in event:  231\n",
      "Matching tracks in event:  241\n",
      "Matching tracks in event:  251\n",
      "Matching tracks in event:  261\n",
      "Matching tracks in event:  271\n",
      "Matching tracks in event:  281\n",
      "Matching tracks in event:  291\n",
      "Matching tracks in event:  301\n",
      "Matching tracks in event:  311\n",
      "Matching tracks in event:  321\n",
      "Matching tracks in event:  331\n",
      "Matching tracks in event:  341\n",
      "Matching tracks in event:  351\n",
      "Matching tracks in event:  361\n",
      "Matching tracks in event:  371\n",
      "Matching tracks in event:  381\n",
      "Matching tracks in event:  391\n",
      "Matching tracks in event:  401\n",
      "Matching tracks in event:  411\n",
      "Matching tracks in event:  421\n",
      "Matching tracks in event:  431\n",
      "Matching tracks in event:  441\n",
      "Matching tracks in event:  451\n",
      "Matching tracks in event:  461\n",
      "Matching tracks in event:  471\n",
      "Matching tracks in event:  481\n",
      "Matching tracks in event:  491\n",
      "Matching tracks in event:  501\n",
      "Matching tracks in event:  511\n",
      "Matching tracks in event:  521\n",
      "Matching tracks in event:  531\n",
      "Matching tracks in event:  541\n",
      "Matching tracks in event:  551\n",
      "Matching tracks in event:  561\n",
      "Matching tracks in event:  571\n",
      "Matching tracks in event:  581\n",
      "Matching tracks in event:  591\n",
      "Matching tracks in event:  601\n",
      "Matching tracks in event:  611\n",
      "Matching tracks in event:  621\n",
      "Matching tracks in event:  631\n",
      "Matching tracks in event:  641\n",
      "Matching tracks in event:  651\n",
      "Matching tracks in event:  661\n",
      "Matching tracks in event:  671\n",
      "Matching tracks in event:  681\n",
      "Matching tracks in event:  691\n",
      "Matching tracks in event:  701\n",
      "Matching tracks in event:  711\n",
      "Matching tracks in event:  721\n",
      "Matching tracks in event:  731\n",
      "Matching tracks in event:  741\n",
      "Matching tracks in event:  751\n",
      "Matching tracks in event:  761\n",
      "Matching tracks in event:  771\n",
      "Matching tracks in event:  781\n",
      "Matching tracks in event:  791\n",
      "Matching tracks in event:  801\n",
      "Matching tracks in event:  811\n",
      "Matching tracks in event:  821\n",
      "Matching tracks in event:  831\n",
      "Matching tracks in event:  841\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Match Rechits to Tracks.\n",
    "Create the Global Track Array and Global Track Parameter Array.\n",
    "'''\n",
    "# TODO: Refactor this to enable placing track cuts before forming dataframe and reduce processing by 75%\n",
    "# The 75% metric follows from: For 100 events track cuts reduce tracks by 75%\n",
    "# Initialize the Global Track Parameter Map\n",
    "track_param_global_map_ = OrderedDict({})\n",
    "for key in ['track_id', 'track_eta', 'track_phi', 'track_qoverp', 'track_dxy', 'track_dsz', 'track_pt']:\n",
    "    track_param_global_map_[key] = []\n",
    "    \n",
    "# Define the dictionaries to be cast into dataframes\n",
    "track_to_rechit_map_ = OrderedDict({'event_id': [], 'track_id': [], 'track_tp_index': [], \n",
    "                                    'rechit_ids': [], 'match_count': [], 'rechit_local_ids': []})\n",
    "\n",
    "# Future Requirement?\n",
    "rechit_to_track_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                    'track_ids': [], 'match_count': []})\n",
    "\n",
    "# Initialize the Global Track ID\n",
    "global_track_id_ = 0\n",
    "\n",
    "for event_id_ in range(len(track_tp_idx_)):\n",
    "    if event_id_ % 10 == 1:\n",
    "        print(\"Matching tracks in event: \", event_id_)\n",
    "    num_tracks_in_event_ = len(track_tp_idx_[event_id_])\n",
    "\n",
    "    # Add track data to the dict in an efficient manner\n",
    "    track_to_rechit_map_['event_id'].extend([event_id_] * num_tracks_in_event_)\n",
    "    \n",
    "    global_track_id_range_ = range(global_track_id_, global_track_id_ + num_tracks_in_event_)\n",
    "    \n",
    "    track_to_rechit_map_['track_id'].extend(global_track_id_range_)\n",
    "    track_to_rechit_map_['track_tp_index'].extend(track_tp_idx_[event_id_])\n",
    "    \n",
    "    # Append multiple empty lists in place of the values not filled yet\n",
    "    track_to_rechit_map_['match_count'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_['rechit_ids'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_['rechit_local_ids'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    \n",
    "    # Fill in the Global Track Parameters\n",
    "    track_param_global_map_['track_id'].extend(global_track_id_range_)\n",
    "    track_param_global_map_['track_eta'].extend(data_.array('trackEta')[event_id_])\n",
    "    track_param_global_map_['track_phi'].extend(data_.array('trackPhi')[event_id_])\n",
    "    track_param_global_map_['track_pt'].extend(data_.array('trackPt')[event_id_])\n",
    "    track_param_global_map_['track_qoverp'].extend(data_.array('qoverp')[event_id_])\n",
    "    track_param_global_map_['track_dxy'].extend(data_.array('dxy')[event_id_])\n",
    "    track_param_global_map_['track_dsz'].extend(data_.array('dsz')[event_id_])\n",
    "    \n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    \n",
    "    # Check the TPs matched to tracks and find rechits for each TP (Stereo and Mono)\n",
    "    for track_tp_list_ in track_tp_idx_[event_id_]:\n",
    "        rechit_matches_ = []\n",
    "        rechit_local_matches_ = []\n",
    "        if len(track_tp_list_) == 0:\n",
    "            continue\n",
    "            \n",
    "        if len(track_tp_list_) >= 1:\n",
    "\n",
    "            # Iterate over the index and values of each rechit tp index list\n",
    "            for (idx_, tp_idx_list_) in event_df_['rechit_tp_index'].iteritems():\n",
    "                # Find the match for the first tp index in the track tp list\n",
    "                if track_tp_list_[0] in tp_idx_list_:\n",
    "                    rechit_matches_.append(event_df_.loc[idx_, 'rechit_id'])\n",
    "                    rechit_local_matches_.append(event_df_.loc[idx_, 'rechit_local_id'])\n",
    "                    # Append the global track id to the rechit\n",
    "                    event_df_.loc[idx_, 'track_ids'].append(global_track_id_)\n",
    "            track_to_rechit_map_['match_count'][global_track_id_] = len(rechit_matches_)\n",
    "            track_to_rechit_map_['rechit_ids'][global_track_id_] = set(rechit_matches_)\n",
    "            track_to_rechit_map_['rechit_local_ids'][global_track_id_] = set(rechit_local_matches_)\n",
    "            \n",
    "        # If track has multiple tp indices, pick the one with the most hits\n",
    "\n",
    "        # Note: This approach *possibly* creates match issues if the tp index with more rechit matches\n",
    "        # has more 'common' hits with other tracks and is later discarded due to the common hits \n",
    "        # belonging to other tracks\n",
    "        if len(track_tp_list_) > 1:\n",
    "            rechit_matches_array_ = []\n",
    "            rechit_local_matches_array_ = []\n",
    "            match_count_array_ = []\n",
    "            tmp_dict_ = []  # syntax is off this is actually a list but otherwise ok\n",
    "            \n",
    "            print (\"Found multiple TP indices in event\", event_id_, \"for global track\") \n",
    "            print (global_track_id_, track_tp_list_)\n",
    "            \n",
    "            for track_idx_ in track_tp_list_:\n",
    "                rechit_matches_ = []\n",
    "                rechit_local_matches_ = []\n",
    "                \n",
    "                # Iterate over the index and values of each rechit tp index list\n",
    "                for (idx_, tp_idx_list_) in event_df_['rechit_tp_index'].iteritems():\n",
    "                    if track_idx_ in tp_idx_list_:\n",
    "                        rechit_matches_.append(event_df_.loc[idx_,'rechit_id'])\n",
    "                        rechit_local_matches_.append(event_df_.loc[idx_,'rechit_local_id'])\n",
    "                        # Append the global track id to the rechit\n",
    "                        event_df_.loc[idx_, 'track_ids'].append(global_track_id_)\n",
    "                rechit_matches_array_.append(rechit_matches_)\n",
    "                rechit_local_matches_array_.append(rechit_local_matches_)\n",
    "                match_count_array_.append(len(rechit_matches_))\n",
    "            \n",
    "            # Store the global rechit ids and count of matches in a temporary list\n",
    "            for key, value, extra in zip(match_count_array_, rechit_matches_array_, rechit_local_matches_array_):\n",
    "                tmp_dict_.append((key, value, extra))\n",
    "            \n",
    "            # Pick the largest number of matches and corresponding global rechit ids\n",
    "            tmp_dict_ = sorted(tmp_dict_, reverse=True)\n",
    "            track_to_rechit_map_['match_count'][global_track_id_] = tmp_dict_[0][0]\n",
    "            track_to_rechit_map_['rechit_ids'][global_track_id_] = tmp_dict_[0][1]\n",
    "            track_to_rechit_map_['rechit_local_ids'][global_track_id_] = tmp_dict_[0][2]\n",
    "        \n",
    "        \n",
    "        # Check duplicates\n",
    "        if len(set(rechit_matches_)) < len(rechit_matches_):\n",
    "            raise ValueError('rechit_matches_ has duplicate values: Some Rechits are being matched twice!')\n",
    "        \n",
    "        # Increment the Global Track ID\n",
    "        global_track_id_ += 1\n",
    "    rechit_global_df_.update(event_df_, join='left')\n",
    "    track_param_global_df_ = df.from_dict(track_param_global_map_)\n",
    "track_global_df_ = df.from_dict(track_to_rechit_map_)\n",
    "\n",
    "#Update the match_count for rechits based on the number of total matched tracks\n",
    "match_count_tmp_dict_ = OrderedDict({'match_count': [len(track_id_list_) for track_id_list_ in rechit_global_df_['track_ids']]})\n",
    "print (\"Maximum tracks matched for one particle:\", max(match_count_tmp_dict_['match_count']))\n",
    "\n",
    "rechit_global_df_.update(df.from_dict(match_count_tmp_dict_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data stored in the track_to_rechit_map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_rechit_df_ = df.from_dict(track_to_rechit_map_)\n",
    "#print track_to_rechit_df_[track_to_rechit_df_['event_id']==11].head(10)\n",
    "\n",
    "# Calculate the average number of hits per track\n",
    "average_rechits_per_track_ = 0\n",
    "len_array_ = []\n",
    "for rechit_list_ in track_to_rechit_df_['rechit_ids']:\n",
    "    average_rechits_per_track_ += len(rechit_list_)\n",
    "    len_array_.append(len(rechit_list_))\n",
    "\n",
    "print (\"Average Rechits per track:\", average_rechits_per_track_/len(track_to_rechit_df_['rechit_ids']))\n",
    "print (\"Max. matched hits to track:\", max(len_array_), \"; Global track id:\", len_array_.index(max(len_array_)))\n",
    "\n",
    "\n",
    "# Test to check if the correct tp index has been matched\n",
    "# Change the value of 'trk_id_' to any track that you know has some hits\n",
    "trk_id_ = len_array_.index(max(len_array_))\n",
    "# print track_to_rechit_df_.loc[trk_id_]\n",
    "for rechit_id in track_to_rechit_df_.loc[trk_id_]['rechit_ids']:\n",
    "    for track_idx_ in track_to_rechit_df_.loc[trk_id_]['track_tp_index']:\n",
    "        if track_idx_ in rechit_global_df_.loc[rechit_id]['rechit_tp_index']:\n",
    "            continue\n",
    "        else:\n",
    "            print (\"Error: Track and rechit TP index does not match!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cycler import cycler\n",
    "from matplotlib.colors import Colormap\n",
    "\n",
    "#fig_ = plt.figure()\n",
    "#ax_ = Axes3D(fig_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Matched/Unmatched Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Count the number of matched, unmatched, and total rechits/tracks in the dataframe (PER EVENT)\n",
    "\n",
    "Store the count of unmatched, tp_matched, track/rechit_matched, and total rechits/tracks PER EVENT in an array of length number_of_events_\n",
    "Store all four above arrays (unmatched, tp_matched, track/rechit_matched, total) in a dictionary\n",
    "'''\n",
    "\n",
    "def count_matched_items(item_type_):\n",
    "    other_item_ids_ = 'track_ids' if (item_type_=='rechit') else 'rechit_ids'\n",
    "    other_item_matched_ = 'track_matched' if (item_type_=='rechit') else 'rechit_matched'\n",
    "    item_id_ = item_type_ + '_id'\n",
    "    item_tp_index_ = item_type_ + '_tp_index'\n",
    "\n",
    "    # Initialize one array for counts and one for ids of matched/unmatched rechits\n",
    "    item_count_dict_ = OrderedDict({other_item_matched_:[], 'unmatched':[], 'tp_matched':[], 'total':[]})\n",
    "    item_id_dict_ = OrderedDict({'tp_matched':[], other_item_matched_:[], 'unmatched':[]})\n",
    "\n",
    "    for event_id_ in range(number_of_events_):\n",
    "        \n",
    "        # Create a slice of the dataframe with the data for that event\n",
    "        event_df_ = (rechit_global_df_[rechit_global_df_['event_id']==event_id_]) if (item_type_=='rechit') else (track_global_df_[track_global_df_['event_id']==event_id_])\n",
    "\n",
    "        # Count the number of matched, unmatched, and total rechits \n",
    "        num_matched_ = sum(event_df_['match_count'] > 0)\n",
    "        num_unmatched_ = sum(event_df_['match_count'] == 0)\n",
    "        num_total_ = event_df_.shape[0]  # number of rows/rechits in the event\n",
    "        \n",
    "        # Find and store the indices of matched and unmatched rechits\n",
    "        \n",
    "        item_id_dict_[other_item_matched_].append(set(event_df_.loc[event_df_['match_count'] > 0, (item_id_)].tolist()))\n",
    "        item_id_dict_['unmatched'].append(set(event_df_.loc[event_df_['match_count'] == 0, (item_id_)].tolist()))\n",
    "        \n",
    "        # Sanity checks to ensure data has been added into the dataframe corrrectly\n",
    "        assert num_total_ == (num_matched_ + num_unmatched_), \\\n",
    "        \"Rechit counts (unmatched, matched, total) do not add up\"\n",
    "            \n",
    "        if item_type_ == 'rechit':\n",
    "            # Check the number of total rechits for the event is the same as in raw data\n",
    "            assert (len(rechit_global_df_[rechit_global_df_['event_id']==event_id_])) == num_total_, \\\n",
    "            \"Rechits in dataframe %d and stereo_tp_idx_ %d do not match\" % (num_total_, len(stereo_tp_idx_[event_id_]))\n",
    "        \n",
    "        elif item_type_ == 'track':\n",
    "            # Check the number of total tracks for the event is the same as in raw data\n",
    "            assert len(track_tp_idx_[event_id_]) == num_total_, \\\n",
    "            \"Tracks in dataframe %d and track_tp_idx_ %d do not match\" % (num_total_, len(track_tp_idx_[event_id_]))\n",
    "    \n",
    "        # Append the hit counts into the dataframe\n",
    "        item_count_dict_['unmatched'].append(num_unmatched_)\n",
    "        item_count_dict_['total'].append(num_total_)\n",
    "        \n",
    "        # TODO: Why is default value for tracks -2 and rechits None?\n",
    "        # Criteria for tracks is to check if -2 is in the track_tp_index\n",
    "        # Because default match to tp index value is -2\n",
    "        if item_type_ == 'track':\n",
    "            tp_criteria_ = [(-2 not in list_) for list_ in event_df_[item_tp_index_]]\n",
    "        \n",
    "        # Criteria for rechits is to check if length of rechit_tp_index is greater than 0\n",
    "        # Because default match to tp index is none\n",
    "        elif item_type_ == 'rechit':\n",
    "            tp_criteria_ = [(len(list_) > 0) for list_ in event_df_[item_tp_index_]]\n",
    "            #print len(event_df_[tp_criteria_])\n",
    "        \n",
    "        item_count_dict_['tp_matched'].append(len(event_df_[tp_criteria_]))\n",
    "        item_id_dict_['tp_matched'].append(event_df_[tp_criteria_])\n",
    "        \n",
    "        # Criteria for filtering rechits matched to tracks based on 'track_ids' column\n",
    "        other_item_criteria_ = [len(list_) > 0 for list_ in event_df_[other_item_ids_]]\n",
    "        item_count_dict_[other_item_matched_].append(len(event_df_[other_item_criteria_]))\n",
    "    \n",
    "    return item_count_dict_, item_id_dict_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_count_, track_ids_ = count_matched_items('track')\n",
    "rechit_count_, rechit_ids_ = count_matched_items('rechit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage for TF-DeepHGCal/PyTorch/Graph Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''DataFrame Documentation for Pandas states that writing and reading from msgpack is an experimental feature.\n",
    "It is to be released soon, but please use it with care to ensure data is not corrupted.\n",
    "\n",
    "Note: When working with large datasets (>1000 events), you will not be able to save the data.\n",
    "The filesize for rechit_global_df_ is 76 MB for 100 events thus 760 MB for 1000 events and so on.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Writing to serialized format fails in case of copied dataframes as the columns are sets\n",
    "\n",
    "Solution: Iterate over all Global DataFrames, find the columns to replace, \n",
    "and replace with lists instead of sets so that they are serializable\n",
    "'''\n",
    "'''\n",
    "for dataframe_ in [track_global_df_, track_param_global_df_, rechit_global_df_, rechit_param_global_df_]:\n",
    "    dataframe_to_update_ = dataframe_.copy(deep=True)\n",
    "    columns_to_replace_ = ['rechit_ids', 'rechit_local_ids', 'rechit_tp_index', 'track_tp_index', 'track_matches']\n",
    "    for column_name_ in columns_to_replace_:    \n",
    "        if column_name_ in dataframe_to_update_:\n",
    "            list_arr_ = []\n",
    "            for set_ in dataframe_to_update_[column_name_]:\n",
    "                list_arr_.append(list(set_))\n",
    "            dataframe_to_update_.update(pd.Series(list_arr_, name=column_name_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepHGCal/TFRecords Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RECHIT_LEN = 3600\n",
    "MAX_TRACK_LEN = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "'''This is the function to create graphs in the form readable by DeepHGCal'''\n",
    "\n",
    "data_dict_list_ = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Global Features are track-based so they vary in length per-event\n",
    "# We find the maximum number of tracks that correspond to max_len of global feature vector\n",
    "# Is it a good idea to zero-pad global feature vectors less than max_len?\n",
    "\n",
    "for event_id_ in range(number_of_events_):\n",
    "    data_dict_ = {}\n",
    "    senders_ = []\n",
    "    receivers_ = []\n",
    "    \n",
    "    track_event_df_ = track_global_df_[track_global_df_['event_id'] == event_id_]\n",
    "    track_param_df_ = track_param_global_df_.loc[track_event_df_['track_id']]\n",
    "    track_df_ = track_event_df_.merge(track_param_df_)\n",
    "\n",
    "    # Sort the tracks according to increasing track_eta and associate a label with each track\n",
    "    # This is done by resetting the track index based on increasing track_et\n",
    "    track_df_.sort_values('track_eta', ascending=True, inplace=True)\n",
    "    track_df_.index = pd.RangeIndex(len(track_df_.index))  \n",
    "\n",
    "    rechit_event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    rechit_param_df_ = rechit_param_global_df_[rechit_param_global_df_['event_id']==event_id_]\n",
    "    rechit_param_df_.index = pd.RangeIndex(len(rechit_param_df_.index))  \n",
    "    if len(rechit_event_df_) != len(rechit_param_df_):\n",
    "        print(\"Error - param data and event data are not of equal length!\")\n",
    "    \n",
    "    number_of_rechits_in_event_ = len(rechit_event_df_)\n",
    "    \n",
    "    # Set the node features as the track features that they belong to\n",
    "    node_indices_ = np.array(rechit_param_df_['rechit_local_id'].tolist()).astype(int)\n",
    "    node_labels_ = []\n",
    "    \n",
    "    # Originally, we were setting node-level features based on the nodes but that can be done\n",
    "    # for the test data set; instead here we can set the node-level features as the track features\n",
    "    # at least for training and \"learn\" the track-level features (eta) based on which we can cluster the nodes?\n",
    "    # The question still remains how do we initialize the edges ???\n",
    "    \n",
    "    # Update: Reverting to node-level features for each node as of now \n",
    "    # Modify it to combine some form of track-level features (target?)\n",
    "    \n",
    "    # Skip the event if it has no rechits\n",
    "    if len(node_indices_) == 0:\n",
    "        print(\"Event \", event_id_, \" has no rechits\" )\n",
    "        continue\n",
    "        \n",
    "    rechit_feature_vector_ = np.transpose(np.array([\n",
    "        rechit_param_df_['rechit_r'].tolist(),\n",
    "        rechit_param_df_['rechit_eta'].tolist(),\n",
    "        rechit_param_df_['rechit_phi'].tolist(),\n",
    "        rechit_param_df_['rechit_x'].tolist(),\n",
    "        rechit_param_df_['rechit_y'].tolist(),\n",
    "        rechit_param_df_['rechit_z'].tolist(),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "    ]))\n",
    "    \n",
    "    track_feature_vector_ = np.transpose(np.array([\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        track_df_['track_eta'].tolist(),\n",
    "        track_df_['track_phi'].tolist(),\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        track_df_['track_dsz'].tolist(),\n",
    "        track_df_['track_dxy'].tolist(),\n",
    "        track_df_['track_qoverp'].tolist(),\n",
    "        track_df_['track_pt'].tolist(),\n",
    "    ]))\n",
    "\n",
    "    node_feature_vector_ = np.vstack((rechit_feature_vector_, track_feature_vector_))\n",
    "    node_feature_vector_ = scaler.fit_transform(node_feature_vector_)    \n",
    "    \n",
    "    # Initialize an array of zeros as labels for the nodes\n",
    "    # Replace each zero with the int (label) associated with the track to which the rechit belongs\n",
    "    # This becomes the target label to identify for that training example\n",
    "    # Later, we will one_hot_encode the target label for working with the tensorflow model\n",
    "    node_label_array_ = [0] * len(rechit_feature_vector_)\n",
    "    track_labels_ = []\n",
    "    \n",
    "    # Associate a label with each rechit\n",
    "    for trk_idx_, row in track_df_.iterrows():\n",
    "        track_rechit_id_array_ = row.rechit_local_ids\n",
    "        # We use the rechit local index as a unique label for the track\n",
    "        # Lower indices are thus associated with lower track eta\n",
    "        for id_ in track_rechit_id_array_:\n",
    "            node_label_array_[int(id_)] = trk_idx_ + 1  # We can now use index '0' as a 'noise' label\n",
    "        \n",
    "        # Also associate a label with each track that we consider as a point among the data\n",
    "        track_labels_.append(trk_idx_ + 1)\n",
    "    \n",
    "    print(len(node_label_array_))   \n",
    "   \n",
    "    # Concatenate node and track labels into a single array\n",
    "    # Thereafter, concatenate everything into the node feature matrix \n",
    "    # Note that we duplicate the labels to work with off-the-shelf DeepHGCal Model\n",
    "    nla_ = np.concatenate(((np.array(node_label_array_)), np.array(track_labels_)), axis=0)  # add track labels here\n",
    "    # node_feature_vector_ = np.concatenate((rechit_feature_vector_, nla_[:, None], nla_[:, None]), axis = 1)\n",
    "    assert node_feature_vector_.shape[0] == nla_.shape[0], \"Labels are not equal to training nodes/examples\"\n",
    "    # data comprises of f features--here f is 6--and 1-d vector for labels (total f+1 columns)\n",
    "    # it has n rows of rechits making its shape n x f\n",
    "    # f is constant for all events but n varies with the event thus padding may be necessary\n",
    "    data_dict_ = {\n",
    "    \"data\": node_feature_vector_,\n",
    "    \"labels\": nla_,\n",
    "    }\n",
    "    \n",
    "    data_dict_list_.append(data_dict_)\n",
    "print(len(data_dict_list_), \"graphs generated from data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "'''Convert the dict into a tf.Example then use a proto_buffer to \n",
    "serialize it into a compatible format for TFRecords'''\n",
    "\n",
    "def create_tf_example(graph_dict=None, max_hits=None, max_tracks=None, set_one_hot_labels=False):\n",
    "    \"\"\"\n",
    "    :param graph_dict: dictionary with each key representing a feature vector\n",
    "    :param labels: list with \n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    if max_hits is None:\n",
    "        max_hits = 3600\n",
    "    \n",
    "    if graph_dict is None:\n",
    "        raise ValueError(\"No Rechit Feature Matrix provided\")\n",
    "\n",
    "    # TODO: Can we eliminate this padding? After all, graphs are meant to be dynamic structures, right?\n",
    "    # Presently, all features are concatenated into a matrix and converted to a tensor\n",
    "    if 'data' in graph_dict.keys():\n",
    "        original_data = graph_dict['data']\n",
    "        data_dimensions = original_data.shape[1]\n",
    "        # If number of points are less than max_hits threshold\n",
    "        if original_data.shape[0] < max_hits:\n",
    "            event_data_size = max_hits * data_dimensions\n",
    "            \n",
    "            # Reshape the original data into a 1D array\n",
    "            # TODO: Is np.ravel faster? Does it matter?\n",
    "            original_data = graph_dict['data'].reshape(-1).astype(np.float32)\n",
    "            # Pad the data with zeros to ensure the length of data from each event is constant\n",
    "            padded_data = np.concatenate((original_data, np.zeros(event_data_size - len(original_data))), axis=0)\n",
    "            padded_data = np.reshape(padded_data, (max_hits, data_dimensions))\n",
    "        else:\n",
    "            # Eliminate the first bits of the data (these are definitely rechits)\n",
    "            # The last part of the data is the tracks and we do not want to eliminate those\n",
    "            # Since they are useful as 'centroids' for the clustering\n",
    "            padded_data = original_data[-event_data_size:]\n",
    "    else:\n",
    "        raise ValueError(\"Key 'data' not found in rechit data dictionary\")\n",
    "    \n",
    "    # Handle the labels either separately as one-hot-encoded data or together\n",
    "    if 'labels' in graph_dict.keys():\n",
    "        node_labels = graph_dict['labels']\n",
    "        \n",
    "        # Zero-pad the labels (in the case of one-hot encoded labels, the '0' label\n",
    "        # corresponds to noisy points in the cloud)\n",
    "        if len(node_labels) > max_hits:\n",
    "            padded_labels = node_labels[-max_hits:]\n",
    "        else:\n",
    "            # Pad the data with zeros to ensure the length of data from each event is constant\n",
    "            padded_labels = np.concatenate((node_labels, np.zeros(max_hits - len(node_labels))))\n",
    "        \n",
    "        # Check if one-hot-encoded labels are desired\n",
    "        if set_one_hot_labels is True:\n",
    "            if max_tracks is None:\n",
    "                # Set default value of maximum tracks\n",
    "                # This will add as many dimensions to your data\n",
    "                max_tracks = 100\n",
    "            \n",
    "            # Create one-hot-encoded representation of the target labels for the data\n",
    "            target_labels = one_hot_encoder.fit_transform(np.array([padded_labels]).T).toarray()\n",
    "            # If there are more labels than max_tracks, just cut out the last set of labels\n",
    "            if target_labels.shape[1] > max_tracks:\n",
    "                target_labels = target_labels[:, :max_tracks]\n",
    "            else:\n",
    "                # Append zero-columns as padding to these data items\n",
    "                padding_columns = np.zeros((max_hits, max_tracks-target_labels.shape[1]))\n",
    "                target_labels = np.hstack((target_labels, padding_columns))\n",
    "        else:            \n",
    "            # Code to store regular labels in a duplicated array for DeepHGCal\n",
    "            target_labels = np.concatenate((padded_labels[:, None], padded_labels[:, None]), axis = 1)\n",
    "    else:\n",
    "        raise ValueError(\"Key 'labels' not found in rechit data dictionary\")\n",
    "    \n",
    "    # Once the labels and data is padded and concatenated, finalize the data into a 2D matrix\n",
    "    final_data = np.hstack((padded_data, target_labels))\n",
    "    print(final_data.shape)\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature_matrix = {}\n",
    "        \n",
    "    # We flatten this tensor and convert it into a FloatList that is then serialized\n",
    "    # Define the tf Feature to wrap the FloatList\n",
    "    feature_matrix['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=final_data.ravel()))\n",
    "        \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_matrix))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def _parse_function(example_proto, data_dimensions=None):\n",
    "    '''If you would like to read and parse the data stored in TFRecord format, \n",
    "    refer to the [Dev] Prototyping Graph Neural Networks Notebook'''\n",
    "    # max_tracks = 100; features = 10\n",
    "    if data_dimensions is None:\n",
    "        data_dimensions = (3600, 12)\n",
    "        \n",
    "    # Create a description of the features to be read from the TFRecord file(s).  \n",
    "    feature_description = {\n",
    "    'data': tf.FixedLenFeature(data_dimensions, tf.float32),\n",
    "    }\n",
    "\n",
    "    # Parse the input tf.Example proto using the Feature dictionary above.\n",
    "    return tf.parse_single_example(example_proto, feature_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write the TFRecord File'''\n",
    "\n",
    "with tf.python_io.TFRecordWriter('tfrecords/ttbar-500.tfrecord', \n",
    "                                 options=tf.python_io.TFRecordOptions(\n",
    "                                    tf.python_io.TFRecordCompressionType.GZIP)) as tfwriter:\n",
    "    for event_number_, data_record_ in enumerate(data_dict_list_):\n",
    "        tf_example_ = create_tf_example(data_record_)\n",
    "        tfwriter.write(tf_example_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
