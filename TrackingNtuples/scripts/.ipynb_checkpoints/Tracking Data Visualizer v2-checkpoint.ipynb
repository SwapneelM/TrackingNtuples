{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Keys in the Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_event_ = \"ttbar-100\"\n",
    "outfile_ = \"outfile-\" + gen_event_ + \".root\"\n",
    "data_ = uproot.open(outfile_)[\"ntuples\"][\"tree\"]\n",
    "# data_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Integrity of the Imported Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True \n",
      "Total 100 events\n"
     ]
    }
   ],
   "source": [
    "stereo_tp_idx_ = data_.array('stereoTPIndex')\n",
    "mono_tp_idx_ = data_.array('monoTPIndex')\n",
    "track_tp_idx_ = data_.array('trackTPIdx')\n",
    "\n",
    "# Check that both have been generated for the same number of events\n",
    "# Just for clarity\n",
    "print len(track_tp_idx_) == len(stereo_tp_idx_),\n",
    "print len(track_tp_idx_) == len(mono_tp_idx_),\n",
    "print \"\\nTotal\", len(track_tp_idx_), \"events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any tracks map to multiple tracking particles\n",
    "for i in range(len(track_tp_idx_)):\n",
    "    for track_tp_list_ in track_tp_idx_[i]:\n",
    "        if len(track_tp_list_) > 1:\n",
    "            print \"Track maps to multiple TPs in event\", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches and number of hits with those many TP matches\n",
      "{0: 166358, 1: 429164, 2: 4501, 3: 253, 4: 42, 5: 13, 6: 6, 7: 4, 8: 2, 9: 4}\n",
      "CPU times: user 5.21 s, sys: 47.2 ms, total: 5.25 s\n",
      "Wall time: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Check if any hits map to multiple tracking particles\n",
    "# It is NOT NECESSARY that these TPs map to actual tracks\n",
    "hit_tp_count_ = {}\n",
    "\n",
    "# Iterate over event length in mono and stereo rechits\n",
    "for event_ in range(len(stereo_tp_idx_)):\n",
    "    for stereo_tp_list_ in stereo_tp_idx_[event_]:\n",
    "        tp_len_ = len(stereo_tp_list_)\n",
    "        # Add to a dictionary of <num of TP matches : hit count>\n",
    "        if tp_len_ in hit_tp_count_:\n",
    "            hit_tp_count_[tp_len_] += 1\n",
    "        else:\n",
    "            hit_tp_count_[tp_len_] = 1\n",
    "    \n",
    "    for mono_tp_list_ in mono_tp_idx_[event_]:\n",
    "        tp_len_ = len(mono_tp_list_)\n",
    "        # Add to a dictionary of <num of TP matches : hit count>\n",
    "        if tp_len_ in hit_tp_count_:\n",
    "            hit_tp_count_[tp_len_] += 1\n",
    "        else:\n",
    "            hit_tp_count_[tp_len_] = 1\n",
    "\n",
    "# This prints how many hits map to multiple matches\n",
    "# <num matches to TPs: ncount of hits>\n",
    "print \"Number of matches and number of hits with those many TP matches\\n\", hit_tp_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45849, 1: 119352, 2: 1346, 3: 87, 4: 16, 5: 3, 6: 3, 8: 1}\n",
      "CPU times: user 70.8 ms, sys: 3.94 ms, total: 74.7 ms\n",
      "Wall time: 78.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Correlate the above data to confirm the dataframe is correct\n",
    "# TODO: Delete above code block as it takes too much time\n",
    "hit_tp_count_ = {}\n",
    "\n",
    "for id_, tp_idx_list_ in rechit_global_df_[\"rechit_tp_index\"].items():\n",
    "    tp_len_ = len(tp_idx_list_)\n",
    "    if tp_len_ in hit_tp_count_:\n",
    "        hit_tp_count_[tp_len_] += 1\n",
    "    else:\n",
    "        hit_tp_count_[tp_len_] = 1\n",
    "print hit_tp_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check that the same number of entries are recorded for stereo rechits\n",
    "concat_stereo_tp_idx_ = []\n",
    "for i in range(len(stereo_tp_idx_)):\n",
    "    concat_stereo_tp_idx_.extend(stereo_tp_idx_[i])\n",
    "    \n",
    "print len(concat_stereo_tp_idx_) == len(concat_stereo_param_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_tp_idx_ = data_.array('trackTPIdx')\n",
    "mono_tp_idx_ = data_.array('monoTPIndex')\n",
    "stereo_tp_idx_ = data_.array('stereoTPIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_set(input_array_):\n",
    "    '''\n",
    "    Format: 3-level nested lists - [[[...] ...] ...]\n",
    "    '''\n",
    "    output_array_ = []\n",
    "    for index_ in range(len(input_array_)):\n",
    "        output_array_.append([])\n",
    "        for second_list_ in input_array_[index_]:\n",
    "            output_array_[index_].append(set(second_list_))\n",
    "    return output_array_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "mono_tp_idx_set_ = list_to_set(mono_tp_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "track_tp_idx_ = data_.array('trackTPIdx')\n",
    "mono_tp_idx_ = data_.array('monoTPIndex')\n",
    "len(track_tp_idx_)\n",
    "\n",
    "#track_tp_match_map_ = {}\n",
    "\n",
    "for event_ in range(len(track_tp_idx_)):\n",
    "    for tp_list_ in track_tp_idx_[event_]:\n",
    "        if len(tp_list_) > 1:\n",
    "            print event_, \"has a track with multiple TP matches\"\n",
    "        for mono_tp_list_ in mono_tp_idx_set_[event_]:\n",
    "            if tp_list_[0] in mono_tp_list_:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checking if each item in the lists is converted to the set\n",
    "%%time\n",
    "for i in range(25):\n",
    "    for j in range(len(mono_tp_idx_[i])):\n",
    "        for x in mono_tp_idx_[i][j]:\n",
    "            if x not in mono_tp_idx_set_[i][j]:\n",
    "                print 'Problem', i, j\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Load the track parameters into the respective arrays to be added into the rechit_param_global dataframe\n",
    "'''\n",
    "\n",
    "rechit_cartesian_ = OrderedDict({})\n",
    "for key in ['stereoHitX', 'stereoHitY', 'stereoHitZ', 'monoHitX', 'monoHitY', 'monoHitZ']:\n",
    "    rechit_cartesian_[key] = data_.array(key)\n",
    "\n",
    "rechit_polar_ = OrderedDict({})\n",
    "for key in ['stereoHitR', 'stereoHitEta', 'stereoHitPhi', 'monoHitR', 'monoHitEta', 'monoHitPhi']:\n",
    "    rechit_polar_[key] = data_.array(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 1: Reformat List of Indices to Sets of Indices for each Rechit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all tracking particle index lists to sets for faster search\n",
    "\n",
    "mono_tp_idx_set_ = list_to_set(data_.array('monoTPIndex'))\n",
    "stereo_tp_idx_set_ = list_to_set(data_.array('stereoTPIndex'))\n",
    "track_tp_idx_set_ = list_to_set(data_.array('trackTPIdx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Add all data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Dataframe column titles and datatypes\n",
    "\n",
    ":event_id: int\n",
    ":rechit_id: int\n",
    ":track_id: int\n",
    ":rechit_ids: list(int)\n",
    ":track_ids: list(int)\n",
    ":matched/unmatched_track_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":rechit_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":match_count: int  # count the number of rechits/tracks matched to the track/rechit\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Adding stereo and mono rechit data into a global dataframe\n",
    "\n",
    ":stereo/mono_tp_index_: event-based list of rechit-based list of sets of int (tp_index)\n",
    "\n",
    "'''\n",
    "def create_global_rechit_df(stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_dict_, rechit_polar_dict_):\n",
    "    rechit_global_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                      'track_matches': [], 'match_count': []})\n",
    "    rechit_param_global_map_ = OrderedDict({'rechit_id':[], 'rechit_x': [], 'rechit_y': [], 'rechit_z': [], \n",
    "                                            'rechit_r': [], 'rechit_phi': [], 'rechit_eta': []})\n",
    "    global_counter_ = 0\n",
    "    \n",
    "    if len(stereo_tp_idx_) != len(stereo_tp_idx_):\n",
    "        raise ValueError('Rechit arrays represent differing event lengths [stereo, mono]:', len(stereo_tp_idx_), len(mono_tp_idx_))\n",
    "    \n",
    "    for event_id_ in range(len(stereo_tp_idx_)):\n",
    "        # Count the number of rechits in that event\n",
    "        event_rechit_count_ = len(stereo_tp_idx_[event_id_]) + len(mono_tp_idx_[event_id_])\n",
    "\n",
    "        rechit_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        # appends SAME instance of [event_id] event_rechit_count_ times\n",
    "        \n",
    "        rechit_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))     \n",
    "        rechit_global_map_['rechit_tp_index'].extend(stereo_tp_idx_[event_id_])\n",
    "        rechit_global_map_['rechit_tp_index'].extend(mono_tp_idx_[event_id_])\n",
    "        rechit_global_map_['track_matches'].extend([] for _ in range(event_rechit_count_))\n",
    "        rechit_global_map_['match_count'].extend(0 for _ in range(event_rechit_count_))\n",
    "        global_counter_ += event_rechit_count_\n",
    "        \n",
    "        # Extend the hit_param_global_map_ with rechit parameters\n",
    "        rechit_param_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))\n",
    "        \n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['stereoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['monoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['stereoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['monoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['stereoHitZ'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['monoHitZ'][event_id_])\n",
    "        \n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['stereoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['monoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['stereoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['monoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['stereoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['monoHitEta'][event_id_])\n",
    "        \n",
    "    # Convert dict to dataframe\n",
    "    rechit_global_df_ = df.from_dict(rechit_global_map_)\n",
    "    rechit_param_global_df_ = df.from_dict(rechit_param_global_map_)\n",
    "    return rechit_global_df_, rechit_param_global_df_\n",
    "    \n",
    "# Check Memory Usage of DataFrame\n",
    "# print rechit_global_df_.memory_usage(deep=True)\n",
    "# print rechit_param_global_df_.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Create the Global Rechit Array and Global Rechit Parameters Array'''\n",
    "rechit_global_df_, rechit_param_global_df_ = create_global_rechit_df(\n",
    "    stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_, rechit_polar_)\n",
    "# print rechit_global_df_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Check if/where multiple tracking particle indices occur in the rechits\n",
    "This is to verify the data stored in the dict is ordered correctly\n",
    "'''\n",
    "'''\n",
    "global_counter_ = 0\n",
    "\n",
    "for i in range(len(stereo_tp_idx_)):\n",
    "    for j in range(len(stereo_tp_idx_[i])):\n",
    "        \n",
    "        # If multiple matches are found in the rechits\n",
    "        if len(stereo_tp_idx_[i][j]) > 1:\n",
    "            print \"Multiple Matches found at event\", i, \"rechit\", j\n",
    "            if rechit_global_map_[\"unmatched_track_tp_index\"][global_counter_] == stereo_tp_idx_[i][j]:\n",
    "                continue\n",
    "            else:\n",
    "                print \"Rechit TP Index is not being stored completely/correctly in the global map\"\n",
    "        global_counter_ += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the Tracks to Rechits in a Global Array of Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Match Rechits to Tracks.\n",
    "Create the Global Track Array and Global Track Parameter Array.\n",
    "'''\n",
    "\n",
    "# Initialize the Global Track Parameter Map\n",
    "track_param_global_map_ = OrderedDict({})\n",
    "for key in ['track_id', 'track_eta', 'track_phi', 'track_qoverp', 'track_dxy', 'track_dsz', 'track_pt']:\n",
    "    track_param_global_map_[key] = []\n",
    "    \n",
    "# Define the dictionaries to be cast into dataframes\n",
    "track_to_rechit_map_ = OrderedDict({\"event_id\": [], \"track_id\": [], \"track_tp_index\": [], \"rechit_ids\": [], \"match_count\": []})\n",
    "\n",
    "# Future Requirement?\n",
    "rechit_to_track_map_ = OrderedDict({\"event_id\": [], \"rechit_id\": [], \"rechit_tp_index\": [], \"track_ids\": [], \"match_count\": []})\n",
    "\n",
    "# Initialize the Global Track ID\n",
    "global_track_id_ = 0\n",
    "\n",
    "for event_id_ in range(len(track_tp_idx_)):\n",
    "    \n",
    "    num_tracks_in_event_ = len(track_tp_idx_[event_id_])\n",
    "\n",
    "    # Add track data to the dict in an efficient manner\n",
    "    track_to_rechit_map_[\"event_id\"].extend([event_id_] * num_tracks_in_event_)\n",
    "    \n",
    "    global_track_id_range_ = range(global_track_id_, global_track_id_ + num_tracks_in_event_)\n",
    "    \n",
    "    track_to_rechit_map_[\"track_id\"].extend(global_track_id_range_)\n",
    "    track_to_rechit_map_[\"track_tp_index\"].extend(track_tp_idx_[event_id_])\n",
    "    \n",
    "    # Append multiple empty lists in place of the values not filled yet\n",
    "    track_to_rechit_map_[\"match_count\"].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_[\"rechit_ids\"].extend([] for _ in range(num_tracks_in_event_))\n",
    "    \n",
    "    # Fill in the Global Track Parameters\n",
    "    track_param_global_map_[\"track_id\"].extend(global_track_id_range_)\n",
    "    track_param_global_map_[\"track_eta\"].extend(data_.array('trackEta')[event_id_])\n",
    "    track_param_global_map_[\"track_phi\"].extend(data_.array('trackPhi')[event_id_])\n",
    "    track_param_global_map_[\"track_pt\"].extend(data_.array('trackPt')[event_id_])\n",
    "    track_param_global_map_[\"track_qoverp\"].extend(data_.array('qoverp')[event_id_])\n",
    "    track_param_global_map_[\"track_dxy\"].extend(data_.array('dxy')[event_id_])\n",
    "    track_param_global_map_[\"track_dsz\"].extend(data_.array('dsz')[event_id_])\n",
    "    \n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    \n",
    "    # Check the TPs matched to tracks and find rechits for each TP (Stereo and Mono)\n",
    "    for track_tp_list_ in track_tp_idx_[event_id_]:\n",
    "        rechit_matches_ = []\n",
    "        \n",
    "        if len(track_tp_list_) <= 1:\n",
    "\n",
    "            # Iterate over the index and values of each rechit tp index list\n",
    "            for idx_, tp_idx_list_ in event_df_['rechit_tp_index'].items():\n",
    "            \n",
    "                # Find the match for the first tp index in the track tp list\n",
    "                if track_tp_list_[0] in tp_idx_list_:\n",
    "                    rechit_matches_.append(event_df_['rechit_id'][idx_])\n",
    "                \n",
    "                    # Append the global track id to the rechit\n",
    "                    event_df_.loc[idx_, ('track_matches')].append(global_track_id_)\n",
    "                    \n",
    "            track_to_rechit_map_[\"match_count\"][global_track_id_] = len(rechit_matches_)\n",
    "            track_to_rechit_map_[\"rechit_ids\"][global_track_id_] = set(rechit_matches_)\n",
    "        \n",
    "        # If track has multiple tp indices, pick the one with the most hits\n",
    "\n",
    "        # Note: This approach fails if the tp index with more rechit matches has more \n",
    "        # 'common' hits with other tracks and is later discarded due to the common hits \n",
    "        # belonging to other tracks\n",
    "        if len(track_tp_list_) > 1:\n",
    "            rechit_matches_array_ = []\n",
    "            match_count_array_ = []\n",
    "            \n",
    "            print \"Found multiple TP indices in event\", event_id_, \"for global track\", \n",
    "            print global_track_id_, track_tp_list_\n",
    "            \n",
    "            for track_idx_ in track_tp_list_:\n",
    "                rechit_matches_ = []\n",
    "                \n",
    "                # Iterate over the index and values of each rechit tp index list\n",
    "                for idx_, tp_idx_list_ in event_df_['rechit_tp_index'].items():\n",
    "                    if track_idx_ in tp_idx_list_:\n",
    "                        rechit_matches_.append(event_df_['rechit_id'][idx_])\n",
    "                        # Append the global track id to the rechit\n",
    "                        # Append the global track id to the rechit\n",
    "                        event_df_.loc[idx_, ('track_matches')].append(global_track_id_)\n",
    "                        \n",
    "                rechit_matches_array_.append(rechit_matches_)\n",
    "                match_count_array_.append(len(rechit_matches_))\n",
    "            \n",
    "            # Store the global rechit ids and count of matches in a temporary list\n",
    "            for key, value in zip(match_count_array_, rechit_matches_array_):\n",
    "                tmp_dict_.append((key, value))\n",
    "            \n",
    "            # Pick the largest number of matches and corresponding global rechit ids\n",
    "            tmp_dict_ = sorted(tmp_dict_, reverse=True)\n",
    "            track_to_rechit_map_[\"match_count\"][global_track_id_] = tmp_dict_[0][0]\n",
    "            track_to_rechit_map_[\"rechit_ids\"][global_track_id_] = tmp_dict_[0][1]\n",
    "\n",
    "        # Check duplicates\n",
    "        if len(set(rechit_matches_)) < len(rechit_matches_):\n",
    "            raise ValueError('rechit_matches_ has duplicate values: Some Rechits are being matched twice!')\n",
    "        \n",
    "        # Increment the Global Track ID\n",
    "        global_track_id_ += 1\n",
    "    rechit_global_df_.update(event_df_, join='left')\n",
    "    track_param_global_df_ = df.from_dict(track_param_global_map_)\n",
    "track_global_df_ = df.from_dict(track_to_rechit_map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Update the match_count for rechits based on the number of matched tp_indices\n",
    "'''\n",
    "match_count_tmp_dict_ = OrderedDict({'match_count': [len(l) for l in rechit_global_df_['track_matches']]})\n",
    "print \"Maximum tracks matched for one particle:\", max(match_count_tmp_dict_['match_count'])\n",
    "\n",
    "rechit_global_df_.update(df.from_dict(match_count_tmp_dict_))\n",
    "#print rechit_global_df_[rechit_global_df_['match_count'] > 1]['event_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance of df.loc versus multi-index retrieval [ ][ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the data for testing\n",
    "test_dict_ = {'sample_column':[], 'sample_column_copy':[], 'sample_column_copy_1':[]}\n",
    "\n",
    "global_track_id_ = 999\n",
    "for idx_ in range(1000):\n",
    "    test_dict_['sample_column'].append([global_track_id_])\n",
    "    test_dict_['sample_column_copy'].append([global_track_id_])\n",
    "    test_dict_['sample_column_copy_1'].append([global_track_id_])\n",
    "test_df_ = df.from_dict(test_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for idx_ in range(1000):\n",
    "    test_df_['sample_column'][idx_].append(998)\n",
    "# print test_df_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for idx_ in range(1000):\n",
    "    test_df_.loc[idx_, ('sample_column')].append(998)\n",
    "# print test_df_.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data stored in the track_to_rechit_map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in track_to_rechit_map_.keys():\n",
    "    print key, \":\", len(track_to_rechit_map_[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_to_rechit_df_ = df.from_dict(track_to_rechit_map_)\n",
    "track_to_rechit_df_.head(10)\n",
    "\n",
    "# Calculate the average number of hits per track\n",
    "average_rechits_per_track_ = 0\n",
    "len_array_ = []\n",
    "for rechit_list_ in track_to_rechit_df_['rechit_ids']:\n",
    "    average_rechits_per_track_ += len(rechit_list_)\n",
    "    len_array_.append(len(rechit_list_))\n",
    "\n",
    "print \"Average Rechits per track:\", average_rechits_per_track_/len(track_to_rechit_df_['rechit_ids'])\n",
    "print \"Max. matched hits to track:\", max(len_array_), \"; Global track id:\", len_array_.index(85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to check if the correct tp index has been matched\n",
    "# Change the value of 'trk_id_' to any track that you know has some hits\n",
    "trk_id_ = 5\n",
    "# print track_to_rechit_df_.loc[trk_id_]\n",
    "for rechit_id in track_to_rechit_df_.loc[trk_id_]['rechit_ids']:\n",
    "    for track_idx_ in track_to_rechit_df_.loc[trk_id_]['track_tp_index']:\n",
    "        if track_idx_ in rechit_global_df_.loc[rechit_id]['rechit_tp_index']:\n",
    "            continue\n",
    "        else:\n",
    "            print \"Error: Track and rechit TP index does not match!\"\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_ = plt.figure()\n",
    "#ax_ = Axes3D(fig_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Number of Events based on any property length\n",
    "track_eta_ = data_.array(\"trackEta\")\n",
    "print \"Number of Events: \", len(track_eta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Matched/Unmatched Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Check that the count of hits entered in the dataframe match the stereo/mono hits\n",
    "Count the number of matched, unmatched, and total rechits in the dataframe (PER EVENT)\n",
    "'''\n",
    "\n",
    "# TODO: Create a function of the following code that can be used for both tracks and hits\n",
    "# Add a switch case for the 'assert' statements that skips\n",
    "# the mono+stereo hit-sum assertion if it works with tracks instead of rechits\n",
    "\n",
    "if len(stereo_tp_idx_) != len(mono_tp_idx_):\n",
    "    raise ValueError(\"Number of events in stereo and mono rechits do not match!\")\n",
    "\n",
    "# Initialize one array for counts and one for ids of matched/unmatched rechits\n",
    "rechit_count_ = OrderedDict({'matched':[], 'unmatched':[], 'total':[]})\n",
    "rechit_ids_ = OrderedDict({'matched':[], 'unmatched':[]})\n",
    "\n",
    "for event_id_ in range(len(stereo_tp_idx_)):\n",
    "    # Create a slice of the dataframe with the data for that event\n",
    "    event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    \n",
    "    # Count the number of matched, unmatched, and total rechits \n",
    "    num_matched_hits_ = sum(event_df_['match_count'] > 0)\n",
    "    num_unmatched_hits_ = sum(event_df_['match_count'] == 0)\n",
    "    num_total_hits_ = event_df_.shape[0]  # number of rows/rechits in the event\n",
    "    \n",
    "    # Find and store the indices of matched and unmatched rechits\n",
    "    rechit_ids_['matched'].append(set(event_df_.loc[event_df_['match_count'] > 0, ('rechit_id')].tolist()))\n",
    "    rechit_ids_['unmatched'].append(set(event_df_.loc[event_df_['match_count'] == 0, ('rechit_id')].tolist()))\n",
    "\n",
    "    # Sanity checks to ensure data has been added into the dataframe corrrectly\n",
    "    assert num_total_hits_ == (num_matched_hits_ + num_unmatched_hits_), \\\n",
    "    \"Rechit counts (unmatched, matched, total) do not add up\"\n",
    "    \n",
    "    assert (len(stereo_tp_idx_[event_id_]) + len(mono_tp_idx_[event_id_])) == num_total_hits_, \\\n",
    "    \"Rechits in dataframe %d and stereo_tp_idx_ %d do not match\" % (num_total_hits_, len(stereo_tp_idx_[event_id_]))\n",
    "    \n",
    "    # Append the hit counts into the dataframe\n",
    "    rechit_count_['matched'].append(num_matched_hits_)\n",
    "    rechit_count_['unmatched'].append(num_unmatched_hits_)\n",
    "    rechit_count_['total'].append(num_total_hits_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Matched/Unmatched Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for key in rechit_count_.keys():\n",
    "    ax_ = plt.subplot()\n",
    "    ax_.hist(rechit_count_[key], histtype='stepfilled', bins=len(rechit_count_[key]), \n",
    "             orientation='vertical', alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Frequency')    \n",
    "    plt.xlabel('Count of ' + key + ' rechits')\n",
    "    plt.title(key + 'Rechit Distribution')\n",
    "    plt.savefig('plots/' + gen_event_ + '/rechits/' + key + 'rechits')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Check that track lengths in track_tp_idx_ matches track lengths per event in dataframe\n",
    "Select and store matched/unmatched tracks and their ids for further analysis\n",
    "'''\n",
    "\n",
    "# TODO: Create a function of the following code that can be used for both tracks and hits\n",
    "# Add a switch case for the 'assert' statements that skips\n",
    "# the mono+stereo hit-sum assertion if it works with tracks instead of rechits\n",
    "\n",
    "if len(track_tp_idx_) != len(stereo_tp_idx_):\n",
    "    raise ValueError(\"Tracks (%d) and Stereo Rechits (%d) show different event lengths\" % (len(track_tp_idx_), len(stereo_tp_idx_)))\n",
    "\n",
    "# Initialize track data dicts\n",
    "track_count_ = OrderedDict({'matched':[], 'unmatched':[], 'total':[]})\n",
    "track_ids_ = OrderedDict({'matched':[], 'unmatched':[]})\n",
    "\n",
    "for event_id_ in range(len(track_tp_idx_)):\n",
    "    # Create a slice of the dataframe with the data for that event\n",
    "    event_df_ = track_global_df_[track_global_df_['event_id']==event_id_]\n",
    "    \n",
    "    # Count the number of matched, unmatched, and total rechits \n",
    "    num_matched_tracks_ = sum(event_df_['match_count'] > 0)\n",
    "    num_unmatched_tracks_ = sum(event_df_['match_count'] == 0)\n",
    "    num_total_tracks_ = event_df_.shape[0]  # number of rows/rechits in the event\n",
    "        \n",
    "    # Find and store the indices of matched and unmatched rechits\n",
    "    track_ids_['matched'].append(set(event_df_.loc[event_df_['match_count'] > 0, ('track_id')].tolist()))\n",
    "    track_ids_['unmatched'].append(set(event_df_.loc[event_df_['match_count'] == 0, ('track_id')].tolist()))\n",
    "\n",
    "    # Sanity checks to ensure data has been added into the dataframe corrrectly\n",
    "    assert num_total_tracks_ == (num_matched_tracks_ + num_unmatched_tracks_), \\\n",
    "    \"Track counts (unmatched, matched, total) do not add up\"\n",
    "    \n",
    "    assert len(track_tp_idx_[event_id_]) == num_total_tracks_, \\\n",
    "    \"Tracks in dataframe %d and track_tp_idx_ %d do not match\" % (num_total_tracks_, len(track_tp_idx_[event_id_]))\n",
    "    \n",
    "    # Append the hit counts into the dataframe\n",
    "    track_count_['matched'].append(num_matched_tracks_)\n",
    "    track_count_['unmatched'].append(num_unmatched_tracks_)\n",
    "    track_count_['total'].append(num_total_tracks_)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for key in track_count_.keys():\n",
    "    ax_ = plt.subplot()\n",
    "    ax_.hist(track_count_[key], histtype='stepfilled', bins=len(track_count_[key]), orientation='vertical', alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Frequency')    \n",
    "    plt.xlabel('Count of ' + key + ' tracks')\n",
    "    plt.title(key + 'Track Distribution')\n",
    "    plt.savefig('plots/' + gen_event_ + '/track/' + key + 'tracks')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Calculate the difference in track and rechit eta per event\n",
    "Note: Gets Rechit Eta Values from Rechit Global DF\n",
    "'''\n",
    "\n",
    "# lists of delta-eta between tracks and matched rechits indexed by global_track_id\n",
    "# also contains the mean of these values stored in a separate array using index 'mean_difference'\n",
    "matched_track_eta_difference_ = {}\n",
    "\n",
    "for event_id_ in range(len(track_tp_idx_)):\n",
    "    #event_df_ = track_global_df_[track_global_df_['event_id']==event_id_]\n",
    "    # iterate over the selected tracks and the rechit ids matched to each track\n",
    "    matched_track_eta_difference_[event_id_] = {}\n",
    "    for track_id_ in track_ids_['matched'][event_id_]:\n",
    "        trk_eta_diff_ = []\n",
    "        track_id_ = int(track_id_)\n",
    "        # Retrieve eta for the track\n",
    "        trk_eta_ = track_param_global_df_.iloc[track_id_]['track_eta']\n",
    "        rechit_ids_for_track_ = track_global_df_.iloc[track_id_]['rechit_ids']\n",
    "        for rechit_id_ in rechit_ids_for_track_:\n",
    "            rechit_id_ = int(rechit_id_)\n",
    "            trk_eta_diff_.append(trk_eta_ - rechit_param_global_df_.iloc[rechit_id_]['rechit_eta'])\n",
    "        matched_track_eta_difference_[event_id_][track_id_] = trk_eta_diff_\n",
    "    matched_track_eta_difference_[event_id_]['mean_difference'] = [sum(diff_list_)/len(diff_list_) for diff_list_ in matched_track_eta_difference_[event_id_].values()] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "from matplotlib.colors import Colormap\n",
    "\n",
    "# TODO: Plot interactive histogram\n",
    "# plt.rc('axes', prop_cycle=(cycler('color', ['r', 'g', 'b', 'y']) + cycler('alpha', ['0.2', '0.4', '0.6', '0.8'])))\n",
    "# print rechit_param_global_df_.iloc[:50]['rechit_eta']\n",
    "\n",
    "for event_id_ in range(len(matched_track_eta_difference_.keys())):\n",
    "    ax_ = plt.subplot()\n",
    "    ax_.hist(matched_track_eta_difference_[event_id_]['mean_difference'], histtype='stepfilled', \n",
    "             bins=len(matched_track_eta_difference_[event_id_]['mean_difference'])+10, \n",
    "             orientation='vertical')\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Frequency')    \n",
    "    plt.xlabel('Rechit difference')\n",
    "    plt.title('Rechit vs. Track eta-difference Distribution for event ' + str(event_id_))\n",
    "    #plt.savefig('plots/' + gen_event_ + '/track/' + key + 'tracks')\n",
    "    #plt.show()\n",
    "    plt.pause(2)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Iterate over 5 track parameters and plot their distribution\n",
    "# Note: THIS IS LOG SCALE\n",
    "\n",
    "for key in track_param_global_df_.columns:\n",
    "    if key == 'track_id':\n",
    "        continue\n",
    "    ax_ = plt.subplot()\n",
    "    ax_.hist(track_param_global_df_[key], histtype='stepfilled', bins=100, orientation='vertical')\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Comment the next line for a linear scale\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.xlabel('Distribution of ' + key)\n",
    "    plt.title(key + ' Distribution')\n",
    "    plt.savefig('plots/' + gen_event_ + '/track/' + key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization discarding high-pt events for more clarity\n",
    "\n",
    "track_pt_ = data_.array('trackPt')\n",
    "concat_track_pt_ = []\n",
    "high_pt_events_ = {}\n",
    "\n",
    "for event_ in range(len(track_pt_)):\n",
    "    for trk_pt_val_ in track_pt_[event_]:\n",
    "        \n",
    "        # What is a reasonable general threshold for Track Pt?\n",
    "        if trk_pt_val_ < 30:\n",
    "            concat_track_pt_.append(trk_pt_val_)\n",
    "        else:\n",
    "            if event_ in high_pt_events_:\n",
    "                high_pt_events_[event_] += 1\n",
    "            else:\n",
    "                high_pt_events_[event_] = 1\n",
    "            concat_track_pt_.append(30)\n",
    "            \n",
    "print \"Iterating over track pt from\", len(concat_track_pt_), \"tracks\"\n",
    "\n",
    "plt.clf()\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(concat_track_pt_, histtype='stepfilled', bins=len(high_pt_events_.keys())*2, orientation='vertical')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Distribution of Track Pt')\n",
    "plt.title('Track Pt Distribution')\n",
    "plt.savefig('plots/' + gen_event_ + '/track/track-pt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events with high pt tracks and their distribution\n",
    "\n",
    "plt.clf()\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(high_pt_events_.keys(), weights=high_pt_events_.values(), bins=len(high_pt_events_.keys())*2, orientation='vertical')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Number of Tracks')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Event Number')\n",
    "plt.title('Distribution of High Pt Tracks')\n",
    "plt.savefig('plots/' + gen_event_ + '/track/high-pt-events')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input the range of values with odd track phi in case it is a specific part\n",
    "\n",
    "# Note: This is ONLY useful if there is a 'sub-portion' of the graph that is weird-looking\n",
    "# If it is oddly distributed throughout, an alternative means of visualization is needed\n",
    "\n",
    "track_phi_ = data_.array('trackPhi')\n",
    "concat_track_phi_ = []\n",
    "anomalous_phi_event_number_ = {}\n",
    "for event_ in range(len(track_phi_)):\n",
    "    \n",
    "    for phi_idx_ in range(len(track_phi_[event_])):\n",
    "        \n",
    "        if (track_phi_[event_][phi_idx_] >= -0.8) and (track_phi_[event_][phi_idx_] <= 2):\n",
    "            if event_ in anomalous_phi_event_number_:\n",
    "                anomalous_phi_event_number_[event_] += 1\n",
    "            else:\n",
    "                anomalous_phi_event_number_[event_] = 1\n",
    "            continue\n",
    "        else:    \n",
    "            concat_track_phi_.append(track_phi_[event_][phi_idx_])\n",
    "        \n",
    "print len(concat_track_phi_)\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(concat_track_phi_, histtype='stepfilled', bins=100, orientation='vertical')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Distribution of Track Phi')\n",
    "plt.title('Weird Track Phi Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram of events with oddly distributed track phi\n",
    "print anomalous_phi_event_number_\n",
    "odd_phi_count_ = anomalous_phi_event_number_.values()\n",
    "x_index_ = range(len(odd_phi_count_))\n",
    "# print anomalous_phi_event_number_\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(x_index_, weights=odd_phi_count_, bins=100, orientation='vertical')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Frequency of Odd Phi')\n",
    "plt.xlabel('Event Numbers')\n",
    "plt.title('Odd Track Phi Distribution across Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of tracks in events \n",
    "# using the length of any paramater array - e.g. track eta\n",
    "\n",
    "track_Eta_ = data_.array(\"trackEta\")\n",
    "concat_num_tracks_ = []\n",
    "\n",
    "for i in range(len(track_Eta_)):\n",
    "     concat_num_tracks_.append(len(track_Eta_[i]))\n",
    "print \"Added\", sum(concat_num_tracks_), \"tracks from\", len(concat_num_tracks_), \"events\"\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(concat_num_tracks_, histtype='bar', bins=80, align='mid', orientation='vertical')\n",
    "\n",
    "plt.xlabel('No. of Tracks')\n",
    "plt.ylabel('No. of Events')\n",
    "plt.title('Tracks vs. Events Distribution')\n",
    "plt.grid(True)\n",
    "plt.savefig('plots/' + gen_event_ + '/track/event-track-distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot SimHit Distribution in X and Y Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhit_x_ = data_.array('simhit_x')\n",
    "simhit_y_ = data_.array('simhit_y')\n",
    "simhit_z_ = data_.array('simhit_z')\n",
    "\n",
    "# Append all simhits into a single array for plotting\n",
    "if len(simhit_x_) == len(simhit_y_):\n",
    "    concat_simhit_x_ = []\n",
    "    concat_simhit_y_ = []\n",
    "\n",
    "    for i in range(len(simhit_x_)):\n",
    "        concat_simhit_x_.extend(simhit_x_[i])\n",
    "        concat_simhit_y_.extend(simhit_y_[i])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax_ = plt.subplot(1,1,1)\n",
    "# Plot the 2D Histogram for Simhits\n",
    "plt.title('Simhit X-Y Distribution')\n",
    "ax_.patch.set_facecolor('black')\n",
    "plt.hist2d(concat_simhit_x_, concat_simhit_y_, bins=500, norm=matplotlib.colors.LogNorm(), cmap='hot')\n",
    "plt.savefig('plots/' + gen_event_ + '/sim/hitdistribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ToDo: Add lognorm colormap\n",
    "\n",
    "mono_x_ = data_.array(\"monoHitX\") \n",
    "mono_y_ = data_.array(\"monoHitY\")\n",
    "mono_z_ = data_.array(\"monoHitZ\")\n",
    "\n",
    "# Append all mono rehits into a single array for plotting\n",
    "if len(mono_x_) == len(mono_y_):\n",
    "    concat_mono_x_ = []\n",
    "    concat_mono_y_ = []\n",
    "\n",
    "    for i in range(len(mono_x_)):\n",
    "        concat_mono_x_.extend(mono_x_[i])\n",
    "        concat_mono_y_.extend(mono_y_[i])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax_ = plt.subplot(1,1,1)\n",
    "# Plot the 2D Histogram for Mono Rechits\n",
    "plt.title('MonoRechit Distribution')\n",
    "ax_.patch.set_facecolor('black')\n",
    "plt.hist2d(concat_mono_x_, concat_mono_y_, bins=350, norm=matplotlib.colors.LogNorm(), cmap='hot')\n",
    "plt.savefig('plots/' + gen_event_ + '/stereo/rechitdistribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to explore the hole in 3 dimensions\n",
    "# Not quite possible to do manually?\n",
    "%matplotlib inline\n",
    "fig_ = plt.figure()\n",
    "ax_ = Axes3D(fig_)\n",
    "concat_mono_x_ = []\n",
    "concat_mono_y_ = []\n",
    "concat_mono_z_ = []\n",
    "\n",
    "for event_ in range(10):\n",
    "    for x, y, z, in zip(mono_x_[event_], mono_y_[event_], mono_z_[event_]):\n",
    "        if x < 55 and x > 15 and y > 10 and y < 50:\n",
    "            concat_mono_x_.append(x)\n",
    "            concat_mono_y_.append(y)\n",
    "            concat_mono_z_.append(z)   \n",
    "\n",
    "ax_.scatter3D(concat_mono_x_, concat_mono_y_, s=0.6)\n",
    "# ax_.scatter3D(concat_mono_x_, concat_mono_y_, concat_mono_z_, s=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "# Trying to explore the hole in 3 dimensions\n",
    "# Not quite possible to do manually?\n",
    "\n",
    "fig_ = plt.figure()\n",
    "ax_ = Axes3D(fig_)\n",
    "\n",
    "concat_simhit_x_ = []\n",
    "concat_simhit_y_ = []\n",
    "concat_simhit_z_ = []\n",
    "\n",
    "for event_ in range(8):\n",
    "    for x, y, z, in zip(simhit_x_[event_], simhit_y_[event_], simhit_z_[event_]):\n",
    "        if x < 55 and x > 15 and y > 10 and y < 50:\n",
    "            concat_simhit_x_.append(x)\n",
    "            concat_simhit_y_.append(y)\n",
    "            concat_simhit_z_.append(z)   \n",
    "\n",
    "ax_.scatter3D(concat_simhit_x_, concat_simhit_y_, s=0.6)  # Better visualization\n",
    "# ax_.scatter3D(concat_simhit_x_, concat_simhit_y_, concat_simhit_z_, s=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Define a loop that plots R, Phi, and Eta for Mono Hits\n",
    "position_ = 1\n",
    "for param in [\"monoHitR\", \"monoHitPhi\", \"monoHitEta\"]:\n",
    "    mono_param_ = data_.array(param)\n",
    "    concat_mono_param_ = []\n",
    "    \n",
    "    for i in range(len(mono_param_)):\n",
    "            concat_mono_param_.extend(mono_param_[i])\n",
    "\n",
    "    plt.clf()\n",
    "    fig, ax_ = plt.subplots(figsize=(8, 6))\n",
    "    position_ += 1\n",
    "    # Plot the 2D Histogram for Mono Rechits\n",
    "    ax_.set_title('MonoRechit Distribution of ' + param)\n",
    "    ax_.hist(concat_mono_param_, bins=50, histtype='stepfilled', align='mid', orientation='vertical', density=True)\n",
    "    plt.xlabel('Value of ' + param)\n",
    "    plt.ylabel('Distribution of ' + param)\n",
    "    plt.title(param + ' Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('plots/' + gen_event_ + '/mono/' + param)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# ToDo: Add lognorm colormap\n",
    "\n",
    "stereo_x_ = data_.array(\"stereoHitX\") \n",
    "stereo_y_ = data_.array(\"stereoHitY\")\n",
    "stereo_z_ = data_.array(\"stereoHitZ\")\n",
    "\n",
    "if len(mono_x_) == len(mono_y_):\n",
    "    concat_stereo_x_ = []\n",
    "    concat_stereo_y_ = []\n",
    "\n",
    "    for i in range(len(mono_x_)):\n",
    "        concat_stereo_x_.extend(stereo_x_[i])\n",
    "        concat_stereo_y_.extend(stereo_y_[i])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax_ = plt.subplot(1,1,1)\n",
    "# Plot the 2D Histogram for Mono Rechits\n",
    "ax_.set_title('Stereo Distribution')\n",
    "ax_.patch.set_facecolor('black')\n",
    "ax_.hist2d(concat_stereo_x_, concat_stereo_y_, bins=300, norm=matplotlib.colors.LogNorm(), cmap='hot')\n",
    "plt.savefig('plots/' + gen_event_ + '/stereo/rechitdistribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a loop that plots R, Phi, and Eta for Stereo Hits\n",
    "\n",
    "for param in [\"stereoHitR\", \"stereoHitPhi\", \"stereoHitEta\"]:\n",
    "    stereo_param_ = data_.array(param)\n",
    "    concat_stereo_param_ = []\n",
    "    \n",
    "    for i in range(len(mono_param_)):\n",
    "            concat_stereo_param_.extend(stereo_param_[i])\n",
    "\n",
    "    plt.clf()\n",
    "    fig, ax_ = plt.subplots(figsize=(8, 6))\n",
    "    position_ += 1\n",
    "    # Plot the 2D Histogram for Mono Rechits\n",
    "    ax_.set_title('StereoRechit Distribution of ' + param)\n",
    "    ax_.hist(concat_stereo_param_, bins=50, histtype='stepfilled', align='mid', orientation='vertical', density=True)\n",
    "    plt.xlabel('Value of ' + param)\n",
    "    plt.ylabel('Distribution of ' + param)\n",
    "    plt.title(param + ' Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('plots/' + gen_event_ + '/stereo/' + param)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
