{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of events in data\n",
    "number_of_events_ = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the DataFrames Stored during Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refer to Data Preprocessing Notebook to Understand the Semantics and Column Headers'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Refer to Data Preprocessing Notebook to Understand the Semantics and Column Headers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.02 s, sys: 142 ms, total: 2.16 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "track_global_df_ = pd.read_msgpack('data/track_global_df_.msgpack')\n",
    "track_param_global_df_ = pd.read_msgpack('data/track_param_global_df_.msgpack')\n",
    "rechit_global_df_ = pd.read_msgpack('data/rechit_global_df_.msgpack')\n",
    "rechit_param_global_df_ = pd.read_msgpack('data/rechit_param_global_df_.msgpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.3 ms, sys: 21 ms, total: 108 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "track_count_ = pd.DataFrame.to_dict(pd.read_csv('data/track_count_.csv'))\n",
    "rechit_count_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_count_.csv'))\n",
    "track_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/track_ids_.csv'))\n",
    "rechit_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_ids_.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Adjacency Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGraphs are created on a per-event basis. We build an adjacency matrix of rechits for each individual event.\\nThe rechit connections are defined by the track that they belong to.\\nThe target label for each node is taken as the first tp_index in its rechit_tp_list\\n\\n#TODO: Incorporate a more flexible labeling schema - can you label the edges using the 'extra' tp index?\\nCan this learn more interesting structures for the graph(s)?\\n# Solution: Use the same node in different graphs - same as above but implementation-wise easier to do.\\n\\n#TODO: What information do we use to weight the edges in the graph?\\nDifferences in rechit parameters?\\nRechit vs. Track Parameters?\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Graphs are created on a per-event basis. We build an adjacency matrix of rechits for each individual event.\n",
    "The rechit connections are defined by the track that they belong to.\n",
    "The target label for each node is taken as the first tp_index in its rechit_tp_list\n",
    "\n",
    "#TODO: Incorporate a more flexible labeling schema - can you label the edges using the 'extra' tp index?\n",
    "Can this learn more interesting structures for the graph(s)?\n",
    "# Solution: Use the same node in different graphs - same as above but implementation-wise easier to do.\n",
    "\n",
    "#TODO: What information do we use to weight the edges in the graph?\n",
    "Differences in rechit parameters?\n",
    "Rechit vs. Track Parameters?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uncut_rechit_ids_ = np.load('data/uncut_rechit_ids_.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_features_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'node_features_' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "'''TODO: Convert data to string instead of bytes and check overheads'''\n",
    "\n",
    "edge_src_ = []\n",
    "edge_dest_ = []\n",
    "\n",
    "x = []\n",
    "\n",
    "data_dict = {}\n",
    "for event_id_ in range(number_of_events_):\n",
    "    track_event_df_ = track_global_df_[track_global_df_[b'event_id'] == event_id_]\n",
    "    number_of_rechits_in_event_ = len(rechit_global_df_[rechit_global_df_[b'event_id']==event_id_])\n",
    "    \n",
    "    # Retrieve a set of valid rechits arranged in event-wise lists\n",
    "    valid_hits_ = set(uncut_rechit_ids_[event_id_])\n",
    "    node_labels_ = []\n",
    "    node_features_ = []\n",
    "    # For each track, append to the list of source nodes, destination nodes, and node feature vectors\n",
    "    for track_rechit_id_array_ in track_event_df_[b'rechit_ids']:\n",
    "        src_vertices_ = []\n",
    "        dest_vertices_ = []\n",
    "        # Filter the rechits based on the uncut rechit ids segregated event-wise for faster searching\n",
    "        final_rechits_ = [i for i in track_rechit_id_array_ if i in valid_hits_]\n",
    "        # Sort the rechits based on values of Rechit R\n",
    "        # Start track building from the inside and move all the way outside\n",
    "        final_rechits_ = sorted(final_rechits_, key=lambda hit_: rechit_param_global_df_.iloc[int(hit_)][b'rechit_r'])\n",
    "        if len(final_rechits_) < 2:\n",
    "            continue\n",
    "        elif len(final_rechits_) == 2:\n",
    "            src_vertices_.append(final_rechits_[0])\n",
    "            dest_vertices_.append(final_rechits_[1])\n",
    "        # In order to extend this to 2 skip-connections (expanding to the assumption that 3 hits can be \n",
    "        # on the same layer, thus all of them should be connected to a hit on the next layer)\n",
    "        # Create another else case for len(final_rechits_) == 3: and add the corresponding vertices\n",
    "        # to src and dest arrays. Then you can modify the addition procedure to include src+[3:] and dest+[:-3]\n",
    "        # So you will have (1,2), (1,3), and (1,4) edges as a simple example of adding 2-skip-connections\n",
    "        else:\n",
    "            # Add the edges starting from node a and going to both a+1 and a+2\n",
    "            # We define this as 1-skip-connection because hits might lie on the same layer\n",
    "            # We originally sort them by the radius to ensure skip-connections have a meaning\n",
    "            src_vertices_.extend(final_rechits_[:-1]+final_rechits_[:-2])\n",
    "            dest_vertices_.extend(final_rechits_[1:]+final_rechits_[2:])\n",
    "        \n",
    "        for rechit_id_ in final_rechits_:\n",
    "            node_labels_.append([rechit_id_])\n",
    "            hit_df_ = rechit_param_global_df_.iloc[rechit_id_]\n",
    "            # Create the node feature vector using the physics data obtained from the simulation\n",
    "            # This can be modified to the parameters we would like to learn/find most important\n",
    "            node_features_.append(hit_df_[b'rechit_r'], hit_df_[b'rechit_phi'], hit_df_[b'rechit_eta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5dc0a7d8b5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMessagePassing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages with \"add\" aggregation.\n",
    "        return self.propagate('add', edge_index, x=x, num_nodes=x.size(0))\n",
    "\n",
    "    def message(self, x_j, edge_index, num_nodes):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, num_nodes, dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Sample Edge Label Definition for Rechits - Adjacency List?\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define the 2-layer GCN'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA available on cmg-gpu1080\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
