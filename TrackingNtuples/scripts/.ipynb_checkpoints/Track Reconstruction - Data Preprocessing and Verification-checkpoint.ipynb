{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "### Part I - Preprocessing\n",
    "---------------------------\n",
    "\n",
    "1. [View Keys in Root Data](#View-the-Keys-in-the-Imported-Data)\n",
    "\n",
    "2. [Optimisation Tests I: Python](#Optimisation-Tests)\n",
    "\n",
    "3. [Data Conversion using Uproot](#Load-Data-into-Arrays)\n",
    "\n",
    "4. Preprocessing\n",
    "\t- [Optimisation](#Preprocessing-1:-Reformat-List-of-Indices-to-Sets-of-Indices-for-each-Rechit)\n",
    "\t- [Convert to Dataframes](#Preprocessing-2:-Add-all-data-into-dataframes)\n",
    "\t- [Create Global Rechit Dataframe](#Create-a-Global-Dataframe-of-Rechits)\n",
    "\n",
    "5. [Rechit to Track Matching](#Match-the-Rechits-to-Tracks-and-Create-a-Global-Array-of-Tracks)\n",
    "\n",
    "6. [Optimisation Tests II: Dataframes](#Test-Performance-of-df.loc-versus-multi-index-retrieval-[-][-])\n",
    "\n",
    "\n",
    "### Part II: Raw Data Analysis and Plots\n",
    "-----------------------------------------\n",
    "\n",
    "1. [Count Data in Track to Rechit Map](#Analyse-the-data-stored-in-the-track_to_rechit_map_)\n",
    "\n",
    "2. [Generate Match Count Plots](#Generate-Plots)\n",
    "\t- [Counting Matched vs. Unmatched Rechits](#Analyse-Matched/Unmatched-Rechits)\n",
    "\t- [Plot Count of Track and TP Matched Rechits](#Plot-the-Rechits-Matched-to-TP,-Track,-or-Unmatched)\n",
    "\n",
    "3. Track Analysis\n",
    "\t- [Compare Eta between Tracks and their Rechits](#Compare-Track-and-Matched-Rechit-Eta)\n",
    "\t- [Plot Track Parameter Distribution Histograms](#Plot-Track-Parameters)\n",
    "\t- [Analyse/Filter High-Pt Events](#Filter-High-Pt-Events)\n",
    "\n",
    "4. Rechit/Simhit Analysis\n",
    "\t- [Plot Simhit Distribution 2D](#Plot-SimHit-Distribution-in-X-and-Y-Axes)\n",
    "\t- [Simhit Match Count](#Count-Simhits-Matched-to-Tracks)\n",
    "\t- [Realistic Geometry Simulation: Hole](#Plot-the-Hole-in-the-Data-(2D-Plot;-3D-Axes))\n",
    "\t- [Plot MonoRechit Distribution 2D](#Visualize-the-Mono-Rechits)\n",
    "\t- [Plot Rechit Parameter Distribution Histograms](#Plot-Rechit-Parameters)\n",
    "\t- [Plot StereoRechit Distribution 2D](#Visualize-the-Stereo-Rechits)\n",
    "\n",
    "5. Data Storage\n",
    "\t- [Verify Data is not Corrupted](#Testing-Integrity-of-internal-data-storage)\n",
    "\t- [Write Data to Serialized Output Format](#Data-Storage-for-TF/PyTorch/Graph-Library)\n",
    "\n",
    "\n",
    "### Part III: Filtered (Cut) Data Analysis and Plots\n",
    "--------------------------------------------------\n",
    "\n",
    "1. [Place Cuts on Rechits](#Place-the-Cuts-on-Rechits-=>-eta-(-0.9,-0.9))\n",
    "    - [Scatter Plot of Filtered Rechits](#Plot-the-Hits-without-Connections)\n",
    "\n",
    "2. [Plot 2D Rechit Parameters](#Plot-the-2D-Rechit-Parameters-for-Filtered-Hits)\n",
    "\n",
    "3. [Place Cuts on Tracks](#Place-the-cuts-on-tracks-by-Eta-and-Pt)\n",
    "\n",
    "4. [Plot only Filtered Rechits](#Plot-only-the-filtered-rechits)\n",
    "\n",
    "5. [Plot Matched/Unmatched Track Distribution](#Plot-Track-Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Keys in the Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_event_ = \"ttbar-10\"\n",
    "number_of_events_ = 10\n",
    "outfile_ = \"outfile-\" + gen_event_ + \".root\"\n",
    "data_ = uproot.open(outfile_)[\"ntuples\"][\"tree\"]\n",
    "# data_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Integrity of the Imported Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_tp_idx_ = data_.array('stereoTPIndex')\n",
    "mono_tp_idx_ = data_.array('monoTPIndex')\n",
    "track_tp_idx_ = data_.array('trackTPIndex')\n",
    "\n",
    "# Check that both have been generated for the same number of events\n",
    "# Just for clarity\n",
    "print len(track_tp_idx_) == len(stereo_tp_idx_),\n",
    "print len(track_tp_idx_) == len(mono_tp_idx_),\n",
    "print \"\\nTotal\", len(track_tp_idx_), \"events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if any tracks map to multiple tracking particles\n",
    "for i in range(len(track_tp_idx_)):\n",
    "    for track_tp_list_ in track_tp_idx_[i]:\n",
    "        if len(track_tp_list_) > 1:\n",
    "            print \"Track maps to multiple TPs in event\", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Check if any hits map to multiple tracking particles\n",
    "# It is NOT NECESSARY that these TPs map to actual tracks\n",
    "hit_tp_count_ = {}\n",
    "\n",
    "# Iterate over event length in mono and stereo rechits\n",
    "for event_ in range(len(stereo_tp_idx_)):\n",
    "    for stereo_tp_list_ in stereo_tp_idx_[event_]:\n",
    "        tp_len_ = len(stereo_tp_list_)\n",
    "        # Add to a dictionary of <num of TP matches : hit count>\n",
    "        if tp_len_ in hit_tp_count_:\n",
    "            hit_tp_count_[tp_len_] += 1\n",
    "        else:\n",
    "            hit_tp_count_[tp_len_] = 1\n",
    "    \n",
    "    for mono_tp_list_ in mono_tp_idx_[event_]:\n",
    "        tp_len_ = len(mono_tp_list_)\n",
    "        # Add to a dictionary of <num of TP matches : hit count>\n",
    "        if tp_len_ in hit_tp_count_:\n",
    "            hit_tp_count_[tp_len_] += 1\n",
    "        else:\n",
    "            hit_tp_count_[tp_len_] = 1\n",
    "\n",
    "# This prints how many hits map to multiple matches\n",
    "# <num matches to TPs: ncount of hits>\n",
    "print \"Number of matches and number of hits with those many TP matches\\n\", hit_tp_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_set(input_array_):\n",
    "    '''\n",
    "    Format: 3-level nested lists - [[[...] ...] ...]\n",
    "    '''\n",
    "    output_array_ = []\n",
    "    for index_ in range(len(input_array_)):\n",
    "        output_array_.append([])\n",
    "        for second_list_ in input_array_[index_]:\n",
    "            output_array_[index_].append(set(second_list_))\n",
    "    return output_array_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "mono_tp_idx_set_ = list_to_set(mono_tp_idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for event_ in range(len(track_tp_idx_)):\n",
    "    for tp_list_ in track_tp_idx_[event_]:\n",
    "        if len(tp_list_) > 1:\n",
    "            print event_, \" has a track with multiple TP matches\"\n",
    "        for mono_tp_list_ in mono_tp_idx_set_[event_]:\n",
    "            if tp_list_[0] in mono_tp_list_:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checking if each item in the lists is converted to the set\n",
    "%%time\n",
    "for i in range(25):\n",
    "    for j in range(len(mono_tp_idx_[i])):\n",
    "        for x in mono_tp_idx_[i][j]:\n",
    "            if x not in mono_tp_idx_set_[i][j]:\n",
    "                print 'Problem', i, j\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Load the track parameters into the respective arrays to be added into the rechit_param_global dataframe\n",
    "'''\n",
    "\n",
    "rechit_cartesian_ = OrderedDict({})\n",
    "for key in ['stereoHitX', 'stereoHitY', 'stereoHitZ', 'monoHitX', 'monoHitY', 'monoHitZ']:\n",
    "    rechit_cartesian_[key] = data_.array(key)\n",
    "\n",
    "rechit_polar_ = OrderedDict({})\n",
    "for key in ['stereoHitR', 'stereoHitEta', 'stereoHitPhi', 'monoHitR', 'monoHitEta', 'monoHitPhi']:\n",
    "    rechit_polar_[key] = data_.array(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 1: Reformat List of Indices to Sets of Indices for each Rechit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert all tracking particle index lists to sets for faster search\n",
    "\n",
    "mono_tp_idx_set_ = list_to_set(mono_tp_idx_)\n",
    "stereo_tp_idx_set_ = list_to_set(stereo_tp_idx_)\n",
    "track_tp_idx_set_ = list_to_set(track_tp_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Add all data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Global Dataframe of Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Adding stereo and mono rechit data into a global dataframe\n",
    "\n",
    ":event_id: int\n",
    ":rechit_id: int\n",
    ":track_id: int\n",
    ":rechit_ids: list(int)\n",
    ":track_ids: list(int)\n",
    ":track_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":rechit_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":match_count: int  # count the number of rechits/tracks matched to the given track/rechit\n",
    ":rechit_tp_index_: event-based list of rechit-based list of sets of int (tp_index)\n",
    "\n",
    "'''\n",
    "def create_global_rechit_df(stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_dict_, rechit_polar_dict_):\n",
    "    rechit_global_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                      'track_ids': [], 'match_count': [], 'rechit_local_id': []})\n",
    "    rechit_param_global_map_ = OrderedDict({'event_id': [], 'rechit_id':[], 'rechit_x': [], 'rechit_y': [], 'rechit_z': [], \n",
    "                                            'rechit_r': [], 'rechit_phi': [], 'rechit_eta': [], 'rechit_local_id': []})\n",
    "    global_counter_ = 0\n",
    "    \n",
    "    if len(stereo_tp_idx_) != len(stereo_tp_idx_):\n",
    "        raise ValueError('Rechit arrays represent differing event lengths [stereo, mono]:', len(stereo_tp_idx_), len(mono_tp_idx_))\n",
    "    \n",
    "    for event_id_ in range(len(stereo_tp_idx_)):\n",
    "        # Count the number of rechits in that event\n",
    "        event_rechit_count_ = len(stereo_tp_idx_[event_id_]) + len(mono_tp_idx_[event_id_])\n",
    "\n",
    "        rechit_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        # appends SAME instance of [event_id] event_rechit_count_ times\n",
    "        \n",
    "        rechit_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))     \n",
    "        rechit_global_map_['rechit_tp_index'].extend(stereo_tp_idx_[event_id_])\n",
    "        rechit_global_map_['rechit_tp_index'].extend(mono_tp_idx_[event_id_])\n",
    "        rechit_global_map_['track_ids'].extend([[] for _ in range(event_rechit_count_)])\n",
    "        rechit_global_map_['match_count'].extend([0 for _ in range(event_rechit_count_)])\n",
    "        rechit_global_map_['rechit_local_id'].extend(range(event_rechit_count_))\n",
    "        \n",
    "        # Extend the hit_param_global_map_ with rechit parameters\n",
    "        rechit_param_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))\n",
    "        rechit_param_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['stereoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['monoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['stereoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['monoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['stereoHitZ'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['monoHitZ'][event_id_])\n",
    "        \n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['stereoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['monoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['stereoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['monoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['stereoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['monoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_local_id'].extend(range(event_rechit_count_))\n",
    "        global_counter_ += event_rechit_count_\n",
    "    # Convert dict to dataframe\n",
    "    rechit_global_df_ = df.from_dict(rechit_global_map_)\n",
    "    rechit_param_global_df_ = df.from_dict(rechit_param_global_map_)\n",
    "    return rechit_global_df_, rechit_param_global_df_\n",
    "    \n",
    "# Check Memory Usage of DataFrame\n",
    "# print rechit_global_df_.memory_usage(deep=True)\n",
    "# print rechit_param_global_df_.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Create the Global Rechit Array and Global Rechit Parameters Array'''\n",
    "rechit_global_df_uncut_, rechit_param_global_df_uncut_ = create_global_rechit_df(\n",
    "    stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_, rechit_polar_)\n",
    "#print rechit_global_df_.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the Cuts (create DF for Graph Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''Check the maximum number of hits in an event'''\n",
    "max_len_ = 0 \n",
    "for i in range(number_of_events_):\n",
    "    len_idx_ = len(stereo_tp_idx_[i]) + len(mono_tp_idx_[i])\n",
    "    if len_idx_ > max_len_:\n",
    "        max_len_ = len_idx_\n",
    "        #print max_len_\n",
    "        \n",
    "print \"Maximum (unfiltered) hits in an event are: \", max_len_\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''This is done here to generate a reduced number of local indices for the tracks to match to rechits.\n",
    "We will replace the rechit_global_df_ generated above and used below for matches with this new dataframe.'''\n",
    "\n",
    "intermediate_df_ = rechit_param_global_df_uncut_[rechit_param_global_df_uncut_['rechit_eta'] <= 0.9]\n",
    "rechit_param_global_df_ = intermediate_df_[intermediate_df_['rechit_eta'] >= -0.9].copy()\n",
    "rechit_global_df_ = rechit_global_df_uncut_.iloc[rechit_param_global_df_['rechit_id']].copy()\n",
    "\n",
    "# Reset the Index of the Cut Dataframe that will become the new Global DataFrame\n",
    "# This will lose the former global rechit index - can this affect the analysis in the future?\n",
    "rechit_global_df_.index = pd.RangeIndex(len(rechit_global_df_.index))  \n",
    "rechit_param_global_df_.index = pd.RangeIndex(len(rechit_global_df_.index))  \n",
    "\n",
    "# Reset the local_rechit_ids for graph networks to have sequential nodes\n",
    "# And so that the node feature vector can be simpler to create sequentially\n",
    "rechit_local_id_dict_ = {'rechit_local_id' : []}\n",
    "# Find the minimum number of rechits in the final list of events\n",
    "min_num_of_rechits_ = 9999\n",
    "for event_id_ in range(number_of_events_):\n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    rechit_local_range_ = range(len(rechit_global_df_[rechit_global_df_['event_id']==event_id_]))\n",
    "    rechit_local_id_dict_['rechit_local_id'].extend(rechit_local_range_)\n",
    "    if rechit_local_range_[-1] < min_num_of_rechits_:\n",
    "        min_num_of_rechits_ = rechit_local_range_[-1]\n",
    "\n",
    "# Update the Global Rechit IDs\n",
    "rechit_global_id_dict_ = {}\n",
    "rechit_global_id_dict_['rechit_id'] = range(len(rechit_global_df_))\n",
    "rechit_global_df_.update(pd.DataFrame.from_dict(rechit_local_id_dict_))    \n",
    "rechit_param_global_df_.update(pd.DataFrame.from_dict(rechit_local_id_dict_))    \n",
    "\n",
    "# Update the Local Rechit IDs\n",
    "rechit_global_df_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))    \n",
    "rechit_param_global_df_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))\n",
    "\n",
    "print len(rechit_param_global_df_), \"of\", len(rechit_param_global_df_uncut_), \\\n",
    "float(len(rechit_param_global_df_))/float(len(rechit_global_df_uncut_)), \"hits remain\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(rechit_local_id_dict_['rechit_local_id']))\n",
    "for i in range(number_of_events_):\n",
    "    print(len(rechit_global_df_[rechit_global_df_['event_id']==i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the Rechits to Tracks and Create a Global Array of Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Match Rechits to Tracks.\n",
    "Create the Global Track Array and Global Track Parameter Array.\n",
    "'''\n",
    "# TODO: Refactor this to enable placing track cuts before forming dataframe and reduce processing by 75%\n",
    "# The 75% metric follows from: For 100 events track cuts reduce tracks by 75%\n",
    "# Initialize the Global Track Parameter Map\n",
    "track_param_global_map_ = OrderedDict({})\n",
    "for key in ['track_id', 'track_eta', 'track_phi', 'track_qoverp', 'track_dxy', 'track_dsz', 'track_pt']:\n",
    "    track_param_global_map_[key] = []\n",
    "    \n",
    "# Define the dictionaries to be cast into dataframes\n",
    "track_to_rechit_map_ = OrderedDict({'event_id': [], 'track_id': [], 'track_tp_index': [], \n",
    "                                    'rechit_ids': [], 'match_count': [], 'rechit_local_ids': []})\n",
    "\n",
    "# Future Requirement?\n",
    "rechit_to_track_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                    'track_ids': [], 'match_count': []})\n",
    "\n",
    "# Initialize the Global Track ID\n",
    "global_track_id_ = 0\n",
    "\n",
    "for event_id_ in range(len(track_tp_idx_)):\n",
    "    \n",
    "    num_tracks_in_event_ = len(track_tp_idx_[event_id_])\n",
    "\n",
    "    # Add track data to the dict in an efficient manner\n",
    "    track_to_rechit_map_['event_id'].extend([event_id_] * num_tracks_in_event_)\n",
    "    \n",
    "    global_track_id_range_ = range(global_track_id_, global_track_id_ + num_tracks_in_event_)\n",
    "    \n",
    "    track_to_rechit_map_['track_id'].extend(global_track_id_range_)\n",
    "    track_to_rechit_map_['track_tp_index'].extend(track_tp_idx_[event_id_])\n",
    "    \n",
    "    # Append multiple empty lists in place of the values not filled yet\n",
    "    track_to_rechit_map_['match_count'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_['rechit_ids'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_['rechit_local_ids'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    \n",
    "    # Fill in the Global Track Parameters\n",
    "    track_param_global_map_['track_id'].extend(global_track_id_range_)\n",
    "    track_param_global_map_['track_eta'].extend(data_.array('trackEta')[event_id_])\n",
    "    track_param_global_map_['track_phi'].extend(data_.array('trackPhi')[event_id_])\n",
    "    track_param_global_map_['track_pt'].extend(data_.array('trackPt')[event_id_])\n",
    "    track_param_global_map_['track_qoverp'].extend(data_.array('qoverp')[event_id_])\n",
    "    track_param_global_map_['track_dxy'].extend(data_.array('dxy')[event_id_])\n",
    "    track_param_global_map_['track_dsz'].extend(data_.array('dsz')[event_id_])\n",
    "    \n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    \n",
    "    # Check the TPs matched to tracks and find rechits for each TP (Stereo and Mono)\n",
    "    for track_tp_list_ in track_tp_idx_[event_id_]:\n",
    "        rechit_matches_ = []\n",
    "        rechit_local_matches_ = []\n",
    "        if len(track_tp_list_) == 0:\n",
    "            continue\n",
    "            \n",
    "        if len(track_tp_list_) == 1:\n",
    "\n",
    "            # Iterate over the index and values of each rechit tp index list\n",
    "            for (idx_, tp_idx_list_) in event_df_['rechit_tp_index'].iteritems():\n",
    "                # Find the match for the first tp index in the track tp list\n",
    "                if track_tp_list_[0] in tp_idx_list_:\n",
    "                    rechit_matches_.append(event_df_.loc[idx_, 'rechit_id'])\n",
    "                    rechit_local_matches_.append(event_df_.loc[idx_, 'rechit_local_id'])\n",
    "                    # Append the global track id to the rechit\n",
    "                    event_df_.loc[idx_, 'track_ids'].append(global_track_id_)\n",
    "            track_to_rechit_map_['match_count'][global_track_id_] = len(rechit_matches_)\n",
    "            track_to_rechit_map_['rechit_ids'][global_track_id_] = set(rechit_matches_)\n",
    "            track_to_rechit_map_['rechit_local_ids'][global_track_id_] = set(rechit_local_matches_)\n",
    "            \n",
    "        # If track has multiple tp indices, pick the one with the most hits\n",
    "\n",
    "        # Note: This approach *possibly* creates match issues if the tp index with more rechit matches\n",
    "        # has more 'common' hits with other tracks and is later discarded due to the common hits \n",
    "        # belonging to other tracks\n",
    "        if len(track_tp_list_) > 1:\n",
    "            rechit_matches_array_ = []\n",
    "            rechit_local_matches_array_ = []\n",
    "            match_count_array_ = []\n",
    "            \n",
    "            print \"Found multiple TP indices in event\", event_id_, \"for global track\", \n",
    "            print global_track_id_, track_tp_list_\n",
    "            \n",
    "            for track_idx_ in track_tp_list_:\n",
    "                rechit_matches_ = []\n",
    "                rechit_local_matches_ = []\n",
    "                \n",
    "                # Iterate over the index and values of each rechit tp index list\n",
    "                for (idx_, tp_idx_list_) in event_df_['rechit_tp_index'].iteritems():\n",
    "                    if track_idx_ in tp_idx_list_:\n",
    "                        rechit_matches_.append(event_df_.loc[idx_,'rechit_id'])\n",
    "                        rechit_local_matches_.append(event_df_.loc[idx_,'rechit_local_id'])\n",
    "                        # Append the global track id to the rechit\n",
    "                        event_df_.loc[idx_, 'track_ids'].append(global_track_id_)\n",
    "                rechit_matches_array_.append(rechit_matches_)\n",
    "                rechit_local_matches_array_.append(rechit_local_matches_)\n",
    "                match_count_array_.append(len(rechit_matches_))\n",
    "            \n",
    "            # Store the global rechit ids and count of matches in a temporary list\n",
    "            for key, value in zip(match_count_array_, rechit_matches_array_):\n",
    "                tmp_dict_.append((key, value))\n",
    "            \n",
    "            # Pick the largest number of matches and corresponding global rechit ids\n",
    "            tmp_dict_ = sorted(tmp_dict_, reverse=True)\n",
    "            track_to_rechit_map_['match_count'][global_track_id_] = tmp_dict_[0][0]\n",
    "            track_to_rechit_map_['rechit_ids'][global_track_id_] = tmp_dict_[0][1]\n",
    "            track_to_rechit_map_['rechit_local_ids'][global_track_id_] = set(rechit_local_matches_array_)\n",
    "\n",
    "        # Check duplicates\n",
    "        if len(set(rechit_matches_)) < len(rechit_matches_):\n",
    "            raise ValueError('rechit_matches_ has duplicate values: Some Rechits are being matched twice!')\n",
    "        \n",
    "        # Increment the Global Track ID\n",
    "        global_track_id_ += 1\n",
    "    rechit_global_df_.update(event_df_, join='left')\n",
    "    track_param_global_df_ = df.from_dict(track_param_global_map_)\n",
    "track_global_df_ = df.from_dict(track_to_rechit_map_)\n",
    "\n",
    "#Update the match_count for rechits based on the number of total matched tracks\n",
    "match_count_tmp_dict_ = OrderedDict({'match_count': [len(track_id_list_) for track_id_list_ in rechit_global_df_['track_ids']]})\n",
    "print \"Maximum tracks matched for one particle:\", max(match_count_tmp_dict_['match_count'])\n",
    "\n",
    "rechit_global_df_.update(df.from_dict(match_count_tmp_dict_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(stereo_tp_idx_[0])\n",
    "print rechit_global_df_.loc[9505]\n",
    "print track_global_df_.loc[86]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance of df.loc versus multi-index retrieval [ ][ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idx_, event_df_.head()\n",
    "%timeit event_df_['rechit_local_id']\n",
    "%timeit event_df_.iloc[0]['rechit_id']\n",
    "%timeit event_df_.loc[idx_, 'rechit_local_id'] # this turns out the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create the data for testing\n",
    "test_dict_ = {'sample_column':[], 'sample_column_copy':[], 'sample_column_copy_1':[]}\n",
    "\n",
    "global_track_id_ = 999\n",
    "for idx_ in range(1000):\n",
    "    test_dict_['sample_column'].append([global_track_id_])\n",
    "    test_dict_['sample_column_copy'].append([global_track_id_])\n",
    "    test_dict_['sample_column_copy_1'].append([global_track_id_])\n",
    "test_df_ = df.from_dict(test_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for idx_ in range(1000):\n",
    "    test_df_['sample_column'][idx_].append(998)\n",
    "# print test_df_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for idx_ in range(1000):\n",
    "    test_df_.loc[idx_, ('sample_column')].append(998)\n",
    "# print test_df_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conclusion: Use df.loc[idx, 'column']'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data stored in the track_to_rechit_map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in track_to_rechit_map_.keys():\n",
    "    print key, \":\", len(track_to_rechit_map_[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "track_to_rechit_df_ = df.from_dict(track_to_rechit_map_)\n",
    "#print track_to_rechit_df_[track_to_rechit_df_['event_id']==11].head(10)\n",
    "\n",
    "# Calculate the average number of hits per track\n",
    "average_rechits_per_track_ = 0\n",
    "len_array_ = []\n",
    "for rechit_list_ in track_to_rechit_df_['rechit_ids']:\n",
    "    average_rechits_per_track_ += len(rechit_list_)\n",
    "    len_array_.append(len(rechit_list_))\n",
    "\n",
    "print \"Average Rechits per track:\", average_rechits_per_track_/len(track_to_rechit_df_['rechit_ids'])\n",
    "print \"Max. matched hits to track:\", max(len_array_), \"; Global track id:\", len_array_.index(max(len_array_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test to check if the correct tp index has been matched\n",
    "# Change the value of 'trk_id_' to any track that you know has some hits\n",
    "trk_id_ = 5\n",
    "# print track_to_rechit_df_.loc[trk_id_]\n",
    "for rechit_id in track_to_rechit_df_.loc[trk_id_]['rechit_ids']:\n",
    "    for track_idx_ in track_to_rechit_df_.loc[trk_id_]['track_tp_index']:\n",
    "        if track_idx_ in rechit_global_df_.loc[rechit_id]['rechit_tp_index']:\n",
    "            continue\n",
    "        else:\n",
    "            print \"Error: Track and rechit TP index does not match!\"\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cycler import cycler\n",
    "from matplotlib.colors import Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fig_ = plt.figure()\n",
    "#ax_ = Axes3D(fig_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Matched/Unmatched Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Count the number of matched, unmatched, and total rechits/tracks in the dataframe (PER EVENT)\n",
    "\n",
    "Store the count of unmatched, tp_matched, track/rechit_matched, and total rechits/tracks PER EVENT in an array of length number_of_events_\n",
    "Store all four above arrays (unmatched, tp_matched, track/rechit_matched, total) in a dictionary\n",
    "'''\n",
    "\n",
    "def count_matched_items(item_type_):\n",
    "    other_item_ids_ = 'track_ids' if (item_type_=='rechit') else 'rechit_ids'\n",
    "    other_item_matched_ = 'track_matched' if (item_type_=='rechit') else 'rechit_matched'\n",
    "    item_id_ = item_type_ + '_id'\n",
    "    item_tp_index_ = item_type_ + '_tp_index'\n",
    "\n",
    "    # Initialize one array for counts and one for ids of matched/unmatched rechits\n",
    "    item_count_dict_ = OrderedDict({other_item_matched_:[], 'unmatched':[], 'tp_matched':[], 'total':[]})\n",
    "    item_id_dict_ = OrderedDict({'tp_matched':[], other_item_matched_:[], 'unmatched':[]})\n",
    "\n",
    "    for event_id_ in range(number_of_events_):\n",
    "        \n",
    "        # Create a slice of the dataframe with the data for that event\n",
    "        event_df_ = (rechit_global_df_[rechit_global_df_['event_id']==event_id_]) if (item_type_=='rechit') else (track_global_df_[track_global_df_['event_id']==event_id_])\n",
    "\n",
    "        # Count the number of matched, unmatched, and total rechits \n",
    "        num_matched_ = sum(event_df_['match_count'] > 0)\n",
    "        num_unmatched_ = sum(event_df_['match_count'] == 0)\n",
    "        num_total_ = event_df_.shape[0]  # number of rows/rechits in the event\n",
    "        \n",
    "        # Find and store the indices of matched and unmatched rechits\n",
    "        \n",
    "        item_id_dict_[other_item_matched_].append(set(event_df_.loc[event_df_['match_count'] > 0, (item_id_)].tolist()))\n",
    "        item_id_dict_['unmatched'].append(set(event_df_.loc[event_df_['match_count'] == 0, (item_id_)].tolist()))\n",
    "        \n",
    "        # Sanity checks to ensure data has been added into the dataframe corrrectly\n",
    "        assert num_total_ == (num_matched_ + num_unmatched_), \\\n",
    "        \"Rechit counts (unmatched, matched, total) do not add up\"\n",
    "            \n",
    "        if item_type_ == 'rechit':\n",
    "            # Check the number of total rechits for the event is the same as in raw data\n",
    "            assert (len(rechit_global_df_[rechit_global_df_['event_id']==event_id_])) == num_total_, \\\n",
    "            \"Rechits in dataframe %d and stereo_tp_idx_ %d do not match\" % (num_total_, len(stereo_tp_idx_[event_id_]))\n",
    "        \n",
    "        elif item_type_ == 'track':\n",
    "            # Check the number of total tracks for the event is the same as in raw data\n",
    "            assert len(track_tp_idx_[event_id_]) == num_total_, \\\n",
    "            \"Tracks in dataframe %d and track_tp_idx_ %d do not match\" % (num_total_, len(track_tp_idx_[event_id_]))\n",
    "    \n",
    "        # Append the hit counts into the dataframe\n",
    "        item_count_dict_['unmatched'].append(num_unmatched_)\n",
    "        item_count_dict_['total'].append(num_total_)\n",
    "        \n",
    "        # TODO: Why is default value for tracks -2 and rechits None?\n",
    "        # Criteria for tracks is to check if -2 is in the track_tp_index\n",
    "        # Because default match to tp index value is -2\n",
    "        if item_type_ == 'track':\n",
    "            tp_criteria_ = [(-2 not in list_) for list_ in event_df_[item_tp_index_]]\n",
    "        \n",
    "        # Criteria for rechits is to check if length of rechit_tp_index is greater than 0\n",
    "        # Because default match to tp index is none\n",
    "        elif item_type_ == 'rechit':\n",
    "            tp_criteria_ = [(len(list_) > 0) for list_ in event_df_[item_tp_index_]]\n",
    "            #print len(event_df_[tp_criteria_])\n",
    "        \n",
    "        item_count_dict_['tp_matched'].append(len(event_df_[tp_criteria_]))\n",
    "        item_id_dict_['tp_matched'].append(event_df_[tp_criteria_])\n",
    "        \n",
    "        # Criteria for filtering rechits matched to tracks based on 'track_ids' column\n",
    "        other_item_criteria_ = [len(list_) > 0 for list_ in event_df_[other_item_ids_]]\n",
    "        item_count_dict_[other_item_matched_].append(len(event_df_[other_item_criteria_]))\n",
    "    \n",
    "    return item_count_dict_, item_id_dict_\n",
    "\n",
    "def plot_matched_vs_unmatched(item_count_, keys_, item_type_):\n",
    "    ax_ = plt.subplot()\n",
    "    alpha_ = 0.4\n",
    "    for key in keys_:\n",
    "        ax_.hist(item_count_[key], histtype='stepfilled', bins=number_of_events_, \n",
    "             orientation='vertical', alpha=alpha_, label=key)\n",
    "        alpha_ += 0.2\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Count of ' + item_type_)\n",
    "    plt.title(item_type_ + ' Distribution')\n",
    "    plt.savefig('plots/' + gen_event_ + '/' + item_type_ + '/matchdistribution')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "track_count_, track_ids_ = count_matched_items('track')\n",
    "rechit_count_, rechit_ids_ = count_matched_items('rechit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df_ = rechit_global_df_[rechit_global_df_['event_id']==10]\n",
    "tp_criteria_ = [(len(list_) > 0) for list_ in event_df_['rechit_tp_index']]\n",
    "print len(event_df_)\n",
    "event_df_ = event_df_[tp_criteria_]\n",
    "print len(event_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Rechits Matched to TP, Track, or Unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# This is redundant code because we have already filtered out a major chunk of rechits\n",
    "# Check if tp_matched hits outnumber unmatched hits for any event\n",
    "# No particular reason for doing this\n",
    "non_tp_matched_events_ = []\n",
    "for event_id_ in range(number_of_events_):\n",
    "    #for ind_2 in range(len(rechit_count_['unmatched'][ind_1])):\n",
    "    if rechit_count_['tp_matched'][event_id_] > rechit_count_['unmatched'][event_id_]:\n",
    "        continue\n",
    "    else:\n",
    "        non_tp_matched_events_.append(event_id_)\n",
    "print \"Events \", non_tp_matched_events_, \"have more Unmatched than TP-Matched Rechits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_matched_vs_unmatched(rechit_count_, ['track_matched', 'tp_matched', 'total'], 'rechits')\n",
    "plot_matched_vs_unmatched(track_count_, ['rechit_matched', 'tp_matched', 'total'], 'track')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Track and Matched Rechit Eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Calculate the difference in track and rechit eta per event\n",
    "Note: Gets Rechit Eta Values from Rechit Global DF\n",
    "'''\n",
    "\n",
    "# lists of delta-eta between tracks and matched rechits indexed by global_track_id\n",
    "# also contains the mean of these values stored in a separate array using index 'mean_difference'\n",
    "matched_track_eta_difference_ = {}\n",
    "\n",
    "for event_id_ in range(number_of_events_):\n",
    "    #event_df_ = track_global_df_[track_global_df_['event_id']==event_id_]\n",
    "    # iterate over the selected tracks and the rechit ids matched to each track\n",
    "    matched_track_eta_difference_[event_id_] = {}\n",
    "    for track_id_ in track_ids_['rechit_matched'][event_id_]:\n",
    "        trk_eta_diff_ = []\n",
    "        track_id_ = int(track_id_)\n",
    "        # Retrieve eta for the track\n",
    "        trk_eta_ = track_param_global_df_.iloc[track_id_]['track_eta']\n",
    "        rechit_ids_for_track_ = track_global_df_.iloc[track_id_]['rechit_ids']\n",
    "        for rechit_id_ in rechit_ids_for_track_:\n",
    "            rechit_id_ = int(rechit_id_)\n",
    "            trk_eta_diff_.append(trk_eta_ - rechit_param_global_df_.iloc[rechit_id_]['rechit_eta'])\n",
    "        matched_track_eta_difference_[event_id_][track_id_] = trk_eta_diff_\n",
    "    matched_track_eta_difference_[event_id_]['mean_difference'] = [sum(diff_list_)/len(diff_list_) for diff_list_ in matched_track_eta_difference_[event_id_].values()] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize the Difference in Eta between tracks and rechits belonging to the same track.\n",
    "Repeat for SPECIFIED NUMBER OF events in the dataset.\n",
    "'''\n",
    "\n",
    "# TODO: Try to make this an aminated single graph constantly updating\n",
    "# Issue: Matplotlib has problems with 'inline' and 'notebook' \n",
    "# (interactive plots) used in the same Jupyter Notebook\n",
    "\n",
    "# plt.rc('axes', prop_cycle=(cycler('color', ['r', 'g', 'b', 'y']) + cycler('alpha', ['0.2', '0.4', '0.6', '0.8'])))\n",
    "# print rechit_param_global_df_.iloc[:50]['rechit_eta']\n",
    "\n",
    "for event_id_ in range(len(matched_track_eta_difference_.keys())-10, len(matched_track_eta_difference_.keys())):\n",
    "    if len(matched_track_eta_difference_[event_id_]['mean_difference']) == 0:\n",
    "        continue\n",
    "    ax_ = plt.subplot()\n",
    "    ax_.hist(matched_track_eta_difference_[event_id_]['mean_difference'], histtype='stepfilled', \n",
    "             bins=len(matched_track_eta_difference_[event_id_]['mean_difference']), \n",
    "             orientation='vertical')\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Rechit difference')\n",
    "    plt.title('Rechit vs. Track eta-difference Distribution for event ' + str(event_id_))\n",
    "    #plt.savefig('plots/' + gen_event_ + '/track/' + key + 'tracks')\n",
    "    plt.show()\n",
    "    #plt.pause(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Track Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Iterate over 5 track parameters and plot their distribution\n",
    "# Note: THIS IS LOG SCALE\n",
    "\n",
    "for key in track_param_global_df_.columns:\n",
    "    if key == 'track_id':\n",
    "        continue\n",
    "    ax_ = plt.subplot()\n",
    "    ax_.hist(track_param_global_df_[key], histtype='stepfilled', bins=100, orientation='vertical')\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Comment the next line for a linear scale\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.xlabel('Distribution of ' + key)\n",
    "    plt.title(key + ' Distribution')\n",
    "    plt.savefig('plots/' + gen_event_ + '/track/' + key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter High-Pt Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Additional visualization discarding high-pt events for more clarity\n",
    "\n",
    "track_pt_ = data_.array('trackPt')\n",
    "concat_track_pt_ = []\n",
    "high_pt_events_ = {}\n",
    "\n",
    "for event_ in range(len(track_pt_)):\n",
    "    for trk_pt_val_ in track_pt_[event_]:\n",
    "        \n",
    "        # What is a reasonable general threshold for Track Pt?\n",
    "        if trk_pt_val_ < 30:\n",
    "            concat_track_pt_.append(trk_pt_val_)\n",
    "        else:\n",
    "            if event_ in high_pt_events_:\n",
    "                high_pt_events_[event_] += 1\n",
    "            else:\n",
    "                high_pt_events_[event_] = 1\n",
    "            concat_track_pt_.append(30)\n",
    "            \n",
    "print \"Iterating over track pt from\", len(concat_track_pt_), \"tracks\"\n",
    "\n",
    "plt.clf()\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(concat_track_pt_, histtype='stepfilled', bins=len(high_pt_events_.keys())*2, orientation='vertical')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Distribution of Track Pt')\n",
    "plt.title('Track Pt Distribution')\n",
    "plt.savefig('plots/' + gen_event_ + '/track/track-pt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events with high pt tracks and their distribution\n",
    "\n",
    "plt.clf()\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(high_pt_events_.keys(), weights=high_pt_events_.values(), bins=len(high_pt_events_.keys())*2, orientation='vertical')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Number of Tracks')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Event Number')\n",
    "plt.title('Distribution of High Pt Tracks')\n",
    "plt.savefig('plots/' + gen_event_ + '/track/high-pt-events')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot SimHit Distribution in X and Y Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_2d_rechit_parameters(params_, rechit_type_, n_bins_=300, figsize_x_=10, figsize_y_=10, uncut_rechit_ids_=None):\n",
    "    '''\n",
    "    Plot any 2 dimensional representation of the rechits \n",
    "    :param params_: 2-element list of parameters to plot\n",
    "    :param rechit_type_: name of folder to store the file in\n",
    "    :param n_bins_: int\n",
    "    :param figsize_i: size of figure in inches along 'i' axis (i := x or y)\n",
    "\n",
    "    :type params_: list(str)\n",
    "    :type rechit_type_: str\n",
    "    :type n_bins_: int\n",
    "    :type figsize_i: str\n",
    "\n",
    "    :rtype: None \n",
    "    '''\n",
    "    # Add updates to filter uncut Rechit IDs\n",
    "    if uncut_rechit_ids_ is not None:\n",
    "        print \"Method not defined to filter by uncut ids\"\n",
    "    else:\n",
    "        x_coordinate_ = data_.array(params_[0])\n",
    "        y_coordinate_ = data_.array(params_[1])\n",
    "\n",
    "        if len(x_coordinate_) == len(y_coordinate_):\n",
    "            concat_x_ = []\n",
    "            concat_y_ = []\n",
    "\n",
    "            for i in range(len(x_coordinate_)):\n",
    "                concat_x_.extend(x_coordinate_[i])\n",
    "                concat_y_.extend(y_coordinate_[i])\n",
    "\n",
    "        plt.figure(figsize=(figsize_x_, figsize_y_))\n",
    "        ax_ = plt.subplot(1,1,1)\n",
    "        plt.xlabel(params_[0])\n",
    "        plt.ylabel(params_[1])\n",
    "        # Plot the 2D Histogram for Mono Rechits\n",
    "        ax_.set_title(rechit_type_ + ' Distribution')\n",
    "        ax_.patch.set_facecolor('black')\n",
    "        ax_.hist2d(concat_x_, concat_y_, bins=n_bins_, norm=matplotlib.colors.LogNorm(), cmap='hot')\n",
    "        plt.savefig('plots/' + gen_event_ + '/' + rechit_type_ + '/2drechitdistribution')\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "'''\n",
    "Function to plot the rechit parameters as a histogram\n",
    "'''\n",
    "def plot_rechit_parameters(params_, rechit_type_, n_bins_=50, figsize_x_=8, figsize_y_=6):\n",
    "    '''\n",
    "    Plot any 2 dimensional representation of the rechits \n",
    "    :param params_: 2-element list of parameters to plot\n",
    "    :param rechit_type_: name of folder to store the file in\n",
    "    :param n_bins_: int\n",
    "    :param figsize_i_: size of figure in inches along 'i' axis (i := x or y)\n",
    "\n",
    "    :type params_: list(str)\n",
    "    :type rechit_type_: str\n",
    "    :type n_bins_: int\n",
    "    :type figsize_i_: str\n",
    "\n",
    "    :rtype: None \n",
    "    '''\n",
    "    # Define a loop that plots R, Phi, and Eta for Rechits\n",
    "    for property_ in params_:\n",
    "        param_array_ = data_.array(property_)\n",
    "        concat_param_array_ = []\n",
    "\n",
    "        for i in range(len(param_array_)):\n",
    "                concat_param_array_.extend(param_array_[i])\n",
    "\n",
    "        plt.clf()\n",
    "        fig, ax_ = plt.subplots(figsize=(figsize_x_, figsize_y_))\n",
    "        # Plot the 2D Histogram for Mono Rechits\n",
    "        ax_.set_title(rechit_type_ + 'Rechit Distribution of ' + property_)\n",
    "        ax_.hist(concat_param_array_, bins=n_bins_, histtype='stepfilled', align='mid', orientation='vertical')\n",
    "        plt.xlabel('Value of ' + property_)\n",
    "        plt.ylabel('Distribution of ' + property_)\n",
    "        plt.title(property_ + ' Distribution')\n",
    "        plt.grid(True)\n",
    "        plt.savefig('plots/' + gen_event_ + '/' + rechit_type_ + '/' + property_)\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_2d_rechit_parameters([\"simHitY\", \"simHitX\"], 'sim', n_bins_=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Simhits Matched to Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Extract the Simhits Matched to Tracks already from the C++ code\n",
    "'''\n",
    "\n",
    "# Count the simhit matches to tracks\n",
    "simhit_tp_match_ = data_.array(\"simHitMatch\")\n",
    "simhit_count_ = {'track_matched': [], 'total': []}\n",
    "\n",
    "for event_id_ in range(number_of_events_):\n",
    "    match_count_ = 0\n",
    "    for tp_ in simhit_tp_match_[event_id_]:\n",
    "        # Change this to == 0 in the next set of ttbar outfiles\n",
    "        if tp_ > 0:\n",
    "            match_count_ += 1\n",
    "        else:\n",
    "            continue\n",
    "    simhit_count_['total'].append(len(simhit_tp_match_[event_id_]))\n",
    "    simhit_count_['track_matched'].append(match_count_)\n",
    "\n",
    "count_total_ = []\n",
    "count_matched_ = []\n",
    "print \"Event, Total Hit Count, Matched Hit Count\"\n",
    "\n",
    "for i in range(number_of_events_):\n",
    "    count_total_.append(simhit_count_['total'][i])\n",
    "    count_matched_.append(simhit_count_['track_matched'][i])\n",
    "    if simhit_count_['track_matched'][i] == 0:\n",
    "        print i, \"\\t\", simhit_count_['total'][i], \"\\t\", simhit_count_['track_matched'][i]\n",
    "\n",
    "print float(sum(count_matched_))/float(sum(count_total_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "ax_ = plt.subplot()\n",
    "ax_.hist(simhit_count_['track_matched'], bins=len(simhit_count_['track_matched']), orientation='vertical', label='track_matched simhits')\n",
    "ax_.hist(simhit_count_['total'], bins=len(simhit_count_['total']), orientation='vertical', label='track_matched simhits')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Number of Simhits')\n",
    "plt.xscale('log')\n",
    "plt.title('No. of Track Matches for Simhits')\n",
    "#plt.savefig('plots/' + gen_event_ + '/simhit/track-matches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Hole in the Data (2D Plot; 3D Axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Trying to explore the hole in 3 dimensions\n",
    "# Not quite possible to do manually?\n",
    "fig_ = plt.figure()\n",
    "ax_ = Axes3D(fig_)\n",
    "\n",
    "mono_x_ = data_.array('monoHitX')\n",
    "mono_y_ = data_.array('monoHitY')\n",
    "mono_z_ = data_.array('monoHitZ')\n",
    "\n",
    "concat_mono_x_ = []\n",
    "concat_mono_y_ = []\n",
    "concat_mono_z_ = []\n",
    "\n",
    "for event_ in range(10):\n",
    "    for x, y, z, in zip(mono_x_[event_], mono_y_[event_], mono_z_[event_]):\n",
    "        if x < 55 and x > 15 and y > 10 and y < 50:\n",
    "            concat_mono_x_.append(x)\n",
    "            concat_mono_y_.append(y)\n",
    "            concat_mono_z_.append(z)   \n",
    "\n",
    "ax_.scatter3D(concat_mono_x_, concat_mono_y_, s=0.6)\n",
    "#ax_.scatter3D(concat_mono_x_, concat_mono_y_, concat_mono_z_, s=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "# These are the SIMHITS lying along the same cross-section\n",
    "# Clearly there is no hole in the Simhits\n",
    "\n",
    "fig_ = plt.figure()\n",
    "ax_ = Axes3D(fig_)\n",
    "\n",
    "simhit_x_ = data_.array('simHitX')\n",
    "simhit_y_ = data_.array('simHitY')\n",
    "simhit_z_ = data_.array('simHitZ')\n",
    "\n",
    "concat_simhit_x_ = []\n",
    "concat_simhit_y_ = []\n",
    "concat_simhit_z_ = []\n",
    "\n",
    "for event_ in range(8):\n",
    "    for x, y, z, in zip(simhit_x_[event_], simhit_y_[event_], simhit_z_[event_]):\n",
    "        if x < 55 and x > 15 and y > 10 and y < 50:\n",
    "            concat_simhit_x_.append(x)\n",
    "            concat_simhit_y_.append(y)\n",
    "            concat_simhit_z_.append(z)   \n",
    "\n",
    "ax_.scatter3D(concat_simhit_x_, concat_simhit_y_, s=0.6)  # Better visualization\n",
    "# ax_.scatter3D(concat_simhit_x_, concat_simhit_y_, concat_simhit_z_, s=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Mono Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_2d_rechit_parameters([\"monoHitX\", \"monoHitY\"], 'mono')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Rechit Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the plot function to plot histograms of rechit parameters\n",
    "plot_rechit_parameters([\"monoHitR\", \"monoHitPhi\", \"monoHitEta\"], 'mono')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Stereo Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_rechit_parameters([\"stereoHitX\", \"stereoHitY\"], 'stereo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot R, Phi, and Eta for Stereo Hits\n",
    "plot_rechit_parameters([\"stereoHitR\", \"stereoHitPhi\", \"stereoHitEta\"], 'stereo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Integrity of internal data storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Correlate the data to confirm the dataframe has not been corrupted\n",
    "hit_tp_count_ = {}\n",
    "\n",
    "for (id_, tp_idx_list_) in rechit_global_df_[\"rechit_tp_index\"].iteritems():\n",
    "    tp_len_ = len(tp_idx_list_)\n",
    "    if tp_len_ in hit_tp_count_:\n",
    "        hit_tp_count_[tp_len_] += 1\n",
    "    else:\n",
    "        hit_tp_count_[tp_len_] = 1\n",
    "print hit_tp_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage for TF/PyTorch/Graph Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DataFrame Documentation for Pandas states that writing and reading from msgpack is an experimental feature.\n",
    "It is to be released soon, but please use it with care to ensure data is not corrupted.\n",
    "\n",
    "Note: When working with large datasets (>1000 events), you will not be able to save the data.\n",
    "The filesize for rechit_global_df_ is 76 MB for 100 events thus 760 MB for 1000 events and so on.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Writing to serialized format fails in case of copied dataframes as the columns are sets\n",
    "\n",
    "Solution: Iterate over all Global DataFrames, find the columns to replace, \n",
    "and replace with lists instead of sets so that they are serializable\n",
    "'''\n",
    "\n",
    "i = 0\n",
    "# List of strings with dataframe names to set as filenames for storage\n",
    "name_list_ = ['data/track_global_df_', 'data/track_param_global_df_', \n",
    "              'data/rechit_global_df_', 'data/rechit_param_global_df_']\n",
    "\n",
    "for dataframe_ in [track_global_df_, track_param_global_df_, rechit_global_df_, rechit_param_global_df_]:\n",
    "    dataframe_to_update_ = dataframe_.copy(deep=True)\n",
    "    columns_to_replace_ = ['rechit_ids', 'rechit_local_ids', 'rechit_tp_index', 'track_tp_index', 'track_matches']\n",
    "    for column_name_ in columns_to_replace_:    \n",
    "        if column_name_ in dataframe_to_update_:\n",
    "            list_arr_ = []\n",
    "            for set_ in dataframe_to_update_[column_name_]:\n",
    "                list_arr_.append(list(set_))\n",
    "            dataframe_to_update_.update(pd.Series(list_arr_, name=column_name_))\n",
    "    filename_ =  name_list_[i] + '.msgpack'\n",
    "    dataframe_to_update_.to_msgpack(filename_, encoding='utf-8')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "track_global_df_ = pd.read_msgpack('data/track_global_df_.msgpack', encoding='utf-8')\n",
    "track_param_global_df_ = pd.read_msgpack('data/track_param_global_df_.msgpack', encoding='utf-8')\n",
    "rechit_global_df_ = pd.read_msgpack('data/rechit_global_df_.msgpack', encoding='utf-8')\n",
    "rechit_param_global_df_ = pd.read_msgpack('data/rechit_param_global_df_.msgpack', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "This is for future compatibility to allow extraction of data in any format necessary\n",
    "\n",
    "Write the dictionaries containing matched/unmatched rechit/track data to CSV files\n",
    "Read them back to confirm that the data is not corrupted or altered in any manner\n",
    "'''\n",
    "\n",
    "#names_of_csv_files_ = ['data/track_count_.csv', 'data/rechit_count_.csv', \n",
    "                       #'data/track_ids_.csv', 'data/rechit_ids_.csv']    \n",
    "#dicts_to_csv_ = [track_count_, rechit_count_, track_ids_, rechit_ids_]\n",
    "#for i in range(len(names_of_csv_files_)):    \n",
    "\n",
    "pd.DataFrame.from_dict(track_count_).to_csv('data/track_count_.csv', index=False)\n",
    "pd.DataFrame.from_dict(rechit_count_).to_csv('data/rechit_count_.csv', index=False)\n",
    "pd.DataFrame.from_dict(track_ids_).to_csv('data/track_ids_.csv', index=False)\n",
    "pd.DataFrame.from_dict(rechit_ids_).to_csv('data/rechit_ids_.csv', index=False)\n",
    "\n",
    "track_count_ = pd.DataFrame.to_dict(pd.read_csv('data/track_count_.csv'))\n",
    "rechit_count_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_count_.csv'))\n",
    "track_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/track_ids_.csv'))\n",
    "rechit_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_ids_.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the Cuts on Rechits => eta (-0.9, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the list of remaining rechits to filter the global rechit index\n",
    "# Done: Move this cell to a location before the tracks are matched to rechits\n",
    "# Thus you automatically match only the hits that are valid given the cut\n",
    "\n",
    "rechit_param_global_df_cut_ = rechit_param_global_df_.copy()\n",
    "rechit_global_df_cut_ = rechit_global_df_.copy()\n",
    "\n",
    "print len(rechit_global_df_cut_), \"entries in global rechit df\"\n",
    "\n",
    "# Create a dict/hash map of the modified indices for searching efficiently\n",
    "# This seems faster than checking rechit_id in the dataframe each time\n",
    "uncut_rechit_id_map_ = {}\n",
    "for (idx_, item_) in rechit_param_global_df_cut_['rechit_id'].iteritems():\n",
    "    uncut_rechit_id_map_[item_] = idx_\n",
    "\n",
    "# Check that the ID Ordering scheme in the global rechit ordered dicts has \n",
    "# not changed as a result of dropping the rechits that have been cut\n",
    "\n",
    "for x,y in zip(rechit_global_df_cut_['rechit_id'], rechit_param_global_df_cut_['rechit_id']):\n",
    "    if x != y:\n",
    "        print (\"Rechit ID Mismatch: \", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Modify the uncut_rechit_id_map_ to work on a per-event basis.\n",
    "This is done to speed up building the adjacency matrix for the graph based on checking (per id) if a rechit is cut or uncut.\n",
    "Now we can check rechit IDs as being uncut on a per-event basis for each track in the event.\n",
    "\n",
    "#TODO: Arguably this is pointless if Python set access is O(1) because either way we can use sets and size doesn't matter,\n",
    "but we can compare that at a later point.'''\n",
    "\n",
    "uncut_rechit_ids_per_event_ = []\n",
    "for event_id_ in range(number_of_events_):\n",
    "    rechit_event_df_ = rechit_global_df_cut_[rechit_global_df_cut_['event_id']==event_id_]\n",
    "    uncut_rechit_ids_per_event_.append(rechit_event_df_['rechit_id'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/uncut_rechit_ids_', np.array(uncut_rechit_ids_per_event_))\n",
    "uncut_rechit_ids_reloaded_ = np.load('data/uncut_rechit_ids_.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Assert that the read and written are equal in all respects and file-writing does not corrupt data.'''\n",
    "for i in range(number_of_events_):\n",
    "    for (j, k) in zip(uncut_rechit_ids_per_event_, uncut_rechit_ids_reloaded_):\n",
    "        if j == k:\n",
    "            continue\n",
    "        else:\n",
    "            print \"Error: Mismatch in read and written arrays\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Subset of Tracks with and without Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solve for a and b\n",
    "def best_fit(X, Y):\n",
    "\n",
    "    xbar = sum(X)/len(X)\n",
    "    ybar = sum(Y)/len(Y)\n",
    "    n = len(X) # or len(Y)\n",
    "\n",
    "    numer = sum([xi*yi for xi,yi in zip(X, Y)]) - n * xbar * ybar\n",
    "    denum = sum([xi**2 for xi in X]) - n * xbar**2\n",
    "\n",
    "    b = numer / denum\n",
    "    a = ybar - b * xbar\n",
    "\n",
    "    #print('best fit line:\\ny = {:.2f} + {:.2f}x'.format(a, b))\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "# Plot the tracks that have been matched to rechits (WITHOUT THE CUT)\n",
    "\n",
    "# Set a criteria to filter only tracks matched to actual hits\n",
    "criteria_ = [(len(rechit_id_list_) > 0) for rechit_id_list_ in track_global_df_['rechit_ids']]\n",
    "track_matched_hit_ids_ = track_global_df_[criteria_]['rechit_ids']\n",
    "\n",
    "# Verify that you have selected the correct tracks with matched rechits\n",
    "assert len(track_matched_hit_ids_) == len(track_global_df_[track_global_df_['match_count'] > 0]), \\\n",
    "\"Incorrect tracks selected; re-check count of tracks matched to rechits\"\n",
    "\n",
    "\n",
    "#def plot_filtered_tracks(key, tracks_begin_=10, tracks_end_=45):\n",
    "fig_ = plt.figure(figsize=[20, 10])\n",
    "ax_ = plt.subplot(121)\n",
    "\n",
    "event_id_ = 10\n",
    "tracks_begin_, tracks_end_ = 10, 45\n",
    "\n",
    "# Num_tracks should be less thatn the number of actual tracks matched to hits\n",
    "\n",
    "for i in range(tracks_begin_, tracks_end_):\n",
    "    matched_hit_x_ = []\n",
    "    matched_hit_y_ = []\n",
    "    #matched_hit_z_ = []\n",
    "    for matched_id_ in track_matched_hit_ids_.iloc[i]:\n",
    "        matched_hit_x_.append(rechit_param_global_df_.loc[matched_id_, ('rechit_x')]) \n",
    "        matched_hit_y_.append(rechit_param_global_df_.loc[matched_id_, ('rechit_y')]) \n",
    "        #matched_hit_z_.append(rechit_param_global_df_.iloc[matched_id_]['rechit_z'])   \n",
    "    #sorted_x_, sorted_y_, sorted_z_ = zip(*sorted(zip(matched_hit_x_, matched_hit_y_, matched_hit_z_)))\n",
    "    sorted_x_, sorted_y_= zip(*sorted(zip(matched_hit_x_, matched_hit_y_)))\n",
    "    plt.scatter(matched_hit_x_, matched_hit_y_, s=5)\n",
    "    a, b = best_fit(sorted_x_, sorted_y_)\n",
    "    yfit = [a + b * xi for xi in sorted_x_]\n",
    "    plt.plot(sorted_x_, yfit, label=i) # Best-fit line visualization\n",
    "plt.xlabel('rechit_x')\n",
    "plt.ylabel('rechit_y')\n",
    "#plt.zlabel('RechitZ')\n",
    "plt.legend()\n",
    "# ax_.scatter3D(concat_simhit_x_, concat_simhit_y_, concat_simhit_z_, s=0.6)\n",
    "\n",
    "\n",
    "ax_ = plt.subplot(122)\n",
    "\n",
    "for i in range(tracks_begin_, tracks_end_):\n",
    "    matched_hit_x_ = []\n",
    "    matched_hit_y_ = []\n",
    "    #matched_hit_z_ = []\n",
    "    for matched_id_ in track_matched_hit_ids_.iloc[i]:\n",
    "        if matched_id_ in uncut_rechit_id_map_:\n",
    "            matched_hit_x_.append(rechit_param_global_df_.loc[matched_id_, ('rechit_x')]) \n",
    "            matched_hit_y_.append(rechit_param_global_df_.loc[matched_id_, ('rechit_y')]) \n",
    "            #matched_hit_z_.append(rechit_param_global_df_.loc[matched_id_, ('rechit_z')])\n",
    "    if len(matched_hit_x_) <= 1:\n",
    "        print \"Skipped all rechits in track\", i\n",
    "        continue\n",
    "    #sorted_x_, sorted_y_, sorted_z_ = zip(*sorted(zip(matched_hit_x_, matched_hit_y_, matched_hit_z_)))\n",
    "    sorted_x_, sorted_y_= zip(*sorted(zip(matched_hit_x_, matched_hit_y_)))\n",
    "    plt.scatter(matched_hit_x_, matched_hit_y_, s=5)\n",
    "    a, b = best_fit(sorted_x_, sorted_y_)\n",
    "    yfit = [a + b * xi for xi in sorted_x_]\n",
    "    plt.plot(sorted_x_, yfit, label=i) # Best-fit line visualization\n",
    "    #plt.plot(sorted_x_, sorted_y_, label=i) # Better visualization\n",
    "plt.xlabel('RechitX')\n",
    "plt.ylabel('RechitY')\n",
    "#plt.zlabel('RechitZ')\n",
    "plt.legend()\n",
    "plt.savefig('plots/ttbar-100/track/reconstruction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Hits without Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "# Plot the tracks that have been matched to rechits (WITHOUT THE CUT)\n",
    "\n",
    "# Set a criteria to filter only tracks matched to hits\n",
    "criteria_ = [(len(rechit_id_list_) > 0) for rechit_id_list_ in track_global_df_['rechit_ids']]\n",
    "track_matched_hit_ids_ = track_global_df_[criteria_]['rechit_ids']\n",
    "track_matched_hit_ids_.index = pd.RangeIndex(len(track_matched_hit_ids_.index))  \n",
    "\n",
    "# Verify that you have selected the correct tracks with matched rechits\n",
    "assert len(track_matched_hit_ids_) == len(track_global_df_[track_global_df_['match_count'] > 0]), \\\n",
    "\"Incorrect tracks selected; re-check count of tracks matched to rechits\"\n",
    "\n",
    "fig_ = plt.figure()\n",
    "ax_ = Axes3D(fig_)\n",
    "\n",
    "# Defined in previous cell\n",
    "tracks_begin_, tracks_end_ = 0, 10\n",
    "# TODO: Select tracks by event_id\n",
    "event_id_ = 15\n",
    "filtered_rechit_ids_from_track_cuts_ = {}\n",
    "# Num_tracks should be less thatn the number of actual tracks matched to hits\n",
    "\n",
    "for i in range(tracks_begin_, tracks_end_):\n",
    "    matched_hit_x_ = []\n",
    "    matched_hit_y_ = []\n",
    "    matched_hit_z_ = []\n",
    "    for matched_id_ in track_matched_hit_ids_.iloc[i]:\n",
    "        matched_hit_x_.append(rechit_param_global_df_.iloc[int(matched_id_)]['rechit_r']) \n",
    "        matched_hit_y_.append(rechit_param_global_df_.iloc[int(matched_id_)]['rechit_eta']) \n",
    "        matched_hit_z_.append(rechit_param_global_df_.iloc[int(matched_id_)]['rechit_phi'])   \n",
    "    sorted_x_, sorted_y_, sorted_z_ = zip(*sorted(zip(matched_hit_x_, matched_hit_y_, matched_hit_z_)))\n",
    "    ax_.scatter3D(matched_hit_x_, matched_hit_y_, matched_hit_z_, s=5, label=i)\n",
    "    #ax_.plot(sorted_x_, sorted_y_, sorted_z_, label=i)# Better visualization\n",
    "plt.xlabel('RechitR')\n",
    "plt.ylabel('RechitEta')\n",
    "ax_.set_zlabel('RechitPhi')\n",
    "plt.legend()\n",
    "# ax_.scatter3D(concat_simhit_x_, concat_simhit_y_, concat_simhit_z_, s=0.6)\n",
    "\n",
    "fig_ = plt.figure()\n",
    "ax_ = Axes3D(fig_)\n",
    "\n",
    "for i in range(tracks_begin_, tracks_end_):\n",
    "    matched_hit_x_ = []\n",
    "    matched_hit_y_ = []\n",
    "    matched_hit_z_ = []\n",
    "    filtered_rechit_ids_from_track_cuts_[i] = []\n",
    "    for matched_id_ in track_matched_hit_ids_.iloc[i]:\n",
    "        if matched_id_ in uncut_rechit_id_map_:\n",
    "            matched_hit_x_.append(rechit_param_global_df_.iloc[int(matched_id_)]['rechit_r']) \n",
    "            matched_hit_y_.append(rechit_param_global_df_.iloc[int(matched_id_)]['rechit_eta'])\n",
    "            matched_hit_z_.append(rechit_param_global_df_.iloc[int(matched_id_)]['rechit_phi'])\n",
    "            filtered_rechit_ids_from_track_cuts_[i].append(matched_id_)\n",
    "    if len(matched_hit_x_) <= 1:\n",
    "        print \"Skipped all rechits in track\", i\n",
    "        continue\n",
    "    sorted_x_, sorted_y_, sorted_z_ = zip(*sorted(zip(matched_hit_x_, matched_hit_y_, matched_hit_z_)))\n",
    "    # REMOVED Y AXIS ENTIRELY (R-PHI) -------------------------------------------------------------------------\n",
    "    ax_.scatter(matched_hit_x_, matched_hit_y_, matched_hit_z_, s=5, label=i)\n",
    "    #ax_.plot(sorted_x_, sorted_y_, sorted_x_, sorted_z_, label=i)# Better visualization\n",
    "plt.xlabel('RechitR')\n",
    "plt.ylabel('RechitEta')\n",
    "ax_.set_zlabel('RechitPhi')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the 2D Rechit Parameters for Filtered Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: This function can be combined with the previously defined 'plot_2d_rechit_parameters'\n",
    "\n",
    "def plot_filtered_2d_rechit_parameters(params_, rechit_type_, n_bins_=300, figsize_x_=10, figsize_y_=10, uncut_rechit_ids_=None, ax_=None):\n",
    "    '''\n",
    "    Plot any 2 dimensional representation of the rechits \n",
    "    :param params_: 2-element list of parameters to plot\n",
    "    :param rechit_type_: name of folder to store the file in\n",
    "    :param n_bins_: int\n",
    "    :param figsize_i: size of figure in inches along 'i' axis (i := x or y)\n",
    "\n",
    "    :type params_: list(str)\n",
    "    :type rechit_type_: str\n",
    "    :type n_bins_: int\n",
    "    :type figsize_i: str\n",
    "\n",
    "    :rtype: None \n",
    "    '''\n",
    "    if ax_ is None:\n",
    "        plt.figure(figsize=(figsize_x_, figsize_y_))\n",
    "        ax_ = plt.subplot(1,1,1)\n",
    "        \n",
    "    x_coordinate_ = rechit_param_global_df_[params_[0]]\n",
    "    y_coordinate_ = rechit_param_global_df_[params_[1]]\n",
    "    concat_x_ = []\n",
    "    concat_y_ = []\n",
    "    # Add updates to filter uncut Rechit IDs    \n",
    "    if uncut_rechit_ids_ is not None:\n",
    "        for track_id_ in uncut_rechit_ids_:\n",
    "            concat_x_ = []\n",
    "            concat_y_ = []\n",
    "            for id_ in uncut_rechit_ids_[track_id_]:    \n",
    "                concat_x_.append(x_coordinate_[id_])\n",
    "                concat_y_.append(y_coordinate_[id_])\n",
    "            ax_.scatter(concat_x_, concat_y_, alpha=0.9, label=track_id_)\n",
    "        plt.xlabel(params_[0])\n",
    "        plt.ylabel(params_[1])\n",
    "        # Plot the 2D Histogram for Rechit Parameters\n",
    "        ax_.set_title(rechit_type_ + ' Distribution')\n",
    "        plt.legend()\n",
    "        \n",
    "    else:\n",
    "        print(\"No uncut rechit ID List provided\")\n",
    "        for i in range(len(x_coordinate_)):\n",
    "            concat_x_.append(x_coordinate_[i])\n",
    "            concat_y_.append(y_coordinate_[i])\n",
    "   \n",
    "        plt.xlabel(params_[0])\n",
    "        plt.ylabel(params_[1])\n",
    "        # Plot the 2D Histogram for Mono Rechits\n",
    "        ax_.set_title(rechit_type_ + ' Distribution')\n",
    "        ax_.patch.set_facecolor('black')\n",
    "        ax_.hist2d(concat_x_, concat_y_, cmap='hot')\n",
    "    plt.savefig('plots/' + gen_event_ + '/' + rechit_type_ + '/2drechit' + params_[0] + params_[1] + 'distribution')\n",
    "    plt.show()\n",
    "    return ax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_filtered_2d_rechit_parameters([\"rechit_r\", \"rechit_phi\"], 'rechits', uncut_rechit_ids_=filtered_rechit_ids_from_track_cuts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_filtered_2d_rechit_parameters([\"rechit_eta\", \"rechit_phi\"], 'rechits', uncut_rechit_ids_=filtered_rechit_ids_from_track_cuts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_filtered_2d_rechit_parameters([\"rechit_r\", \"rechit_eta\"], 'rechits', uncut_rechit_ids_=filtered_rechit_ids_from_track_cuts_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the cuts on tracks by Eta and Pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Moved this line to top -> replaced track_global_df_ and track_param_global_df_ with cut data\n",
    "# Uncomment next line and comment next 3 lines to cut only by track_pt for checking % tracks remaining\n",
    "# track_param_global_df_cut_ = track_param_global_df_[track_param_global_df_['track_pt'] <= 10]\n",
    "\n",
    "intermediate_df_ = track_param_global_df_[track_param_global_df_['track_eta'] <= 0.9]\n",
    "intermediate_df_ = intermediate_df_[intermediate_df_['track_pt'] <= 1]\n",
    "intermediate_df_ = intermediate_df_[intermediate_df_['track_pt'] >= 0]\n",
    "track_param_global_df_cut_ = intermediate_df_[intermediate_df_['track_eta'] >= -0.9]\n",
    "print len(track_param_global_df_cut_), \"of\", len(track_param_global_df_), \\\n",
    "\"(\", float(len(track_param_global_df_cut_))*100/float(len(track_param_global_df_)), \"%) tracks remain\"\n",
    "\n",
    "track_global_df_cut_ = track_global_df_.iloc[track_param_global_df_cut_['track_id']]\n",
    "print len(track_global_df_cut_), \"entries in global track df\"\n",
    "\n",
    "# Create a dict/hash map of the modified indices for searching efficiently\n",
    "# This seems faster than checking rechit_id in the dataframe each time\n",
    "uncut_track_id_map_ = {}\n",
    "for (idx_, item_) in track_param_global_df_cut_['track_id'].iteritems():\n",
    "    uncut_track_id_map_[item_] = idx_\n",
    "\n",
    "# Check that the ID Ordering scheme in the global track ordered dicts has \n",
    "# not changed as a result of dropping the tracks that have been cut\n",
    "for x,y in zip(track_global_df_cut_['track_id'], track_param_global_df_cut_['track_id']):\n",
    "    if x != y:\n",
    "        print \"Track ID Mismatch: \", x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''Check the average hits per track (post-filtering)'''\n",
    "fig_ = plt.figure(figsize=[8, 8])\n",
    "ax_ = plt.subplot(111)\n",
    "ax_.hist(track_global_df_cut_['match_count'], bins=number_of_events_)\n",
    "plt.xlabel(\"Number of rechit matches to track\")\n",
    "plt.ylabel(\"Number of tracks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot only the filtered rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Append all uncut rechits into arrays (x, y) for plotting\n",
    "concat_x_ = []\n",
    "concat_y_ = []\n",
    "\n",
    "hit_x_df_ = rechit_param_global_df_['rechit_x']\n",
    "hit_y_df_ = rechit_param_global_df_['rechit_y']\n",
    "\n",
    "for rechit_id_ in uncut_rechit_id_map_:\n",
    "    concat_x_.append(hit_x_df_[uncut_rechit_id_map_[rechit_id_]])\n",
    "    concat_y_.append(hit_y_df_[uncut_rechit_id_map_[rechit_id_]])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax_ = plt.subplot(1,1,1)\n",
    "# Plot the 2D Histogram for Mono Rechits\n",
    "ax_.set_title('Uncut Rechit Distribution')\n",
    "ax_.patch.set_facecolor('black')\n",
    "ax_.hist2d(concat_x_, concat_y_, bins=500, norm=matplotlib.colors.LogNorm(), cmap='hot')\n",
    "plt.savefig('plots/' + gen_event_ + '/rechits/uncutrechitdistribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Track Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The figure tells us how the tracks are distributed on average.\n",
    "Note that unmatched tracks usually rank low on the spectrum (0-20 per event)\n",
    "at least for 100 events.'''\n",
    "\n",
    "fig_ = plt.figure(figsize=[8, 8])\n",
    "ax_ = plt.subplot(121)\n",
    "alpha_ = 0.3\n",
    "for key in track_count_.keys():\n",
    "    ax_ = plt.subplot(111)\n",
    "    ax_.hist(track_count_[key].values(), bins=number_of_events_, histtype='stepfilled', \\\n",
    "             orientation='vertical', alpha=alpha_, label=key + ' tracks')\n",
    "    alpha_ += 0.3\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
