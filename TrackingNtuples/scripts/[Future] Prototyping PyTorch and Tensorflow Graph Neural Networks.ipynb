{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define number of events in data\n",
    "number_of_events_ = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the DataFrames Stored during Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refer to Data Preprocessing Notebook to Understand the Semantics and Column Headers'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Refer to Data Preprocessing Notebook to Understand the Semantics and Column Headers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Set new string names of column to avoid using b'' syntax to access the rows and columns\n",
    "def decode_byte_headers(df_=None): \n",
    "    if df_ is not None:\n",
    "        new_names = []\n",
    "        for column_name in df_.columns:\n",
    "            new_names.append(str(column_name.decode()))\n",
    "        df_.columns = new_names\n",
    "        return df_\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.3 ms, sys: 5.46 ms, total: 74.7 ms\n",
      "Wall time: 81.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "track_global_df_ = decode_byte_headers(pd.read_msgpack('data/track_global_df_.msgpack'))\n",
    "track_param_global_df_ = decode_byte_headers(pd.read_msgpack('data/track_param_global_df_.msgpack'))\n",
    "rechit_global_df_ = decode_byte_headers(pd.read_msgpack('data/rechit_global_df_.msgpack'))\n",
    "rechit_param_global_df_ = decode_byte_headers(pd.read_msgpack('data/rechit_param_global_df_.msgpack'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_df_ = track_param_global_df_[track_param_global_df_['track_eta'] <= 0.9]\n",
    "intermediate_df_ = intermediate_df_[intermediate_df_['track_pt'] <= 5]\n",
    "intermediate_df_ = intermediate_df_[intermediate_df_['track_pt'] >= 0.5]\n",
    "track_param_global_df_ = intermediate_df_[intermediate_df_['track_eta'] >= -0.9]\n",
    "track_global_df_ = track_global_df_.iloc[track_param_global_df_['track_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrack_count_ = pd.DataFrame.to_dict(pd.read_csv('data/track_count_.csv'))\\nrechit_count_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_count_.csv'))\\ntrack_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/track_ids_.csv'))\\nrechit_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_ids_.csv'))\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# This cell has been rendered obsolete and is only run to check for backward compatibility\n",
    "# For the other notebook titled 'Data Preprocessing ...'\n",
    "'''\n",
    "track_count_ = pd.DataFrame.to_dict(pd.read_csv('data/track_count_.csv'))\n",
    "rechit_count_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_count_.csv'))\n",
    "track_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/track_ids_.csv'))\n",
    "rechit_ids_ = pd.DataFrame.to_dict(pd.read_csv('data/rechit_ids_.csv'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepHGCal/TFRecords Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_RECHIT_LEN = 3600\n",
    "MAX_TRACK_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320\n",
      "1554\n",
      "2596\n",
      "2939\n",
      "1502\n",
      "1862\n",
      "1204\n",
      "1281\n",
      "2420\n",
      "1566\n",
      "Event  10  has no rechits\n",
      "Event  11  has no rechits\n",
      "Event  12  has no rechits\n",
      "Event  13  has no rechits\n",
      "Event  14  has no rechits\n",
      "Event  15  has no rechits\n",
      "Event  16  has no rechits\n",
      "Event  17  has no rechits\n",
      "Event  18  has no rechits\n",
      "Event  19  has no rechits\n",
      "Event  20  has no rechits\n",
      "Event  21  has no rechits\n",
      "Event  22  has no rechits\n",
      "Event  23  has no rechits\n",
      "Event  24  has no rechits\n",
      "Event  25  has no rechits\n",
      "Event  26  has no rechits\n",
      "Event  27  has no rechits\n",
      "Event  28  has no rechits\n",
      "Event  29  has no rechits\n",
      "Event  30  has no rechits\n",
      "Event  31  has no rechits\n",
      "Event  32  has no rechits\n",
      "Event  33  has no rechits\n",
      "Event  34  has no rechits\n",
      "Event  35  has no rechits\n",
      "Event  36  has no rechits\n",
      "Event  37  has no rechits\n",
      "Event  38  has no rechits\n",
      "Event  39  has no rechits\n",
      "Event  40  has no rechits\n",
      "Event  41  has no rechits\n",
      "Event  42  has no rechits\n",
      "Event  43  has no rechits\n",
      "Event  44  has no rechits\n",
      "Event  45  has no rechits\n",
      "Event  46  has no rechits\n",
      "Event  47  has no rechits\n",
      "Event  48  has no rechits\n",
      "Event  49  has no rechits\n",
      "Event  50  has no rechits\n",
      "Event  51  has no rechits\n",
      "Event  52  has no rechits\n",
      "Event  53  has no rechits\n",
      "Event  54  has no rechits\n",
      "Event  55  has no rechits\n",
      "Event  56  has no rechits\n",
      "Event  57  has no rechits\n",
      "Event  58  has no rechits\n",
      "Event  59  has no rechits\n",
      "Event  60  has no rechits\n",
      "Event  61  has no rechits\n",
      "Event  62  has no rechits\n",
      "Event  63  has no rechits\n",
      "Event  64  has no rechits\n",
      "Event  65  has no rechits\n",
      "Event  66  has no rechits\n",
      "Event  67  has no rechits\n",
      "Event  68  has no rechits\n",
      "Event  69  has no rechits\n",
      "Event  70  has no rechits\n",
      "Event  71  has no rechits\n",
      "Event  72  has no rechits\n",
      "Event  73  has no rechits\n",
      "Event  74  has no rechits\n",
      "Event  75  has no rechits\n",
      "Event  76  has no rechits\n",
      "Event  77  has no rechits\n",
      "Event  78  has no rechits\n",
      "Event  79  has no rechits\n",
      "Event  80  has no rechits\n",
      "Event  81  has no rechits\n",
      "Event  82  has no rechits\n",
      "Event  83  has no rechits\n",
      "Event  84  has no rechits\n",
      "Event  85  has no rechits\n",
      "Event  86  has no rechits\n",
      "Event  87  has no rechits\n",
      "Event  88  has no rechits\n",
      "Event  89  has no rechits\n",
      "Event  90  has no rechits\n",
      "Event  91  has no rechits\n",
      "Event  92  has no rechits\n",
      "Event  93  has no rechits\n",
      "Event  94  has no rechits\n",
      "Event  95  has no rechits\n",
      "Event  96  has no rechits\n",
      "Event  97  has no rechits\n",
      "Event  98  has no rechits\n",
      "Event  99  has no rechits\n",
      "10 graphs generated from data\n",
      "CPU times: user 856 ms, sys: 46.6 ms, total: 902 ms\n",
      "Wall time: 879 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "'''This is the function to create graphs in the form readable by DeepHGCal'''\n",
    "\n",
    "data_dict_list_ = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Global Features are track-based so they vary in length per-event\n",
    "# We find the maximum number of tracks that correspond to max_len of global feature vector\n",
    "# Is it a good idea to zero-pad global feature vectors less than max_len?\n",
    "\n",
    "for event_id_ in range(number_of_events_):\n",
    "    data_dict_ = {}\n",
    "    senders_ = []\n",
    "    receivers_ = []\n",
    "    \n",
    "    track_event_df_ = track_global_df_[track_global_df_['event_id'] == event_id_]\n",
    "    track_param_df_ = track_param_global_df_.loc[track_event_df_['track_id']]\n",
    "    track_df_ = track_event_df_.merge(track_param_df_)\n",
    "\n",
    "    # Sort the tracks according to increasing track_eta and associate a label with each track\n",
    "    # This is done by resetting the track index based on increasing track_et\n",
    "    track_df_.sort_values('track_eta', ascending=True, inplace=True)\n",
    "    track_df_.index = pd.RangeIndex(len(track_df_.index))  \n",
    "\n",
    "    rechit_event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    rechit_param_df_ = rechit_param_global_df_[rechit_param_global_df_['event_id']==event_id_]\n",
    "    rechit_param_df_.index = pd.RangeIndex(len(rechit_param_df_.index))  \n",
    "    if len(rechit_event_df_) != len(rechit_param_df_):\n",
    "        print(\"Error - param data and event data are not of equal length!\")\n",
    "    \n",
    "    number_of_rechits_in_event_ = len(rechit_event_df_)\n",
    "    \n",
    "    # Set the node features as the track features that they belong to\n",
    "    node_indices_ = np.array(rechit_param_df_['rechit_local_id'].tolist()).astype(int)\n",
    "    node_labels_ = []\n",
    "    \n",
    "    # Originally, we were setting node-level features based on the nodes but that can be done\n",
    "    # for the test data set; instead here we can set the node-level features as the track features\n",
    "    # at least for training and \"learn\" the track-level features (eta) based on which we can cluster the nodes?\n",
    "    # The question still remains how do we initialize the edges ???\n",
    "    \n",
    "    # Update: Reverting to node-level features for each node as of now \n",
    "    # Modify it to combine some form of track-level features (target?)\n",
    "    \n",
    "    # Skip the event if it has no rechits\n",
    "    if len(node_indices_) == 0:\n",
    "        print(\"Event \", event_id_, \" has no rechits\" )\n",
    "        continue\n",
    "        \n",
    "    rechit_feature_vector_ = np.transpose(np.array([\n",
    "        rechit_param_df_['rechit_r'].tolist(),\n",
    "        rechit_param_df_['rechit_eta'].tolist(),\n",
    "        rechit_param_df_['rechit_phi'].tolist(),\n",
    "        rechit_param_df_['rechit_x'].tolist(),\n",
    "        rechit_param_df_['rechit_y'].tolist(),\n",
    "        rechit_param_df_['rechit_z'].tolist(),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "        np.zeros(len(rechit_param_df_['rechit_eta'])),\n",
    "    ]))\n",
    "    \n",
    "    track_feature_vector_ = np.transpose(np.array([\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        track_df_['track_eta'].tolist(),\n",
    "        track_df_['track_phi'].tolist(),\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        np.zeros(len(track_df_['track_eta'])),\n",
    "        track_df_['track_dsz'].tolist(),\n",
    "        track_df_['track_dxy'].tolist(),\n",
    "        track_df_['track_qoverp'].tolist(),\n",
    "        track_df_['track_pt'].tolist(),\n",
    "    ]))\n",
    "\n",
    "    node_feature_vector_ = np.vstack((rechit_feature_vector_, track_feature_vector_))\n",
    "    node_feature_vector_ = scaler.fit_transform(node_feature_vector_)    \n",
    "    \n",
    "    # Initialize an array of zeros as labels for the nodes\n",
    "    # Replace each zero with the int (label) associated with the track to which the rechit belongs\n",
    "    # This becomes the target label to identify for that training example\n",
    "    # Later, we will one_hot_encode the target label for working with the tensorflow model\n",
    "    node_label_array_ = [0] * len(rechit_feature_vector_)\n",
    "    track_labels_ = []\n",
    "    \n",
    "    # Associate a label with each rechit\n",
    "    for trk_idx_, row in track_df_.iterrows():\n",
    "        track_rechit_id_array_ = row.rechit_local_ids\n",
    "        # We use the rechit local index as a unique label for the track\n",
    "        # Lower indices are thus associated with lower track eta\n",
    "        for id_ in track_rechit_id_array_:\n",
    "            node_label_array_[int(id_)] = trk_idx_ + 1  # We can now use index '0' as a 'noise' label\n",
    "        \n",
    "        # Also associate a label with each track that we consider as a point among the data\n",
    "        track_labels_.append(trk_idx_ + 1)\n",
    "    \n",
    "    print(len(node_label_array_))   \n",
    "    #print(node_indices_.shape)\n",
    "    #node_labels_ = np.vstack((node_indices_, label_array_)).T\n",
    "    #print(node_labels_.shape)\n",
    "    #assert (len(node_labels_)==len(rechit_feature_vector_)), \"Node label and Node feature vector length mismatch\"\n",
    "    \n",
    "    # Concatenate node and track labels into a single array\n",
    "    # Thereafter, concatenate everything into the node feature matrix \n",
    "    # Note that we duplicate the labels to work with off-the-shelf DeepHGCal Model\n",
    "    nla_ = np.concatenate(((np.array(node_label_array_)), np.array(track_labels_)), axis=0)  # add track labels here\n",
    "    # node_feature_vector_ = np.concatenate((rechit_feature_vector_, nla_[:, None], nla_[:, None]), axis = 1)\n",
    "    assert node_feature_vector_.shape[0] == nla_.shape[0], \"Labels are not equal to training nodes/examples\"\n",
    "    # data comprises of f features--here f is 6--and 1-d vector for labels (total f+1 columns)\n",
    "    # it has n rows of rechits making its shape n x f\n",
    "    # f is constant for all events but n varies with the event thus padding may be necessary\n",
    "    data_dict_ = {\n",
    "    \"data\": node_feature_vector_,\n",
    "    \"labels\": nla_,\n",
    "    }\n",
    "    \n",
    "    data_dict_list_.append(data_dict_)\n",
    "print(len(data_dict_list_), \"graphs generated from data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Testing np stacking and concatenation speeds\n",
    "%%time \n",
    "print(rechit_feature_vector_.shape)\n",
    "nla = np.array(node_label_array_)\n",
    "c = np.concatenate((rechit_feature_vector_, nla[:, None], nla[:, None]), axis = 1)\n",
    "# Concatenate works faster than column_stack - TODO look into their implementations, seems interesting!\n",
    "# P.S. Column stack just uses vstack in the backend according to StackOverflow\n",
    "# c = np.column_stack((rechit_feature_vector_, nla, nla))\n",
    "\n",
    "print(c.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "'''Convert the dict into a tf.Example then use a proto_buffer to \n",
    "serialize it into a compatible format for TFRecords'''\n",
    "\n",
    "def create_tf_example(graph_dict=None, max_hits=None, max_tracks=None, set_one_hot_labels=False):\n",
    "    \"\"\"\n",
    "    :param graph_dict: dictionary with each key representing a feature vector\n",
    "    :param labels: list with \n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    if max_hits is None:\n",
    "        max_hits = 3600\n",
    "    \n",
    "    if graph_dict is None:\n",
    "        raise ValueError(\"No Rechit Feature Matrix provided\")\n",
    "\n",
    "    # TODO: Can we eliminate this padding? After all, graphs are meant to be dynamic structures, right?\n",
    "    # Presently, all features are concatenated into a matrix and converted to a tensor\n",
    "    if 'data' in graph_dict.keys():\n",
    "        original_data = graph_dict['data']\n",
    "        data_dimensions = original_data.shape[1]\n",
    "        # If number of points are less than max_hits threshold\n",
    "        if original_data.shape[0] < max_hits:\n",
    "            event_data_size = max_hits * data_dimensions\n",
    "            \n",
    "            # Reshape the original data into a 1D array\n",
    "            # TODO: Is np.ravel faster? Does it matter?\n",
    "            original_data = graph_dict['data'].reshape(-1).astype(np.float32)\n",
    "            # Pad the data with zeros to ensure the length of data from each event is constant\n",
    "            padded_data = np.concatenate((original_data, np.zeros(event_data_size - len(original_data))), axis=0)\n",
    "            padded_data = np.reshape(padded_data, (max_hits, data_dimensions))\n",
    "        else:\n",
    "            # Eliminate the first bits of the data (these are definitely rechits)\n",
    "            # The last part of the data is the tracks and we do not want to eliminate those\n",
    "            # Since they are useful as 'centroids' for the clustering\n",
    "            padded_data = original_data[-event_data_size:]\n",
    "    else:\n",
    "        raise ValueError(\"Key 'data' not found in rechit data dictionary\")\n",
    "    \n",
    "    # Handle the labels either separately as one-hot-encoded data or together\n",
    "    if 'labels' in graph_dict.keys():\n",
    "        node_labels = graph_dict['labels']\n",
    "        \n",
    "        # Zero-pad the labels (in the case of one-hot encoded labels, the '0' label\n",
    "        # corresponds to noisy points in the cloud)\n",
    "        if len(node_labels) > max_hits:\n",
    "            padded_labels = node_labels[-max_hits:]\n",
    "        else:\n",
    "            # Pad the data with zeros to ensure the length of data from each event is constant\n",
    "            padded_labels = np.concatenate((node_labels, np.zeros(max_hits - len(node_labels))))\n",
    "        \n",
    "        # Check if one-hot-encoded labels are desired\n",
    "        if set_one_hot_labels is True:\n",
    "            if max_tracks is None:\n",
    "                # Set default value of maximum tracks\n",
    "                # This will add as many dimensions to your data\n",
    "                max_tracks = 100\n",
    "            \n",
    "            # Create one-hot-encoded representation of the target labels for the data\n",
    "            target_labels = one_hot_encoder.fit_transform(np.array([padded_labels]).T).toarray()\n",
    "            # If there are more labels than max_tracks, just cut out the last set of labels\n",
    "            if target_labels.shape[1] > max_tracks:\n",
    "                target_labels = target_labels[:, :max_tracks]\n",
    "            else:\n",
    "                # Append zero-columns as padding to these data items\n",
    "                padding_columns = np.zeros((max_hits, max_tracks-target_labels.shape[1]))\n",
    "                target_labels = np.hstack((target_labels, padding_columns))\n",
    "        else:            \n",
    "            # Code to store regular labels in a duplicated array for DeepHGCal\n",
    "            target_labels = np.concatenate((padded_labels[:, None], padded_labels[:, None]), axis = 1)\n",
    "    else:\n",
    "        raise ValueError(\"Key 'labels' not found in rechit data dictionary\")\n",
    "    \n",
    "    # Once the labels and data is padded and concatenated, finalize the data into a 2D matrix\n",
    "    final_data = np.hstack((padded_data, target_labels))\n",
    "    print(final_data.shape)\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature_matrix = {}\n",
    "        \n",
    "    # We flatten this tensor and convert it into a FloatList that is then serialized\n",
    "    # Define the tf Feature to wrap the FloatList\n",
    "    feature_matrix['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=final_data.ravel()))\n",
    "        \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_matrix))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def _parse_function(example_proto, data_dimensions=None):\n",
    "    '''If you would like to read and parse the data stored in TFRecord format, \n",
    "    refer to the [Dev] Prototyping Graph Neural Networks Notebook'''\n",
    "    # max_tracks = 100; features = 10\n",
    "    if data_dimensions is None:\n",
    "        data_dimensions = (3600, 12)\n",
    "        \n",
    "    # Create a description of the features to be read from the TFRecord file(s).  \n",
    "    feature_description = {\n",
    "    'data': tf.FixedLenFeature(data_dimensions, tf.float32),\n",
    "    }\n",
    "\n",
    "    # Parse the input tf.Example proto using the Feature dictionary above.\n",
    "    return tf.parse_single_example(example_proto, feature_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dict_list_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-378b8e8eeda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  options=tf.python_io.TFRecordOptions(\n\u001b[1;32m      5\u001b[0m                                     tf.python_io.TFRecordCompressionType.GZIP)) as tfwriter:\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mevent_number_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_record_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict_list_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtf_example_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_record_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtfwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_example_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dict_list_' is not defined"
     ]
    }
   ],
   "source": [
    "'''Write the TFRecord File'''\n",
    "\n",
    "with tf.python_io.TFRecordWriter('tfrecords/ttbar-10.tfrecord', \n",
    "                                 options=tf.python_io.TFRecordOptions(\n",
    "                                    tf.python_io.TFRecordCompressionType.GZIP)) as tfwriter:\n",
    "    for event_number_, data_record_ in enumerate(data_dict_list_):\n",
    "        tf_example_ = create_tf_example(data_record_)\n",
    "        tfwriter.write(tf_example_)\n",
    "        \n",
    "# Check that the tf_example_ above has been correctly serialized to bytes by parsing it back\n",
    "result = tf.train.Example.FromString(tf_example_)\n",
    "print (type(result))\n",
    "print(\"First value: {}\".format(result.features.feature['data'].float_list.value[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Reading the files from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tfrecords/ttbar-10.tfrecord']\n",
      "()\n",
      "<DatasetV1Adapter shapes: {data: (3600, 12)}, types: {data: tf.float32}>\n",
      "{'data': array([[[-1.3331065e+00, -1.0807133e-01,  1.4805235e-01, ...,\n",
      "         -2.6832486e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.3192385e+00, -7.8542136e-02,  2.1887693e-01, ...,\n",
      "         -2.6832486e-02,  2.0000000e+01,  2.0000000e+01],\n",
      "        [-1.3344549e+00, -1.1461693e-01,  3.1031743e-01, ...,\n",
      "         -2.6832486e-02,  5.0000000e+00,  5.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       [[-1.2096591e+00, -2.0974134e-01, -1.8000221e-01, ...,\n",
      "         -1.3210841e-01,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2022940e+00, -2.0176458e-01, -7.3884867e-02, ...,\n",
      "         -1.3210841e-01,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2217621e+00, -2.2735576e-01, -9.9125244e-03, ...,\n",
      "         -1.3210841e-01,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       [[-1.1888012e+00, -1.3225182e+00, -4.3287626e-01, ...,\n",
      "         -1.5100905e-01,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.1864278e+00, -1.3177474e+00, -1.8010770e-01, ...,\n",
      "         -1.5100905e-01,  1.8000000e+01,  1.8000000e+01],\n",
      "        [-1.1866151e+00, -1.3181069e+00, -1.7944787e-01, ...,\n",
      "         -1.5100905e-01,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.2718418e+00, -1.6847404e+00,  2.3576517e-01, ...,\n",
      "         -8.7680578e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2696129e+00, -1.6797477e+00,  6.4499056e-01, ...,\n",
      "         -8.7680578e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2686646e+00, -3.1359708e-01,  7.8087908e-01, ...,\n",
      "         -8.7680578e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       [[-1.1976781e+00,  3.4129083e-02,  2.2750285e-01, ...,\n",
      "         -8.3435163e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2018991e+00, -1.1946065e+00,  2.4180341e-01, ...,\n",
      "         -8.3435163e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2004777e+00,  3.0608859e-02,  5.0723773e-01, ...,\n",
      "         -8.3435163e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       [[-1.1909662e+00, -1.2619408e+00,  1.3558069e-01, ...,\n",
      "         -2.0856595e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.1992698e+00, -1.2790666e+00,  1.6080789e-01, ...,\n",
      "         -2.0856595e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        [-1.2057627e+00, -1.2945569e+00,  1.8913101e-01, ...,\n",
      "         -2.0856595e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32)}\n",
      "{'data': array([[[-9.14265931e-01, -1.17283082e-02,  2.14943796e-01, ...,\n",
      "         -2.01299369e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-9.06695008e-01, -1.01510370e+00,  1.86466128e-01, ...,\n",
      "         -2.01299369e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-9.18238938e-01, -1.77515633e-02,  3.68421674e-01, ...,\n",
      "         -2.01299369e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        ...,\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
      "\n",
      "       [[-1.18554688e+00,  7.60109276e-02,  2.13883176e-01, ...,\n",
      "         -1.21444665e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.18734932e+00,  6.92808703e-02,  2.37002343e-01, ...,\n",
      "         -1.21444665e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.16510046e+00, -1.07176185e+00,  1.21361241e-01, ...,\n",
      "         -1.21444665e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        ...,\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
      "\n",
      "       [[-1.15029609e+00, -6.22369528e-01,  4.37555760e-01, ...,\n",
      "         -1.36201590e-01,  2.10000000e+01,  2.10000000e+01],\n",
      "        [-1.14787531e+00, -1.89717674e+00,  5.56130528e-01, ...,\n",
      "         -1.36201590e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.15317714e+00, -6.31904423e-01,  7.31942117e-01, ...,\n",
      "         -1.36201590e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        ...,\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.22875881e+00,  1.30605951e-01,  4.42783572e-02, ...,\n",
      "         -2.84641497e-02,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.19699395e+00,  1.79326490e-01,  5.16677834e-02, ...,\n",
      "         -2.84641497e-02,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.20054007e+00,  1.75737143e-01,  5.92023693e-02, ...,\n",
      "         -2.84641497e-02,  0.00000000e+00,  0.00000000e+00],\n",
      "        ...,\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
      "\n",
      "       [[-1.23574054e+00, -1.44009185e+00,  1.44538611e-01, ...,\n",
      "         -1.24516696e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.26175773e+00, -3.64089876e-01,  4.86335576e-01, ...,\n",
      "         -1.24516696e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.26048028e+00, -3.59792113e-01,  9.58268464e-01, ...,\n",
      "         -1.24516696e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        ...,\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
      "\n",
      "       [[-1.12274039e+00, -2.87337959e-01,  1.18647022e-02, ...,\n",
      "         -1.22241378e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        [-1.13790894e+00, -3.03122073e-01,  5.11010736e-02, ...,\n",
      "         -1.22241378e-01,  2.70000000e+01,  2.70000000e+01],\n",
      "        [-1.13373971e+00, -1.55564880e+00,  3.90290990e-02, ...,\n",
      "         -1.22241378e-01,  0.00000000e+00,  0.00000000e+00],\n",
      "        ...,\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "'''Test using the tf.TFRecordDataset to check the values have been read'''\n",
    "\n",
    "# This is the future-compatible version of reading TFRecord files\n",
    "# But this requires eager execution to be enabled which is persently not the case\n",
    "import os\n",
    "\n",
    "\n",
    "# filenames = ['jan-tfrecords/' + name for name in os.listdir('jan-tfrecords/')]\n",
    "filenames = ['tfrecords/' + name for name in os.listdir('tfrecords/')]\n",
    "for x in filenames:\n",
    "    # Remove macOS specific metadata files like .DS_Store\n",
    "    if '.tfrecord' not in x:\n",
    "        filenames.remove(x)\n",
    "print (filenames)\n",
    "\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "print(raw_dataset.output_shapes)\n",
    "\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "print(parsed_dataset)\n",
    "\n",
    "'''ACCESS METHOD REQUIRES EAGER EXECUTION TO BE ENABLED'''\n",
    "\n",
    "'''for raw_record in raw_dataset.take(1):\n",
    "  print(type(raw_record))\n",
    "\n",
    "\n",
    "\n",
    "for parsed_record in parsed_dataset.take(10):\n",
    "  print(parsed_record)\n",
    "'''\n",
    "\n",
    "'''ACCESS METHOD WHEN EAGER EXECUTION IS NOT ENABLED'''\n",
    "\n",
    "iterator = parsed_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "parsed_dataset = parsed_dataset.shuffle(buffer_size=20)\n",
    "parsed_dataset = parsed_dataset.repeat(1)\n",
    "parsed_dataset = parsed_dataset.batch(32)\n",
    "\n",
    "iterator = parsed_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(2):\n",
    "        val = sess.run(next_element)\n",
    "        print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Manually attempting to create a TF Dataset for TFRecords for DeepHGCal\n",
    "Format BxNxF where:\n",
    ":param B: batch size\n",
    ":param N: Number of rechits\n",
    ":param F: Number of features for the rechit\n",
    "'''\n",
    "\n",
    "# This should be handled in DeepHGCal and not manually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepMind Graph Nets/Tensorflow Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from graph_nets import blocks\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Graph Tuples from Hit Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "'''\n",
    "Graphs are created on a per-event basis. We build an adjacency matrix of rechits for each individual event.\n",
    "The rechit connections are defined by the track that they belong to.\n",
    "The target label for each node is taken as the first tp_index in its rechit_tp_list\n",
    "\n",
    "#TODO: Incorporate a more flexible labeling schema - can you label the edges using the 'extra' tp index?\n",
    "Can this learn more interesting structures for the graph(s)?\n",
    "# Solution: Use the same node in different graphs - same as above but implementation-wise easier to do.\n",
    "\n",
    "#TODO: What information do we use to weight the edges in the graph?\n",
    "Differences in rechit parameters?\n",
    "Rechit vs. Track Parameters?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "'''This is the function to create graphs in the form readable by the DeepMind Library'''\n",
    "'''TODO: Use the node_label to actually label the nodes!'''\n",
    "data_dict_list_ = []\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Global Features are track-based so they vary in length per-event\n",
    "# We find the maximum number of tracks that correspond to max_len of global feature vector\n",
    "# Is it a good idea to zero-pad global feature vectors less than max_len?\n",
    "GLOBAL_FEATURES_LEN_ = max([\n",
    "    len(\n",
    "        track_global_df_[track_global_df_['event_id']==event_id_]\n",
    "    ) for event_id_ in range(100)\n",
    "])\n",
    "\n",
    "for event_id_ in range(number_of_events_):\n",
    "    data_dict_ = {}\n",
    "    senders_ = []\n",
    "    receivers_ = []\n",
    "    \n",
    "    track_event_df_ = track_global_df_[track_global_df_['event_id'] == event_id_]\n",
    "    track_param_df_ = track_param_global_df_.loc[track_event_df_['track_id']]\n",
    "    track_df_ = track_event_df_.merge(track_param_df_)\n",
    "\n",
    "    # Sort the tracks according to increasing track_eta and associate a label with each track\n",
    "    # This is done by resetting the track index based on increasing track_et\n",
    "    track_df_.sort_values('track_eta', ascending=True, inplace=True)\n",
    "    track_df_.index = pd.RangeIndex(len(track_df_.index))  \n",
    "\n",
    "    rechit_event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    rechit_param_df_ = rechit_param_global_df_[rechit_param_global_df_['event_id']==event_id_]\n",
    "    rechit_param_df_.index = pd.RangeIndex(len(rechit_param_df_.index))  \n",
    "    if len(rechit_event_df_) != len(rechit_param_df_):\n",
    "        print(\"Error - param data and event data are not of equal length!\")\n",
    "    \n",
    "    number_of_rechits_in_event_ = len(rechit_event_df_)\n",
    "    \n",
    "    # Set the node features as the track features that they belong to\n",
    "    node_labels_ = np.array(rechit_param_df_['rechit_local_id'].tolist()).astype(int)\n",
    "    \n",
    "    # Originally, we were setting node-level features based on the nodes but that can be done\n",
    "    # for the test data set; instead here we can set the node-level features as the track features\n",
    "    # at least for training and \"learn\" the track-level features (eta) based on which we can cluster the nodes?\n",
    "    # The question still remains how do we initialize the edges ???\n",
    "    \n",
    "    # Update: Reverting to node-level features for each node as of now \n",
    "    # Modify it to combine some form of track-level features (target?)\n",
    "    if len(node_labels_) == 0:\n",
    "        continue\n",
    "    rechit_feature_vector_ = np.transpose(np.array([rechit_param_df_['rechit_r'].tolist(),\n",
    "                                  rechit_param_df_['rechit_eta'].tolist(),\n",
    "                                  rechit_param_df_['rechit_phi'].tolist()]))\n",
    "    rechit_feature_vector_ = scaler.fit_transform(rechit_feature_vector_)\n",
    "    node_feature_vector_ = rechit_feature_vector_\n",
    "    \n",
    "    # Edge Features uses the track matches to define edge features for each set of nodes\n",
    "    # We are not relying on edges for now but this will be relevant for message-passing in graph neural networks\n",
    "    edge_feature_vector_ = []\n",
    "    \n",
    "    # For each track, append to the list of source nodes, destination nodes, and node feature vectors\n",
    "    # Keep track of the node label equal to len(track_df_)\n",
    "    node_label_i_ = 0\n",
    "    \n",
    "    for row in track_df_.itertuples():\n",
    "        track_edge_features_ = []\n",
    "        # The itertuples() method for dataframes requires acccess by rows\n",
    "        # The matched_rechit_ids array is present in the third column of this 'row'\n",
    "        track_rechit_id_array_ = row[2]\n",
    "        #print (track_rechit_id_array_)\n",
    "        src_vertices_ = []\n",
    "        dest_vertices_ = []\n",
    "        # Sort the rechits based on values of Rechit R\n",
    "        # Start track building from the inside and move all the way outside\n",
    "        final_rechits_ = sorted(track_rechit_id_array_, \n",
    "                                key=lambda hit_: rechit_param_df_.loc[int(hit_)]['rechit_r'])\n",
    "        final_rechits_ = [int(hit_) for hit_ in final_rechits_]\n",
    "        if len(final_rechits_) < 2:\n",
    "            # Increment the node label\n",
    "            node_label_i_ += 1\n",
    "            continue\n",
    "        elif len(final_rechits_) == 2:\n",
    "            src_vertices_.append(final_rechits_[0])\n",
    "            dest_vertices_.append(final_rechits_[1])\n",
    "        \n",
    "        # In order to extend this to 2 skip-connections (expanding to the assumption that 3 hits can be \n",
    "        # on the same layer, thus all of them should be connected to a hit on the next layer)\n",
    "        # Create another else case for len(final_rechits_) == 3: and add the corresponding vertices\n",
    "        # to src and dest arrays. Then you can modify the addition procedure to include src+[3:] and dest+[:-3]\n",
    "        # So you will have (1,2), (1,3), and (1,4) edges as a simple example of adding 2-skip-connections\n",
    "        else:\n",
    "            # Add the edges starting from node a and going to both a+1 and a+2\n",
    "            # We define this as 1-skip-connection because hits might lie on the same layer\n",
    "            # We originally sort them by the radius to ensure skip-connections have a meaning\n",
    "            src_vertices_.extend(final_rechits_[:-1]+final_rechits_[:-2])\n",
    "            dest_vertices_.extend(final_rechits_[1:]+final_rechits_[2:])\n",
    "        # Increment the node label\n",
    "        node_label_i_ += 1\n",
    "        senders_.extend(src_vertices_)\n",
    "        receivers_.extend(dest_vertices_)\n",
    "        # Define the edge feature vectors of same length as number of vertices\n",
    "        # Indices 7, 8, 9 correspond to track eta, phi, and qoverp values respectively\n",
    "        track_edge_features_ = [[row[7], row[8], row[9]]] * len(src_vertices_)\n",
    "        edge_feature_vector_.append(track_edge_features_)\n",
    "    \n",
    "    if node_label_i_ != len(track_df_):\n",
    "        print(\"Error: Node Labels don't match the number of tracks - spurious labels generated?\", node_label_i_, len(track_df_))\n",
    "    \n",
    "    # Define a zero-padded global feature vector\n",
    "    if len(track_df_) < GLOBAL_FEATURES_LEN_:\n",
    "        global_feature_vector_ = track_df_['track_eta'].tolist() + [0]*(GLOBAL_FEATURES_LEN_ - len(track_df_))\n",
    "    else: \n",
    "        global_feature_vector_ = track_df_['track_eta'].values\n",
    "    \n",
    "    if len(track_edge_features_) != len(dest_vertices_):\n",
    "        print(len(track_edge_features_), len(dest_vertices_))\n",
    "        print(\"Edge features and number of destination edges do not match in event\", event_id_)\n",
    "    \n",
    "    data_dict_ = {\n",
    "    \"nodes\": node_feature_vector_,\n",
    "    \"edges\": edge_feature_vector_,\n",
    "    \"senders\": senders_,\n",
    "    \"receivers\": receivers_\n",
    "    }\n",
    "    data_dict_list_.append(data_dict_)\n",
    "print(len(data_dict_list_), \"graphs generated from data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_graphs_tuple(graphs_tuple):\n",
    "  print(\"Shapes of `GraphsTuple`'s fields:\")\n",
    "  print(graphs_tuple.map(lambda x: x if x is None else x.shape, fields=graphs.ALL_FIELDS))\n",
    "  print(\"\\nData contained in `GraphsTuple`'s fields:\")\n",
    "  print(\"globals:\\n{}\".format(graphs_tuple.globals))\n",
    "  print(\"nodes:\\n{}\".format(graphs_tuple.nodes))\n",
    "  print(\"edges:\\n{}\".format(graphs_tuple.edges))\n",
    "  print(\"senders:\\n{}\".format(graphs_tuple.senders))\n",
    "  print(\"receivers:\\n{}\".format(graphs_tuple.receivers))\n",
    "  print(\"n_node:\\n{}\".format(graphs_tuple.n_node))\n",
    "  print(\"n_edge:\\n{}\".format(graphs_tuple.n_edge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the list of dicts into individua graphs\n",
    "# This *should* require a placeholder in order to manage easier padding (in the future?)\n",
    "# There is an error with the graph creation process so placeholders might be the solution to it\n",
    "\n",
    "tracking_graphs_tuple = utils_np.data_dicts_to_graphs_tuple(data_dict_list_)\n",
    "#print_graphs_tuple(tracking_graphs_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "graphs_nx = utils_np.graphs_tuple_to_networkxs(tracking_graphs_tuple)\n",
    "nrows, ncols = 2, 1\n",
    "_, axs = plt.subplots(nrows=nrows, ncols=ncols, squeeze=False, figsize=(12, 10))\n",
    "for iax in range(nrows*ncols):\n",
    "    colnum = iax%ncols\n",
    "    rownum = int(float(iax)/float(ncols))\n",
    "    nx.draw(graphs_nx[iax], ax=axs[rownum][colnum], node_size=2)\n",
    "    axs[rownum][colnum].set_title(\"Graph {}\".format(iax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(i for i in tracking_graphs_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 2\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "\n",
    "# Loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr\n",
    "\n",
    "# Test/generalization loss.\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step.\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_every_seconds = 20\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "  last_iteration = iteration\n",
    "  feed_dict, _ = create_feed_dict(rand, batch_size_tr, num_nodes_min_max_tr,\n",
    "                                  theta, input_ph, target_ph)\n",
    "  train_values = sess.run({\n",
    "      \"step\": step_op,\n",
    "      \"target\": target_ph,\n",
    "      \"loss\": loss_op_tr,\n",
    "      \"outputs\": output_ops_tr\n",
    "  },\n",
    "                          feed_dict=feed_dict)\n",
    "  the_time = time.time()\n",
    "  elapsed_since_last_log = the_time - last_log_time\n",
    "  if elapsed_since_last_log > log_every_seconds:\n",
    "    last_log_time = the_time\n",
    "    feed_dict, raw_graphs = create_feed_dict(\n",
    "        rand, batch_size_ge, num_nodes_min_max_ge, theta, input_ph, target_ph)\n",
    "    test_values = sess.run({\n",
    "        \"target\": target_ph,\n",
    "        \"loss\": loss_op_ge,\n",
    "        \"outputs\": output_ops_ge\n",
    "    },\n",
    "                           feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages with \"add\" aggregation.\n",
    "        return self.propagate('add', edge_index, x=x, num_nodes=x.size(0))\n",
    "\n",
    "    def message(self, x_j, edge_index, num_nodes):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, num_nodes, dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Sample Edge Label Definition for Rechits - Adjacency List?\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Define the 2-layer GCN'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CUDA available on cmg-gpu1080\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
