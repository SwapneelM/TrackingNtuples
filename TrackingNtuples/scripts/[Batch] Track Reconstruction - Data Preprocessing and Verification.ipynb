{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "### Part I - Preprocessing\n",
    "---------------------------\n",
    "\n",
    "1. [View Keys in Root Data](#View-the-Keys-in-the-Imported-Data)\n",
    "\n",
    "2. [Optimisation Tests I: Python](#Optimisation-Tests)\n",
    "\n",
    "3. [Data Conversion using Uproot](#Load-Data-into-Arrays)\n",
    "\n",
    "4. Preprocessing\n",
    "\t- [Optimisation](#Preprocessing-1:-Reformat-List-of-Indices-to-Sets-of-Indices-for-each-Rechit)\n",
    "\t- [Convert to Dataframes](#Preprocessing-2:-Add-all-data-into-dataframes)\n",
    "\t- [Create Global Rechit Dataframe](#Create-a-Global-Dataframe-of-Rechits)\n",
    "\n",
    "5. [Rechit to Track Matching](#Match-the-Rechits-to-Tracks-and-Create-a-Global-Array-of-Tracks)\n",
    "\n",
    "6. [Optimisation Tests II: Dataframes](#Test-Performance-of-df.loc-versus-multi-index-retrieval-[-][-])\n",
    "\n",
    "\n",
    "### Part II: Raw Data Analysis and Plots\n",
    "-----------------------------------------\n",
    "\n",
    "1. [Count Data in Track to Rechit Map](#Analyse-the-data-stored-in-the-track_to_rechit_map_)\n",
    "\n",
    "2. [Generate Match Count Plots](#Generate-Plots)\n",
    "\t- [Counting Matched vs. Unmatched Rechits](#Analyse-Matched/Unmatched-Rechits)\n",
    "\t- [Plot Count of Track and TP Matched Rechits](#Plot-the-Rechits-Matched-to-TP,-Track,-or-Unmatched)\n",
    "\n",
    "3. Track Analysis\n",
    "\t- [Compare Eta between Tracks and their Rechits](#Compare-Track-and-Matched-Rechit-Eta)\n",
    "\t- [Plot Track Parameter Distribution Histograms](#Plot-Track-Parameters)\n",
    "\t- [Analyse/Filter High-Pt Events](#Filter-High-Pt-Events)\n",
    "\n",
    "4. Rechit/Simhit Analysis\n",
    "\t- [Plot Simhit Distribution 2D](#Plot-SimHit-Distribution-in-X-and-Y-Axes)\n",
    "\t- [Simhit Match Count](#Count-Simhits-Matched-to-Tracks)\n",
    "\t- [Realistic Geometry Simulation: Hole](#Plot-the-Hole-in-the-Data-(2D-Plot;-3D-Axes))\n",
    "\t- [Plot MonoRechit Distribution 2D](#Visualize-the-Mono-Rechits)\n",
    "\t- [Plot Rechit Parameter Distribution Histograms](#Plot-Rechit-Parameters)\n",
    "\t- [Plot StereoRechit Distribution 2D](#Visualize-the-Stereo-Rechits)\n",
    "\n",
    "5. Data Storage\n",
    "\t- [Verify Data is not Corrupted](#Testing-Integrity-of-internal-data-storage)\n",
    "\t- [Write Data to Serialized Output Format](#Data-Storage-for-TF/PyTorch/Graph-Library)\n",
    "\n",
    "\n",
    "### Part III: Filtered (Cut) Data Analysis and Plots\n",
    "--------------------------------------------------\n",
    "\n",
    "1. [Place Cuts on Rechits](#Place-the-Cuts-on-Rechits-=>-eta-(-0.9,-0.9))\n",
    "    - [Scatter Plot of Filtered Rechits](#Plot-the-Hits-without-Connections)\n",
    "\n",
    "2. [Plot 2D Rechit Parameters](#Plot-the-2D-Rechit-Parameters-for-Filtered-Hits)\n",
    "\n",
    "3. [Place Cuts on Tracks](#Place-the-cuts-on-tracks-by-Eta-and-Pt)\n",
    "\n",
    "4. [Plot only Filtered Rechits](#Plot-only-the-filtered-rechits)\n",
    "\n",
    "5. [Plot Matched/Unmatched Track Distribution](#Plot-Track-Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Keys in the Imported Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_event_ = \"ttbar-100\"\n",
    "number_of_events_ = 100\n",
    "outfile_ = \"outfile-\" + gen_event_ + \".root\"\n",
    "data_ = uproot.open(outfile_)[\"ntuples\"][\"tree\"]\n",
    "# data_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Integrity of the Imported Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total 100 events\n"
     ]
    }
   ],
   "source": [
    "stereo_tp_idx_ = data_.array('stereoTPIndex')\n",
    "mono_tp_idx_ = data_.array('monoTPIndex')\n",
    "track_tp_idx_ = data_.array('trackTPIndex')\n",
    "\n",
    "# Check that both have been generated for the same number of events\n",
    "# Just for clarity\n",
    "assert len(track_tp_idx_) == len(stereo_tp_idx_), \"Track and Stereo Number of Events do not match\"\n",
    "assert len(track_tp_idx_) == len(mono_tp_idx_), \"Track and Mono Number of Events do not match\"\n",
    "print \"\\nTotal\", len(track_tp_idx_), \"events\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_set(input_array_):\n",
    "    '''\n",
    "    Format: 3-level nested lists - [[[...] ...] ...]\n",
    "    '''\n",
    "    output_array_ = []\n",
    "    for index_ in range(len(input_array_)):\n",
    "        output_array_.append([])\n",
    "        for second_list_ in input_array_[index_]:\n",
    "            output_array_[index_].append(set(second_list_))\n",
    "    return output_array_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_tp_idx_set_ = list_to_set(mono_tp_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load the track parameters into the respective arrays to be added into the rechit_param_global dataframe\n",
    "'''\n",
    "\n",
    "rechit_cartesian_ = OrderedDict({})\n",
    "for key in ['stereoHitX', 'stereoHitY', 'stereoHitZ', 'monoHitX', 'monoHitY', 'monoHitZ']:\n",
    "    rechit_cartesian_[key] = data_.array(key)\n",
    "\n",
    "rechit_polar_ = OrderedDict({})\n",
    "for key in ['stereoHitR', 'stereoHitEta', 'stereoHitPhi', 'monoHitR', 'monoHitEta', 'monoHitPhi']:\n",
    "    rechit_polar_[key] = data_.array(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 1: Reformat List of Indices to Sets of Indices for each Rechit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all tracking particle index lists to sets for faster search\n",
    "\n",
    "mono_tp_idx_set_ = list_to_set(mono_tp_idx_)\n",
    "stereo_tp_idx_set_ = list_to_set(stereo_tp_idx_)\n",
    "track_tp_idx_set_ = list_to_set(track_tp_idx_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2: Add all data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Global Dataframe of Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Adding stereo and mono rechit data into a global dataframe\n",
    "\n",
    ":event_id: int\n",
    ":rechit_id: int\n",
    ":track_id: int\n",
    ":rechit_ids: list(int)\n",
    ":track_ids: list(int)\n",
    ":track_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":rechit_tp_index: set(int)  # iterating over sets has lower complexity\n",
    ":match_count: int  # count the number of rechits/tracks matched to the given track/rechit\n",
    ":rechit_tp_index_: event-based list of rechit-based list of sets of int (tp_index)\n",
    "\n",
    "'''\n",
    "def create_global_rechit_df(stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_dict_, rechit_polar_dict_):\n",
    "    rechit_global_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                      'track_ids': [], 'match_count': [], 'rechit_local_id': []})\n",
    "    rechit_param_global_map_ = OrderedDict({'event_id': [], 'rechit_id':[], 'rechit_x': [], 'rechit_y': [], 'rechit_z': [], \n",
    "                                            'rechit_r': [], 'rechit_phi': [], 'rechit_eta': [], 'rechit_local_id': []})\n",
    "    global_counter_ = 0\n",
    "    \n",
    "    if len(stereo_tp_idx_) != len(stereo_tp_idx_):\n",
    "        raise ValueError('Rechit arrays represent differing event lengths [stereo, mono]:', len(stereo_tp_idx_), len(mono_tp_idx_))\n",
    "    \n",
    "    for event_id_ in range(len(stereo_tp_idx_)):\n",
    "        # Count the number of rechits in that event\n",
    "        event_rechit_count_ = len(stereo_tp_idx_[event_id_]) + len(mono_tp_idx_[event_id_])\n",
    "\n",
    "        rechit_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        # appends SAME instance of [event_id] event_rechit_count_ times\n",
    "        \n",
    "        rechit_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))     \n",
    "        rechit_global_map_['rechit_tp_index'].extend(stereo_tp_idx_[event_id_])\n",
    "        rechit_global_map_['rechit_tp_index'].extend(mono_tp_idx_[event_id_])\n",
    "        rechit_global_map_['track_ids'].extend([[] for _ in range(event_rechit_count_)])\n",
    "        rechit_global_map_['match_count'].extend([0 for _ in range(event_rechit_count_)])\n",
    "        rechit_global_map_['rechit_local_id'].extend(range(event_rechit_count_))\n",
    "        \n",
    "        # Extend the hit_param_global_map_ with rechit parameters\n",
    "        rechit_param_global_map_['rechit_id'].extend(\n",
    "            range(global_counter_, global_counter_ + event_rechit_count_))\n",
    "        rechit_param_global_map_['event_id'].extend([event_id_] * event_rechit_count_)  \n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['stereoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_x'].extend(rechit_cartesian_dict_['monoHitX'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['stereoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_y'].extend(rechit_cartesian_dict_['monoHitY'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['stereoHitZ'][event_id_])\n",
    "        rechit_param_global_map_['rechit_z'].extend(rechit_cartesian_dict_['monoHitZ'][event_id_])\n",
    "        \n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['stereoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_r'].extend(rechit_polar_dict_['monoHitR'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['stereoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_phi'].extend(rechit_polar_dict_['monoHitPhi'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['stereoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_eta'].extend(rechit_polar_dict_['monoHitEta'][event_id_])\n",
    "        rechit_param_global_map_['rechit_local_id'].extend(range(event_rechit_count_))\n",
    "        global_counter_ += event_rechit_count_\n",
    "    # Convert dict to dataframe\n",
    "    rechit_global_df_ = df.from_dict(rechit_global_map_)\n",
    "    rechit_param_global_df_ = df.from_dict(rechit_param_global_map_)\n",
    "    return rechit_global_df_, rechit_param_global_df_\n",
    "    \n",
    "# Check Memory Usage of DataFrame\n",
    "# print rechit_global_df_.memory_usage(deep=True)\n",
    "# print rechit_param_global_df_.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Global Rechit Array and Global Rechit Parameters Array'''\n",
    "rechit_global_df_uncut_, rechit_param_global_df_uncut_ = create_global_rechit_df(\n",
    "    stereo_tp_idx_, mono_tp_idx_, rechit_cartesian_, rechit_polar_)\n",
    "#print rechit_global_df_.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the Cuts (create DF for Graph Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum hits in an event are:  11047\n"
     ]
    }
   ],
   "source": [
    "'''Check the maximum number of hits in an event'''\n",
    "max_len_ = 0 \n",
    "for i in range(number_of_events_):\n",
    "    len_idx_ = len(stereo_tp_idx_[i]) + len(mono_tp_idx_[i])\n",
    "    if len_idx_ > max_len_:\n",
    "        max_len_ = len_idx_\n",
    "        #print max_len_\n",
    "        \n",
    "print \"Maximum hits in an event are: \", max_len_\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152630 of 600347 0.254236300006 hits remain\n"
     ]
    }
   ],
   "source": [
    "'''This is done here to generate a reduced number of local indices for the tracks to match to rechits.\n",
    "We will replace the rechit_global_df_ generated above and used below for matches with this new dataframe.'''\n",
    "\n",
    "intermediate_df_ = rechit_param_global_df_uncut_[rechit_param_global_df_uncut_['rechit_eta'] <= 0.9]\n",
    "rechit_param_global_df_ = intermediate_df_[intermediate_df_['rechit_eta'] >= -0.9].copy()\n",
    "rechit_global_df_ = rechit_global_df_uncut_.iloc[rechit_param_global_df_['rechit_id']].copy()\n",
    "\n",
    "# Reset the Index of the Cut Dataframe that will become the new Global DataFrame\n",
    "# This will lose the former global rechit index - can this affect the analysis in the future?\n",
    "rechit_global_df_.index = pd.RangeIndex(len(rechit_global_df_.index))  \n",
    "rechit_param_global_df_.index = pd.RangeIndex(len(rechit_global_df_.index))  \n",
    "\n",
    "# Reset the local_rechit_ids for graph networks to have sequential nodes\n",
    "# And so that the node feature vector can be simpler to create sequentially\n",
    "rechit_local_id_dict_ = {'rechit_local_id' : []}\n",
    "# Find the minimum number of rechits in the final list of events\n",
    "min_num_of_rechits_ = 9999\n",
    "for event_id_ in range(number_of_events_):\n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    rechit_local_range_ = range(len(rechit_global_df_[rechit_global_df_['event_id']==event_id_]))\n",
    "    rechit_local_id_dict_['rechit_local_id'].extend(rechit_local_range_)\n",
    "    if rechit_local_range_[-1] < min_num_of_rechits_:\n",
    "        min_num_of_rechits_ = rechit_local_range_[-1]\n",
    "\n",
    "# Update the Global Rechit IDs\n",
    "rechit_global_id_dict_ = {}\n",
    "rechit_global_id_dict_['rechit_id'] = range(len(rechit_global_df_))\n",
    "rechit_global_df_.update(pd.DataFrame.from_dict(rechit_local_id_dict_))    \n",
    "rechit_param_global_df_.update(pd.DataFrame.from_dict(rechit_local_id_dict_))    \n",
    "\n",
    "# Update the Local Rechit IDs\n",
    "rechit_global_df_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))    \n",
    "rechit_param_global_df_.update(pd.DataFrame.from_dict(rechit_global_id_dict_))\n",
    "\n",
    "print len(rechit_param_global_df_), \"of\", len(rechit_param_global_df_uncut_), \\\n",
    "float(len(rechit_param_global_df_))/float(len(rechit_global_df_uncut_)), \"hits remain\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3436\n",
      "1327\n",
      "1551\n",
      "2595\n",
      "2942\n",
      "1500\n",
      "1861\n",
      "1209\n",
      "1280\n",
      "2414\n",
      "1573\n",
      "1530\n",
      "472\n",
      "1808\n",
      "1124\n",
      "2491\n",
      "1759\n",
      "1326\n",
      "950\n",
      "1289\n",
      "2425\n",
      "1185\n",
      "1722\n",
      "2582\n",
      "1748\n",
      "1798\n",
      "2644\n",
      "1058\n",
      "1679\n",
      "1800\n",
      "2244\n",
      "1558\n",
      "1624\n",
      "978\n",
      "452\n",
      "822\n",
      "509\n",
      "1495\n",
      "1803\n",
      "2027\n",
      "1730\n",
      "2283\n",
      "1528\n",
      "1861\n",
      "292\n",
      "950\n",
      "1012\n",
      "340\n",
      "1532\n",
      "940\n",
      "1780\n",
      "2777\n",
      "1057\n",
      "1299\n",
      "869\n",
      "988\n",
      "2983\n",
      "1572\n",
      "3437\n",
      "1096\n",
      "1836\n",
      "2560\n",
      "864\n",
      "691\n",
      "1341\n",
      "1803\n",
      "2429\n",
      "1955\n",
      "1754\n",
      "970\n",
      "649\n",
      "2106\n",
      "593\n",
      "1554\n",
      "1593\n",
      "781\n",
      "742\n",
      "1474\n",
      "765\n",
      "859\n",
      "2441\n",
      "1026\n",
      "2099\n",
      "1613\n",
      "751\n",
      "2503\n",
      "618\n",
      "2169\n",
      "1641\n",
      "1287\n",
      "316\n",
      "1976\n",
      "1572\n",
      "1674\n",
      "1310\n",
      "2525\n",
      "1566\n",
      "697\n",
      "1158\n",
      "1928\n",
      "961\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the Rechits to Tracks and Create a Global Array of Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tracks matched for one particle: 3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Match Rechits to Tracks.\n",
    "Create the Global Track Array and Global Track Parameter Array.\n",
    "'''\n",
    "# TODO: Refactor this to enable placing track cuts before forming dataframe and reduce processing by 75%\n",
    "# The 75% metric follows from: For 100 events track cuts reduce tracks by 75%\n",
    "# Initialize the Global Track Parameter Map\n",
    "track_param_global_map_ = OrderedDict({})\n",
    "for key in ['track_id', 'track_eta', 'track_phi', 'track_qoverp', 'track_dxy', 'track_dsz', 'track_pt']:\n",
    "    track_param_global_map_[key] = []\n",
    "    \n",
    "# Define the dictionaries to be cast into dataframes\n",
    "track_to_rechit_map_ = OrderedDict({'event_id': [], 'track_id': [], 'track_tp_index': [], \n",
    "                                    'rechit_ids': [], 'match_count': [], 'rechit_local_ids': []})\n",
    "\n",
    "# Future Requirement?\n",
    "rechit_to_track_map_ = OrderedDict({'event_id': [], 'rechit_id': [], 'rechit_tp_index': [],\n",
    "                                    'track_ids': [], 'match_count': []})\n",
    "\n",
    "# Initialize the Global Track ID\n",
    "global_track_id_ = 0\n",
    "\n",
    "for event_id_ in range(len(track_tp_idx_)):\n",
    "    \n",
    "    num_tracks_in_event_ = len(track_tp_idx_[event_id_])\n",
    "\n",
    "    # Add track data to the dict in an efficient manner\n",
    "    track_to_rechit_map_['event_id'].extend([event_id_] * num_tracks_in_event_)\n",
    "    \n",
    "    global_track_id_range_ = range(global_track_id_, global_track_id_ + num_tracks_in_event_)\n",
    "    \n",
    "    track_to_rechit_map_['track_id'].extend(global_track_id_range_)\n",
    "    track_to_rechit_map_['track_tp_index'].extend(track_tp_idx_[event_id_])\n",
    "    \n",
    "    # Append multiple empty lists in place of the values not filled yet\n",
    "    track_to_rechit_map_['match_count'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_['rechit_ids'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    track_to_rechit_map_['rechit_local_ids'].extend([] for _ in range(num_tracks_in_event_))\n",
    "    \n",
    "    # Fill in the Global Track Parameters\n",
    "    track_param_global_map_['track_id'].extend(global_track_id_range_)\n",
    "    track_param_global_map_['track_eta'].extend(data_.array('trackEta')[event_id_])\n",
    "    track_param_global_map_['track_phi'].extend(data_.array('trackPhi')[event_id_])\n",
    "    track_param_global_map_['track_pt'].extend(data_.array('trackPt')[event_id_])\n",
    "    track_param_global_map_['track_qoverp'].extend(data_.array('qoverp')[event_id_])\n",
    "    track_param_global_map_['track_dxy'].extend(data_.array('dxy')[event_id_])\n",
    "    track_param_global_map_['track_dsz'].extend(data_.array('dsz')[event_id_])\n",
    "    \n",
    "    # Retrieve the subset of the global rechit dataframe for this event_id\n",
    "    event_df_ = rechit_global_df_[rechit_global_df_['event_id']==event_id_]\n",
    "    \n",
    "    # Check the TPs matched to tracks and find rechits for each TP (Stereo and Mono)\n",
    "    for track_tp_list_ in track_tp_idx_[event_id_]:\n",
    "        rechit_matches_ = []\n",
    "        rechit_local_matches_ = []\n",
    "        if len(track_tp_list_) == 0:\n",
    "            continue\n",
    "            \n",
    "        if len(track_tp_list_) == 1:\n",
    "\n",
    "            # Iterate over the index and values of each rechit tp index list\n",
    "            for (idx_, tp_idx_list_) in event_df_['rechit_tp_index'].iteritems():\n",
    "                # Find the match for the first tp index in the track tp list\n",
    "                if track_tp_list_[0] in tp_idx_list_:\n",
    "                    rechit_matches_.append(event_df_.loc[idx_, 'rechit_id'])\n",
    "                    rechit_local_matches_.append(event_df_.loc[idx_, 'rechit_local_id'])\n",
    "                    # Append the global track id to the rechit\n",
    "                    event_df_.loc[idx_, 'track_ids'].append(global_track_id_)\n",
    "            track_to_rechit_map_['match_count'][global_track_id_] = len(rechit_matches_)\n",
    "            track_to_rechit_map_['rechit_ids'][global_track_id_] = set(rechit_matches_)\n",
    "            track_to_rechit_map_['rechit_local_ids'][global_track_id_] = set(rechit_local_matches_)\n",
    "            \n",
    "        # If track has multiple tp indices, pick the one with the most hits\n",
    "\n",
    "        # Note: This approach *possibly* creates match issues if the tp index with more rechit matches\n",
    "        # has more 'common' hits with other tracks and is later discarded due to the common hits \n",
    "        # belonging to other tracks\n",
    "        if len(track_tp_list_) > 1:\n",
    "            rechit_matches_array_ = []\n",
    "            rechit_local_matches_array_ = []\n",
    "            match_count_array_ = []\n",
    "            \n",
    "            print \"Found multiple TP indices in event\", event_id_, \"for global track\", \n",
    "            print global_track_id_, track_tp_list_\n",
    "            \n",
    "            for track_idx_ in track_tp_list_:\n",
    "                rechit_matches_ = []\n",
    "                rechit_local_matches_ = []\n",
    "                \n",
    "                # Iterate over the index and values of each rechit tp index list\n",
    "                for (idx_, tp_idx_list_) in event_df_['rechit_tp_index'].iteritems():\n",
    "                    if track_idx_ in tp_idx_list_:\n",
    "                        rechit_matches_.append(event_df_.loc[idx_,'rechit_id'])\n",
    "                        rechit_local_matches_.append(event_df_.loc[idx_,'rechit_local_id'])\n",
    "                        # Append the global track id to the rechit\n",
    "                        event_df_.loc[idx_, 'track_ids'].append(global_track_id_)\n",
    "                rechit_matches_array_.append(rechit_matches_)\n",
    "                rechit_local_matches_array_.append(rechit_local_matches_)\n",
    "                match_count_array_.append(len(rechit_matches_))\n",
    "            \n",
    "            # Store the global rechit ids and count of matches in a temporary list\n",
    "            for key, value in zip(match_count_array_, rechit_matches_array_):\n",
    "                tmp_dict_.append((key, value))\n",
    "            \n",
    "            # Pick the largest number of matches and corresponding global rechit ids\n",
    "            tmp_dict_ = sorted(tmp_dict_, reverse=True)\n",
    "            track_to_rechit_map_['match_count'][global_track_id_] = tmp_dict_[0][0]\n",
    "            track_to_rechit_map_['rechit_ids'][global_track_id_] = tmp_dict_[0][1]\n",
    "            track_to_rechit_map_['rechit_local_ids'][global_track_id_] = set(rechit_local_matches_array_)\n",
    "\n",
    "        # Check duplicates\n",
    "        if len(set(rechit_matches_)) < len(rechit_matches_):\n",
    "            raise ValueError('rechit_matches_ has duplicate values: Some Rechits are being matched twice!')\n",
    "        \n",
    "        # Increment the Global Track ID\n",
    "        global_track_id_ += 1\n",
    "    rechit_global_df_.update(event_df_, join='left')\n",
    "    track_param_global_df_ = df.from_dict(track_param_global_map_)\n",
    "track_global_df_ = df.from_dict(track_to_rechit_map_)\n",
    "\n",
    "#Update the match_count for rechits based on the number of total matched tracks\n",
    "match_count_tmp_dict_ = OrderedDict({'match_count': [len(track_id_list_) for track_id_list_ in rechit_global_df_['track_ids']]})\n",
    "print \"Maximum tracks matched for one particle:\", max(match_count_tmp_dict_['match_count'])\n",
    "\n",
    "rechit_global_df_.update(df.from_dict(match_count_tmp_dict_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data stored in the track_to_rechit_map_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rechits per track: 5\n",
      "Max. matched hits to track: 77 ; Global track id: 4378\n"
     ]
    }
   ],
   "source": [
    "track_to_rechit_df_ = df.from_dict(track_to_rechit_map_)\n",
    "#print track_to_rechit_df_[track_to_rechit_df_['event_id']==11].head(10)\n",
    "\n",
    "# Calculate the average number of hits per track\n",
    "average_rechits_per_track_ = 0\n",
    "len_array_ = []\n",
    "for rechit_list_ in track_to_rechit_df_['rechit_ids']:\n",
    "    average_rechits_per_track_ += len(rechit_list_)\n",
    "    len_array_.append(len(rechit_list_))\n",
    "\n",
    "print \"Average Rechits per track:\", average_rechits_per_track_/len(track_to_rechit_df_['rechit_ids'])\n",
    "print \"Max. matched hits to track:\", max(len_array_), \"; Global track id:\", len_array_.index(max(len_array_))\n",
    "\n",
    "\n",
    "# Test to check if the correct tp index has been matched\n",
    "# Change the value of 'trk_id_' to any track that you know has some hits\n",
    "trk_id_ = len_array_.index(max(len_array_))\n",
    "# print track_to_rechit_df_.loc[trk_id_]\n",
    "for rechit_id in track_to_rechit_df_.loc[trk_id_]['rechit_ids']:\n",
    "    for track_idx_ in track_to_rechit_df_.loc[trk_id_]['track_tp_index']:\n",
    "        if track_idx_ in rechit_global_df_.loc[rechit_id]['rechit_tp_index']:\n",
    "            continue\n",
    "        else:\n",
    "            print \"Error: Track and rechit TP index does not match!\"\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cycler import cycler\n",
    "from matplotlib.colors import Colormap\n",
    "\n",
    "#fig_ = plt.figure()\n",
    "#ax_ = Axes3D(fig_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Matched/Unmatched Rechits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Count the number of matched, unmatched, and total rechits/tracks in the dataframe (PER EVENT)\n",
    "\n",
    "Store the count of unmatched, tp_matched, track/rechit_matched, and total rechits/tracks PER EVENT in an array of length number_of_events_\n",
    "Store all four above arrays (unmatched, tp_matched, track/rechit_matched, total) in a dictionary\n",
    "'''\n",
    "\n",
    "def count_matched_items(item_type_):\n",
    "    other_item_ids_ = 'track_ids' if (item_type_=='rechit') else 'rechit_ids'\n",
    "    other_item_matched_ = 'track_matched' if (item_type_=='rechit') else 'rechit_matched'\n",
    "    item_id_ = item_type_ + '_id'\n",
    "    item_tp_index_ = item_type_ + '_tp_index'\n",
    "\n",
    "    # Initialize one array for counts and one for ids of matched/unmatched rechits\n",
    "    item_count_dict_ = OrderedDict({other_item_matched_:[], 'unmatched':[], 'tp_matched':[], 'total':[]})\n",
    "    item_id_dict_ = OrderedDict({'tp_matched':[], other_item_matched_:[], 'unmatched':[]})\n",
    "\n",
    "    for event_id_ in range(number_of_events_):\n",
    "        \n",
    "        # Create a slice of the dataframe with the data for that event\n",
    "        event_df_ = (rechit_global_df_[rechit_global_df_['event_id']==event_id_]) if (item_type_=='rechit') else (track_global_df_[track_global_df_['event_id']==event_id_])\n",
    "\n",
    "        # Count the number of matched, unmatched, and total rechits \n",
    "        num_matched_ = sum(event_df_['match_count'] > 0)\n",
    "        num_unmatched_ = sum(event_df_['match_count'] == 0)\n",
    "        num_total_ = event_df_.shape[0]  # number of rows/rechits in the event\n",
    "        \n",
    "        # Find and store the indices of matched and unmatched rechits\n",
    "        \n",
    "        item_id_dict_[other_item_matched_].append(set(event_df_.loc[event_df_['match_count'] > 0, (item_id_)].tolist()))\n",
    "        item_id_dict_['unmatched'].append(set(event_df_.loc[event_df_['match_count'] == 0, (item_id_)].tolist()))\n",
    "        \n",
    "        # Sanity checks to ensure data has been added into the dataframe corrrectly\n",
    "        assert num_total_ == (num_matched_ + num_unmatched_), \\\n",
    "        \"Rechit counts (unmatched, matched, total) do not add up\"\n",
    "            \n",
    "        if item_type_ == 'rechit':\n",
    "            # Check the number of total rechits for the event is the same as in raw data\n",
    "            assert (len(rechit_global_df_[rechit_global_df_['event_id']==event_id_])) == num_total_, \\\n",
    "            \"Rechits in dataframe %d and stereo_tp_idx_ %d do not match\" % (num_total_, len(stereo_tp_idx_[event_id_]))\n",
    "        \n",
    "        elif item_type_ == 'track':\n",
    "            # Check the number of total tracks for the event is the same as in raw data\n",
    "            assert len(track_tp_idx_[event_id_]) == num_total_, \\\n",
    "            \"Tracks in dataframe %d and track_tp_idx_ %d do not match\" % (num_total_, len(track_tp_idx_[event_id_]))\n",
    "    \n",
    "        # Append the hit counts into the dataframe\n",
    "        item_count_dict_['unmatched'].append(num_unmatched_)\n",
    "        item_count_dict_['total'].append(num_total_)\n",
    "        \n",
    "        # TODO: Why is default value for tracks -2 and rechits None?\n",
    "        # Criteria for tracks is to check if -2 is in the track_tp_index\n",
    "        # Because default match to tp index value is -2\n",
    "        if item_type_ == 'track':\n",
    "            tp_criteria_ = [(-2 not in list_) for list_ in event_df_[item_tp_index_]]\n",
    "        \n",
    "        # Criteria for rechits is to check if length of rechit_tp_index is greater than 0\n",
    "        # Because default match to tp index is none\n",
    "        elif item_type_ == 'rechit':\n",
    "            tp_criteria_ = [(len(list_) > 0) for list_ in event_df_[item_tp_index_]]\n",
    "            #print len(event_df_[tp_criteria_])\n",
    "        \n",
    "        item_count_dict_['tp_matched'].append(len(event_df_[tp_criteria_]))\n",
    "        item_id_dict_['tp_matched'].append(event_df_[tp_criteria_])\n",
    "        \n",
    "        # Criteria for filtering rechits matched to tracks based on 'track_ids' column\n",
    "        other_item_criteria_ = [len(list_) > 0 for list_ in event_df_[other_item_ids_]]\n",
    "        item_count_dict_[other_item_matched_].append(len(event_df_[other_item_criteria_]))\n",
    "    \n",
    "    return item_count_dict_, item_id_dict_\n",
    "\n",
    "def plot_matched_vs_unmatched(item_count_, keys_, item_type_):\n",
    "    ax_ = plt.subplot()\n",
    "    alpha_ = 0.4\n",
    "    for key in keys_:\n",
    "        ax_.hist(item_count_[key], histtype='stepfilled', bins=number_of_events_, \n",
    "             orientation='vertical', alpha=alpha_, label=key)\n",
    "        alpha_ += 0.2\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Count of ' + item_type_)\n",
    "    plt.title(item_type_ + ' Distribution')\n",
    "    plt.savefig('plots/' + gen_event_ + '/' + item_type_ + '/matchdistribution')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_count_, track_ids_ = count_matched_items('track')\n",
    "rechit_count_, rechit_ids_ = count_matched_items('rechit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Integrity of internal data storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 30627, 1: 120657, 2: 1265, 3: 65, 4: 9, 5: 2, 6: 2, 8: 1, 9: 2}\n"
     ]
    }
   ],
   "source": [
    "# Correlate the data to confirm the dataframe has not been corrupted\n",
    "hit_tp_count_ = {}\n",
    "\n",
    "for (id_, tp_idx_list_) in rechit_global_df_[\"rechit_tp_index\"].iteritems():\n",
    "    tp_len_ = len(tp_idx_list_)\n",
    "    if tp_len_ in hit_tp_count_:\n",
    "        hit_tp_count_[tp_len_] += 1\n",
    "    else:\n",
    "        hit_tp_count_[tp_len_] = 1\n",
    "print hit_tp_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage for TF-DeepHGCal/PyTorch/Graph Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DataFrame Documentation for Pandas states that writing and reading from msgpack is an experimental feature.\\nIt is to be released soon, but please use it with care to ensure data is not corrupted.\\n\\nNote: When working with large datasets (>1000 events), you will not be able to save the data.\\nThe filesize for rechit_global_df_ is 76 MB for 100 events thus 760 MB for 1000 events and so on.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''DataFrame Documentation for Pandas states that writing and reading from msgpack is an experimental feature.\n",
    "It is to be released soon, but please use it with care to ensure data is not corrupted.\n",
    "\n",
    "Note: When working with large datasets (>1000 events), you will not be able to save the data.\n",
    "The filesize for rechit_global_df_ is 76 MB for 100 events thus 760 MB for 1000 events and so on.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor dataframe_ in [track_global_df_, track_param_global_df_, rechit_global_df_, rechit_param_global_df_]:\\n    dataframe_to_update_ = dataframe_.copy(deep=True)\\n    columns_to_replace_ = ['rechit_ids', 'rechit_local_ids', 'rechit_tp_index', 'track_tp_index', 'track_matches']\\n    for column_name_ in columns_to_replace_:    \\n        if column_name_ in dataframe_to_update_:\\n            list_arr_ = []\\n            for set_ in dataframe_to_update_[column_name_]:\\n                list_arr_.append(list(set_))\\n            dataframe_to_update_.update(pd.Series(list_arr_, name=column_name_))\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Writing to serialized format fails in case of copied dataframes as the columns are sets\n",
    "\n",
    "Solution: Iterate over all Global DataFrames, find the columns to replace, \n",
    "and replace with lists instead of sets so that they are serializable\n",
    "'''\n",
    "'''\n",
    "for dataframe_ in [track_global_df_, track_param_global_df_, rechit_global_df_, rechit_param_global_df_]:\n",
    "    dataframe_to_update_ = dataframe_.copy(deep=True)\n",
    "    columns_to_replace_ = ['rechit_ids', 'rechit_local_ids', 'rechit_tp_index', 'track_tp_index', 'track_matches']\n",
    "    for column_name_ in columns_to_replace_:    \n",
    "        if column_name_ in dataframe_to_update_:\n",
    "            list_arr_ = []\n",
    "            for set_ in dataframe_to_update_[column_name_]:\n",
    "                list_arr_.append(list(set_))\n",
    "            dataframe_to_update_.update(pd.Series(list_arr_, name=column_name_))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
